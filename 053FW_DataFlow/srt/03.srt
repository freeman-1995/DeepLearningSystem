1
00:00:00,000 --> 00:00:06,375
哈喽大家好,今天换了一个新的桌面

2
00:00:06,375 --> 00:00:10,250
我觉得稍微比之前好看一点

3
00:00:10,250 --> 00:00:13,000
或者说时尚一点

4
00:00:13,000 --> 00:00:14,875
这里面这个是MindSpore

5
00:00:14,875 --> 00:00:17,000
MindSpore我把它调整过来了这个图

6
00:00:17,000 --> 00:00:19,475
废话就不多说了

7
00:00:19,475 --> 00:00:24,725
今天来讲讲一个比较难或者在

8
00:00:24,725 --> 00:00:27,025
计算图里面最核心的一个概念

9
00:00:27,025 --> 00:00:29,000
就是计算图与自动微分

10
00:00:32,000 --> 00:00:35,600
在这一节课里面可能讲的内容有三个

11
00:00:35,600 --> 00:00:39,000
第一个是深度学习和微分之间的关系

12
00:00:39,000 --> 00:00:41,550
之前在自动微分系列里面

13
00:00:41,550 --> 00:00:44,725
其实只是讲了自动微分自身的一个概念 

14
00:00:44,725 --> 00:00:47,500
怎么通过计算机去实现自动微分

15
00:00:47,500 --> 00:00:51,000
但是没有讲深度学习跟微分的关系

16
00:00:51,000 --> 00:00:55,920
然后就去回顾一下自动微分的一个原理

17
00:00:55,920 --> 00:00:57,925
搞清楚为什么需要自动微分

18
00:00:57,925 --> 00:01:00,000
自动微分的形式或者实现方式

19
00:01:00,000 --> 00:01:04,670
最后就是讲讲今天主要的课程

20
00:01:04,670 --> 00:01:07,000
计算图怎么去表达自动微分

21
00:01:07,000 --> 00:01:13,275
首先回顾一下深度学习训练的一个流程

22
00:01:13,275 --> 00:01:15,000
训练其实主要是分三个

23
00:01:15,000 --> 00:01:16,775
第一个是前向计算

24
00:01:16,775 --> 00:01:21,000
然后还是以马东梅这个例子做一个前向的计算

25
00:01:21,000 --> 00:01:23,275
接着可能通过自动微分 

26
00:01:23,275 --> 00:01:27,000
或者AI框架实现一个反向的计算

27
00:01:27,000 --> 00:01:33,000
有了这个反向图之后就可以第三步更新可学习的权重参数

28
00:01:33,000 --> 00:01:36,000
这里面可学习的权重参数就是这个W

29
00:01:37,000 --> 00:01:38,875
然后通过自动微分求导

30
00:01:38,875 --> 00:01:41,000
然后不断的更新

31
00:01:41,000 --> 00:01:43,185
因为大家都知道导数是求函数的顺势的变化趋势

32
00:01:43,185 --> 00:01:46,225
拿到这个导数的趋势之后

33
00:01:46,225 --> 00:01:51,000
就会迭代式的去求解优化损失值了

34
00:01:51,000 --> 00:01:54,675
所以在数学里面这个神经网络 

35
00:01:54,675 --> 00:01:59,400
就是刚才马东梅这个非常复杂的神经网络

36
00:01:59,400 --> 00:02:05,000
其实它是一个复杂的带有参数的高度的非凸函数

37
00:02:05,000 --> 00:02:07,525
数据高为平面可以是平的

38
00:02:07,525 --> 00:02:09,000
那更多的是凹的

39
00:02:09,000 --> 00:02:12,725
希望能够找到凹下去的最低点

40
00:02:12,725 --> 00:02:15,475
找到最低点就是损失值

41
00:02:15,475 --> 00:02:17,000
最小的损失值

42
00:02:17,000 --> 00:02:22,840
那中间就是通过这些带参数的进行一阶梯度求导

43
00:02:22,840 --> 00:02:24,000
然后迭代的更新导数

44
00:02:24,000 --> 00:02:26,750
通过优化器去更新参数

45
00:02:26,750 --> 00:02:29,880
使得整个模型找到最小的点

46
00:02:29,880 --> 00:02:32,000
也去找到数据的鞍点

47
00:02:32,000 --> 00:02:37,000
那这个就是深度学习训练流程所代表的数学含义

48
00:02:37,000 --> 00:02:39,650
回到求导里面

49
00:02:39,650 --> 00:02:43,000
其实求导是一个经典的问题 

50
00:02:43,000 --> 00:02:48,440
深度学习的核心就是计算网络模型的参数

51
00:02:48,440 --> 00:02:50,650
刚才讲了权重的参数

52
00:02:50,650 --> 00:02:54,000
然后不断的更新权重参数的梯度

53
00:02:54,000 --> 00:02:55,675
最后以损失函数

54
00:02:55,675 --> 00:02:59,000
输入的是X,然后label就是Y

55
00:02:59,000 --> 00:03:01,350
最后通过更新梯度

56
00:03:01,350 --> 00:03:05,000
然后求得L(W),就是损失值

57
00:03:05,000 --> 00:03:10,506
在这里面,自动微分就是通过一些原子的操作

58
00:03:10,506 --> 00:03:12,550
构造一个比较复杂的前向计算的程序

59
00:03:12,550 --> 00:03:14,000
也就是神经网络

60
00:03:14,000 --> 00:03:16,450
自动微分关注的一个点

61
00:03:16,450 --> 00:03:21,000
就是高效的自动的去生成一个反向的计算程序

62
00:03:21,000 --> 00:03:22,650
就是有一个前向

63
00:03:22,650 --> 00:03:24,275
还构建一个反向

64
00:03:24,275 --> 00:03:26,000
就跟刚才的这个图一样

65
00:03:26,000 --> 00:03:29,200
这个前向是人工的去构造的

66
00:03:29,200 --> 00:03:31,370
反向这个非常复杂

67
00:03:31,370 --> 00:03:34,000
希望系统自动的帮去构建

68
00:03:38,000 --> 00:03:42,000
左边的这个程序就是LeNet这个网络模型

69
00:03:42,000 --> 00:03:45,500
首先人工的去定义了一些算子

70
00:03:45,500 --> 00:03:48,000
就是神经网络的算子

71
00:03:48,000 --> 00:03:49,525
接着把这些算子串起来

72
00:03:49,525 --> 00:03:51,500
然后拼成一个正向的网络模型

73
00:03:51,500 --> 00:03:53,000
就是forward

74
00:03:53,000 --> 00:03:54,550
这个是PyTorch的代码

75
00:03:54,550 --> 00:03:56,000
里面用forward

76
00:03:56,000 --> 00:03:58,000
而MindSpore用construct

77
00:03:58,000 --> 00:04:03,000
基本上这个网络模型的构建的过程或者其实是差不多的

78
00:04:03,000 --> 00:04:05,375
那这个网络模型只是一个正向

79
00:04:05,375 --> 00:04:08,550
就是我的forward只是一个正向

80
00:04:08,550 --> 00:04:11,000
也就是构建了我右边的正向的模型

81
00:04:11,000 --> 00:04:13,000
那反向这个怎么办呢

82
00:04:13,000 --> 00:04:16,975
反向这个希望通过AI框架的自动微分

83
00:04:16,975 --> 00:04:20,000
帮自动的去构建反向的程序

84
00:04:20,000 --> 00:04:22,625
那反向的程序可能并不是像

85
00:04:22,625 --> 00:04:26,000
左边看到的这种形式上的代码

86
00:04:26,000 --> 00:04:28,275
无论是正向还是反向

87
00:04:28,275 --> 00:04:32,000
构建的是一个DAG的有向无环图

88
00:04:32,000 --> 00:04:35,675
通过这个有向无还图或者叫做计算图

89
00:04:35,675 --> 00:04:37,700
去表示正向

90
00:04:37,700 --> 00:04:39,000
表示反向

91
00:04:39,000 --> 00:04:42,525
接下来回顾一下在自动微分系列里面

92
00:04:42,525 --> 00:04:45,300
讲到的几个最简单的计算机

93
00:04:45,300 --> 00:04:47,000
去实现微分的方式

94
00:04:47,000 --> 00:04:50,000
首先第一个就是符号微分

95
00:04:50,000 --> 00:04:54,850
那符号微分最简单的下面两条式就是

96
00:04:54,850 --> 00:04:56,775
念式求导法则的展开或者导数表

97
00:04:56,775 --> 00:04:58,000
这两个字打错了

98
00:04:58,000 --> 00:05:01,300
通过求导法则或者导数的变换公式

99
00:05:01,300 --> 00:05:05,000
然后精确的去计算函数所对应的导数

100
00:05:05,000 --> 00:05:08,675
那优势就是精确值非常高

101
00:05:08,675 --> 00:05:12,290
缺点就是表达式不断的膨胀

102
00:05:12,290 --> 00:05:14,000
因为我要人工或者计算机系统自动的去展开

103
00:05:14,000 --> 00:05:18,000
而这个缺点在深度学习里面会引起比较大的一些问题

104
00:05:18,000 --> 00:05:21,600
首先深度学习的网络模型非常大

105
00:05:21,600 --> 00:05:23,000
层数非常深

106
00:05:23,000 --> 00:05:29,253
这会就导致导数会变得非常复杂

107
00:05:29,253 --> 00:05:32,000
就是函数f(x)可能是急剧性的膨胀急剧的变大

108
00:05:32,000 --> 00:05:33,700
最后就衍生一个问题

109
00:05:33,700 --> 00:05:35,000
就是难以高效的求解

110
00:05:35,000 --> 00:05:38,050
另外还有第二个缺点就

111
00:05:38,050 --> 00:05:43,860
是有部分的算子没办法通过求导去解决的

112
00:05:43,860 --> 00:05:46,000
例如Relu,Switch这些比较特殊的神经网络所对应的算子

113
00:05:46,000 --> 00:05:48,750
那还有第二种实现方式

114
00:05:48,750 --> 00:05:50,325
就是数值微分

115
00:05:50,325 --> 00:05:53,000
数值微分就是用有限差分方式

116
00:05:53,000 --> 00:05:55,750
这个就是差分方式进行近似

117
00:05:55,750 --> 00:05:57,875
优势就是比较好实现

118
00:05:57,875 --> 00:06:01,000
缺点就是结果不精确,复杂度高

119
00:06:01,000 --> 00:06:02,800
那针对这个问题呢

120
00:06:02,800 --> 00:06:06,300
在深度学习里面或者在AI框架里面

121
00:06:06,300 --> 00:06:09,000
就会因为数值的阶段和近似问题

122
00:06:09,000 --> 00:06:12,000
导致没办法得到一个精确的导数

123
00:06:12,000 --> 00:06:15,875
因为去求导数的时候经常都要用浮点运算

124
00:06:15,875 --> 00:06:19,350
最低可能要用float16进行运算

125
00:06:19,350 --> 00:06:23,300
可能部分算子float16或者混合精度都不达标

126
00:06:23,300 --> 00:06:25,000
只能用FP32执行

127
00:06:25,000 --> 00:06:29,000
那这个时候阶段和近似的问题就得不到解了 

128
00:06:29,000 --> 00:06:33,800
所以就有很多研究学者去研究自动微分

129
00:06:33,800 --> 00:06:38,000
怎么通过数值计算把有限的导数进行表示出来

130
00:06:38,000 --> 00:06:44,280
那这个时候就推出一个表达式追踪Evaluation Trace

131
00:06:44,280 --> 00:06:46,000
去跟踪计算的整个中间变量的过程

132
00:06:47,000 --> 00:06:52,125
那下面看看刚才提到一个很重要的概念

133
00:06:52,125 --> 00:06:54,000
就是中间变量

134
00:06:54,000 --> 00:06:57,975
那中间变量希望引入这个中间变量

135
00:06:57,975 --> 00:07:02,400
把这些中间变量分解成一系列的基本的函数

136
00:07:02,400 --> 00:07:05,000
将这些基本的函数构造成一个计算图

137
00:07:05,000 --> 00:07:09,040
那下面来看看一个熟悉的例子

138
00:07:09,040 --> 00:07:09,925
 这个例子呢

139
00:07:09,925 --> 00:07:11,500
贯穿自动微分系列了

140
00:07:11,500 --> 00:07:12,450
 还有AI框架了

141
00:07:12,450 --> 00:07:13,175
基础系列了

142
00:07:13,175 --> 00:07:15,000
现在来到计算图系列

143
00:07:15,000 --> 00:07:17,500
那这个例子还是这个例子

144
00:07:17,500 --> 00:07:23,145
所谓的中间变量就是我的x1x2是原始的输入

145
00:07:23,145 --> 00:07:25,650
那最后的输出呢是这个fx1x2

146
00:07:25,650 --> 00:07:28,075
那中间变量就是这些圈圈 

147
00:07:28,075 --> 00:07:32,450
从v-1到v5都是中间变量

148
00:07:32,450 --> 00:07:34,750
每经过一条边一条运算

149
00:07:34,750 --> 00:07:37,500
都会产生一个中间变量

150
00:07:37,850 --> 00:07:39,750
把中间变量保存起来

151
00:07:39,750 --> 00:07:42,000
然后不断的运算往下执行就可以看到

152
00:07:43,000 --> 00:07:45,450
这个呢就是Forward Trace

153
00:07:45,450 --> 00:07:47,250
就是正向的一个计算公式

154
00:07:47,250 --> 00:07:48,000
从输入到输出

155
00:07:48,000 --> 00:07:50,650
那接下来有个很重要的概念

156
00:07:50,650 --> 00:07:53,750
也是在自动微分里面提过的

157
00:07:53,750 --> 00:07:56,500
引入的中间变量除了正向

158
00:07:56,500 --> 00:08:00,000
其实这些灰色的也是中间变量

159
00:08:00,000 --> 00:08:03,950
这些中间变量就对应于这个v5的delt

160
00:08:03,950 --> 00:08:08,725
v4的delta,v2的delta,v3的delta

161
00:08:08,725 --> 00:08:12,025
你要不断的把这些中间变量全都保存起来

162
00:08:12,025 --> 00:08:16,125
通过这些虚线立向的去求得到整个图

163
00:08:16,125 --> 00:08:18,150
这个叫做计算图的导数

164
00:08:18,150 --> 00:08:22,000
通过这种方式去构造正向和反向的自动微分

165
00:08:23,000 --> 00:08:26,650
那下面更加详细的去展开一下这个图

166
00:08:26,650 --> 00:08:30,000
跟神经网络的图其实是一样的

167
00:08:30,000 --> 00:08:32,325
把这些圈叫做神经元

168
00:08:32,325 --> 00:08:34,075
当作它是神经元

169
00:08:34,075 --> 00:08:38,000
那这些中间的边把它当成一个计算单元

170
00:08:38,000 --> 00:08:40,550
现在看一下其中有一层

171
00:08:40,550 --> 00:08:43,825
把神经网络的一层拿出来

172
00:08:43,825 --> 00:08:46,400
那这一层它的计算是G(x)

173
00:08:46,400 --> 00:08:49,025
不管它是加减乘除也好

174
00:08:49,025 --> 00:08:51,000
你把它当成一个函数

175
00:08:51,000 --> 00:08:53,350
我的输入是X输出是Y

176
00:08:53,350 --> 00:08:55,450
也就代表左边输进去的 

177
00:08:55,450 --> 00:08:57,325
这个是X就是V1V2

178
00:08:57,325 --> 00:09:00,000
它是一个X的向量或者张量

179
00:09:00,000 --> 00:09:02,325
输出就是V4V3

180
00:09:02,325 --> 00:09:06,000
这等于Y输出的张量

181
00:09:06,000 --> 00:09:10,575
为了在数学上更好的去表达Y对X的导数

182
00:09:10,575 --> 00:09:14,350
这里面引入了一个雅可比矩阵

183
00:09:14,350 --> 00:09:17,000
Y对X,Y对所有X的偏导

184
00:09:18,000 --> 00:09:20,100
这个是正向的计算

185
00:09:20,100 --> 00:09:24,025
反向的时候计算完的F(x1)的导数

186
00:09:24,025 --> 00:09:26,000
然后不断的往后传

187
00:09:26,000 --> 00:09:29,180
那在V4V3的时候

188
00:09:29,180 --> 00:09:31,900
反向的时候其实传的是

189
00:09:31,900 --> 00:09:34,780
一个反向传的时候其实是一个Vdelta

190
00:09:35,275 --> 00:09:37,050
现在暂且叫它Vdelta

191
00:09:37,050 --> 00:09:40,000
后面把它叫做Vector-Jacobian的乘积

192
00:09:41,000 --> 00:09:45,725
那往下反向传的时候其实是delta f到V3的导数

193
00:09:45,725 --> 00:09:47,950
delta f对V4的导数

194
00:09:47,950 --> 00:09:51,100
也就是我从L这个L这个损失函数

195
00:09:51,100 --> 00:09:54,025
对每一个反向的反向的输入的导数

196
00:09:54,025 --> 00:09:57,000
就是Vector-Jacobian的乘积

197
00:09:57,000 --> 00:09:57,900
最后呢

198
00:09:57,900 --> 00:10:00,250
通过去计算里面的每个运算

199
00:10:00,250 --> 00:10:02,700
然后乘以雅可比乘积

200
00:10:02,700 --> 00:10:03,475
之后呢

201
00:10:03,475 --> 00:10:06,000
就得到下一次的导数的输出

202
00:10:06,000 --> 00:10:10,125
AI框架里面并不会去存整个雅可比矩阵

203
00:10:10,125 --> 00:10:12,075
因为雅可比矩阵太大了

204
00:10:12,075 --> 00:10:17,000
而需要的,用到的是Vector,雅可比的乘积

205
00:10:17,000 --> 00:10:18,500
所以每一次都是

206
00:10:18,500 --> 00:10:22,700
在AI框架里面存的是雅可比的乘积

207
00:10:22,700 --> 00:10:23,850
当做权重参数

208
00:10:23,850 --> 00:10:25,375
然后通过计算得到雅可比

209
00:10:25,375 --> 00:10:27,700
然后最后成为Vector。

210
00:10:27,700 --> 00:10:30,375
这个就是自动微分的一个概念

211
00:10:30,375 --> 00:10:32,325
那其实再展开

212
00:10:32,325 --> 00:10:33,650
再展开细一点

213
00:10:33,650 --> 00:10:37,100
就是AI框架跟自动微分的关系

214
00:10:37,100 --> 00:10:41,000
刚才已经讲了把它当做一个神经网络的层

215
00:10:41,000 --> 00:10:44,075
那其实呢,在做AI框架的时候

216
00:10:44,075 --> 00:10:48,400
首先需要去注册前向的计算节点和导数的计算节点

217
00:10:48,400 --> 00:10:54,000
就是正向的计算节点V4和反向的计算节点deltaf除以V4

218
00:10:54,000 --> 00:10:55,650
那注册完这个呢

219
00:10:55,650 --> 00:10:59,300
就是系统已经有了这两个节点之后呢

220
00:10:59,300 --> 00:11:00,300
这个节点呢

221
00:11:00,300 --> 00:11:04,000
就会接受输入计算这个节点的输出

222
00:11:04,000 --> 00:11:05,925
那反向也是一样的

223
00:11:05,925 --> 00:11:10,425
反向我接受刚才的向量雅可比的乘积作用的输入 

224
00:11:10,425 --> 00:11:12,025
然后计算输出

225
00:11:12,025 --> 00:11:14,575
每次呢计算这个输入的乘积

226
00:11:14,575 --> 00:11:18,000
然后就得到上一层的导数,就这个

227
00:11:19,925 --> 00:11:24,125
其实刚才讲的都是在当前的这个图

228
00:11:24,575 --> 00:11:26,775
在上一节不是已经说过了吗

229
00:11:26,775 --> 00:11:30,625
实际上计算图是一个有向无环图

230
00:11:30,625 --> 00:11:33,400
图里面的展开是这样的

231
00:11:33,400 --> 00:11:34,550
就是这个呢

232
00:11:34,550 --> 00:11:37,000
黑色的这个正向,其实是正向的

233
00:11:37,000 --> 00:11:38,900
但是我反向的时候

234
00:11:38,900 --> 00:11:41,150
其实我是产生新的节点的

235
00:11:41,150 --> 00:11:43,250
反向就是红色的新的节点 

236
00:11:43,250 --> 00:11:44,825
但是我反向的时候

237
00:11:44,825 --> 00:11:47,400
我还会用到红就橙色的这一条

238
00:11:47,400 --> 00:11:50,200
我还会用到正向的这些节点的变量

239
00:11:50,200 --> 00:11:53,000
所以需要把中间变量都存起来

240
00:11:53,000 --> 00:11:56,425
这个时候为什么经常说非常吃内存

241
00:11:56,425 --> 00:11:58,950
非常吃显存,就是因为这个概念、

242
00:11:58,950 --> 00:12:02,550
神经网络的图存下了大量的中间变量

243
00:12:02,550 --> 00:12:06,075
而这个图呢,就是对应于反向的时候

244
00:12:06,075 --> 00:12:07,875
有V5

245
00:12:07,875 --> 00:12:09,600
然后V5的时候可能还会

246
00:12:09,600 --> 00:12:11,025
求V2的时候

247
00:12:11,025 --> 00:12:15,000
V0的时候,还可能需要V0,是吧,V-0

248
00:12:16,000 --> 00:12:20,075
其实会不断的去用正向的相关的一些计算

249
00:12:20,075 --> 00:12:22,075
你要通过一个有向无环图

250
00:12:22,075 --> 00:12:25,000
去把整个正向的反向的记录下来

251
00:12:26,000 --> 00:12:27,125
这里面呢

252
00:12:27,125 --> 00:12:28,975
我想提出几个问题

253
00:12:28,975 --> 00:12:31,500
跟大家一起去思考一下

254
00:12:31,500 --> 00:12:33,450
就是正向的模式和反向的模式

255
00:12:33,450 --> 00:12:36,425
这两个模式都是自动微分的模式

256
00:12:36,425 --> 00:12:38,000
计算量是否相同呢?

257
00:12:38,825 --> 00:12:40,050
那第二个呢

258
00:12:40,050 --> 00:12:42,675
就是AI框架或者深度学习用户

259
00:12:42,675 --> 00:12:45,000
为什么大部分都用反向模式呢?

260
00:12:45,000 --> 00:12:48,925
其实之前在自动微分系列里面去讲的时候

261
00:12:48,925 --> 00:12:51,245
已经简单的通过一个手把手

262
00:12:51,245 --> 00:12:54,025
教大家实现一个Pytorch AI框架

263
00:12:54,025 --> 00:12:56,000
已经初步的展示了如何去实现的

264
00:12:57,000 --> 00:12:58,425
具体的实现方式有三种

265
00:12:58,425 --> 00:13:00,000
这里面总结了一下

266
00:13:00,000 --> 00:13:02,650
首先呢,就是通过前向的计算

267
00:13:02,650 --> 00:13:05,875
在前向计算的时候保留中间的结果

268
00:13:05,875 --> 00:13:08,650
那接着呢,根据反向模式的原理呢

269
00:13:08,650 --> 00:13:10,375
一次计算中间的导数

270
00:13:10,375 --> 00:13:13,400
就是刚才讲的每次计算中间的导数

271
00:13:13,400 --> 00:13:15,000
记录正向的一个结果

272
00:13:16,000 --> 00:13:18,100
接着呢,通过表达式追踪

273
00:13:18,100 --> 00:13:20,975
那中间的这一等号右边的这个呢

274
00:13:20,975 --> 00:13:22,675
就是表达式

275
00:13:22,675 --> 00:13:24,950
去跟踪记录每一个表达式

276
00:13:24,950 --> 00:13:26,275
每一条表达式

277
00:13:26,275 --> 00:13:28,000
最后把这些过程存起来

278
00:13:28,000 --> 00:13:33,695
问题就是需要保存大量的计算结果和中间变量结果

279
00:13:33,695 --> 00:13:34,325
但是呢

280
00:13:34,325 --> 00:13:36,400
好处就是方便跟踪整个计算过程

281
00:13:36,400 --> 00:13:39,000
就每一条每个过程我都有,它不会偷工减料

282
00:13:40,000 --> 00:13:40,975
使用这种方式

283
00:13:40,975 --> 00:13:43,975
最出名的一个AI框架呢,就是Pytorch

284
00:13:43,975 --> 00:13:46,000
大家用的,可能听的也比较多了

285
00:13:47,000 --> 00:13:48,460
那第二种呢

286
00:13:48,460 --> 00:13:53,025
就是无论是MindSpore和TensorFlow都是使用这种方式

287
00:13:53,025 --> 00:13:55,325
将导数的计算表示成为一个计算图

288
00:13:55,325 --> 00:13:58,000
那计算图就是这个系列里面的重点

289
00:13:59,000 --> 00:14:03,375
然后通过这个计算图,叫做IR GraphIR

290
00:14:03,375 --> 00:14:05,585
对计算图进行一个统一的表示

291
00:14:05,585 --> 00:14:08,375
非常方便计算机去进行操作

292
00:14:08,375 --> 00:14:12,000
但是呢,它可能并不是非常符合人去看

293
00:14:12,000 --> 00:14:13,650
因为它变成一个图

294
00:14:13,650 --> 00:14:15,700
只是计算机能理解的图

295
00:14:15,700 --> 00:14:17,775
这个图随着神经网络的构建

296
00:14:17,775 --> 00:14:19,325
它会急剧的膨胀

297
00:14:19,325 --> 00:14:21,875
包括反向它也会膨胀的非常厉害

298
00:14:21,875 --> 00:14:23,000
非常不利于看

299
00:14:24,000 --> 00:14:24,700
所以呢

300
00:14:24,700 --> 00:14:29,225
它的缺点就是不利于调试跟踪和数学表达过程的一个表示

301
00:14:29,225 --> 00:14:31,800
那优点就是非常方便全局的优化

302
00:14:31,800 --> 00:14:33,625
非常节省内存

303
00:14:33,625 --> 00:14:35,575
所以现在在部署的阶段呢

304
00:14:35,575 --> 00:14:38,375
还是有很多人去用TensorFlow

305
00:14:38,375 --> 00:14:42,275
而现在要做一些又可以训练又可以部署的情况下呢

306
00:14:42,275 --> 00:14:45,675
可能会有越来越多的用户去采用学习

307
00:14:45,950 --> 00:14:47,950
MindSpore头包。

308
00:14:50,125 --> 00:14:53,725
刚才只是讲了具体的一个实现的概念

309
00:14:53,725 --> 00:14:57,575
就是concept,其实在具体的实现呢, 

310
00:14:57,575 --> 00:14:58,775
是做一个优化的pass

311
00:14:58,775 --> 00:15:00,775
做pass呢是编辑器的一个概念

312
00:15:00,775 --> 00:15:03,225
通过优化的pass去实现的

313
00:15:03,225 --> 00:15:06,000
就在编辑器里面去实现的

314
00:15:07,080 --> 00:15:09,700
大家搞清楚基本概念和关系

315
00:15:09,950 --> 00:15:12,000
后面在实现的过程当中就很好理解了

316
00:15:13,000 --> 00:15:15,275
那首先呢就要给进正向的数据流图

317
00:15:15,275 --> 00:15:18,000
那正向的数据流图比较好构建

318
00:15:18,000 --> 00:15:19,950
正向的数据流图呢比较好构建\

319
00:15:19,950 --> 00:15:21,800
就通过这个forward

320
00:15:21,800 --> 00:15:23,950
然后对这个forward进行解析

321
00:15:23,950 --> 00:15:26,975
就可以把它变成一个正向的数据流图

322
00:15:27,275 --> 00:15:29,675
然后呢有了正向的数据流图之后呢,

323
00:15:29,675 --> 00:15:32,300
就会以损失函数作为根节点

324
00:15:32,635 --> 00:15:34,335
就是这个loss作为根节点

325
00:15:35,000 --> 00:15:38,550
然后通过广度优先遍历 前向的数据流图, 

326
00:15:38,550 --> 00:15:40,000
通过对偶的关系

327
00:15:40,000 --> 00:15:43,025
可以看到基本上都是队伍对偶的关系

328
00:15:43,025 --> 00:15:44,700
求出反向的计算图

329
00:15:45,000 --> 00:15:47,025
现在来review一下

330
00:15:47,025 --> 00:15:50,425
首先呢模型表示就是通过前端的代码

331
00:15:50,425 --> 00:15:52,350
然后变成一个计算图

332
00:15:52,350 --> 00:15:54,000
叫做computation graph

333
00:15:54,000 --> 00:15:55,475
表示完之后呢

334
00:15:55,475 --> 00:15:58,175
后面呢就会去实行自动微分

335
00:15:58,175 --> 00:16:01,225
然后呢有了这个正向的计算图之后呢

336
00:16:01,225 --> 00:16:05,000
就会基于反向模式的原理构建反向的计算图

337
00:16:06,000 --> 00:16:08,125
有了正反向的计算图

338
00:16:08,125 --> 00:16:13,000
才能够成为一个完整的深度学习训练之前执行的计算图

339
00:16:14,000 --> 00:16:15,850
今天回顾一下

340
00:16:15,850 --> 00:16:19,300
了解了神经网络或者AI系统当中

341
00:16:19,300 --> 00:16:22,000
训练流程跟微分之间的一个关系

342
00:16:22,600 --> 00:16:24,550
第二个呢就是回顾了

343
00:16:24,550 --> 00:16:28,525
自动微分的正反向模式和计算图的自动微分,

344
00:16:28,525 --> 00:16:30,700
就计算图跟自动微分的关系,

345
00:16:30,700 --> 00:16:33,100
已经搞清楚了

346
00:16:33,775 --> 00:16:35,575
那最后呢还了解了 

347
00:16:35,575 --> 00:16:37,300
自动微分在深度学习是

348
00:16:37,300 --> 00:16:40,775
通过一个优化的pass在编译器里面去实现的

