1
00:00:00,000 --> 00:00:04,900
字幕校对:米哈游天下第一

2
00:00:04,900 --> 00:00:07,680
哈喽大家好,欢迎来到ZOMI的课堂

3
00:00:07,680 --> 00:00:12,240
那这一节主要是跟大家一起去分享AI框架的编程范式

4
00:00:12,240 --> 00:00:16,080
这也是作为AI框架基础的最后一节内容里面

5
00:00:16,080 --> 00:00:22,080
这么早就开完饭,你数什么呢?

6
00:00:22,080 --> 00:00:24,520
那聊到AI编程范式

7
00:00:24,520 --> 00:00:28,560
其实一开始我也是不知道AI的编程范式

8
00:00:28,560 --> 00:00:32,000
声明式编程和命令式编程到底有什么区别

9
00:00:32,000 --> 00:00:35,080
作为一个算法工程师或者程序员

10
00:00:35,080 --> 00:00:39,760
其实一开始我对编程范式的理解并不是非常深入

11
00:00:39,760 --> 00:00:43,400
最多就是知道在C++编写代码的时候

12
00:00:43,400 --> 00:00:46,840
可能会使用面向对象的这种编程范式

13
00:00:46,840 --> 00:00:50,960
现在来看看编程范式维基百科的一个解释

14
00:00:50,960 --> 00:00:53,200
其实我也不是很懂刚查的

15
00:00:53,200 --> 00:00:56,400
编程范式英文叫做Programming

16
00:00:56,720 --> 00:01:01,840
这个英文是Programming Paradigm

17
00:01:01,840 --> 00:01:05,800
是指软件工程的一类典型编程风格

18
00:01:05,800 --> 00:01:07,520
哎,这有点意思

19
00:01:07,520 --> 00:01:09,480
就是典型的编程风格

20
00:01:13,200 --> 00:01:15,480
面向AI或者深度学习

21
00:01:15,480 --> 00:01:18,080
其实有两种编程风格

22
00:01:18,080 --> 00:01:19,920
第一种是声明式编程

23
00:01:19,920 --> 00:01:22,040
第二种是命令式编程

24
00:01:22,040 --> 00:01:23,600
在今天的课程里面

25
00:01:23,600 --> 00:01:25,880
我希望跟大家一起去学习

26
00:01:25,880 --> 00:01:27,760
什么是声明式编程

27
00:01:27,760 --> 00:01:30,320
什么是命令式的编程风格

28
00:01:30,320 --> 00:01:34,480
现在来看看最简单的或者最原始的

29
00:01:34,480 --> 00:01:37,360
也在上一节课里面去提到的

30
00:01:37,360 --> 00:01:39,240
假设我现在从Numpy

31
00:01:39,240 --> 00:01:41,800
就是直接用Python的一个最基础的库

32
00:01:41,800 --> 00:01:45,160
去实现一个简单的先乘后加

33
00:01:45,160 --> 00:01:47,840
然后再求和的一个简单的操作

34
00:01:47,840 --> 00:01:50,440
也就是作为一个最原始的神经元

35
00:01:50,560 --> 00:01:53,800
那这个时候我做了一些简单的计算之后

36
00:01:53,800 --> 00:01:56,720
现在我需要反向的去更新我的梯度

37
00:01:56,720 --> 00:01:58,640
那更新梯度的方式

38
00:01:58,640 --> 00:02:01,760
也是比较简单粗暴的直接写下来了

39
00:02:01,760 --> 00:02:04,760
这是最原始的AI的编程方法

40
00:02:06,040 --> 00:02:08,640
后来有了AI框架PyTorch之后

41
00:02:08,640 --> 00:02:09,840
首先第一步

42
00:02:09,840 --> 00:02:11,480
需要去声明

43
00:02:11,480 --> 00:02:14,400
现在我有几个张量XYZ

44
00:02:14,400 --> 00:02:17,280
那接着我对这些张量进行计算

45
00:02:17,280 --> 00:02:20,240
所以我X乘以Y,A加上Z

46
00:02:20,280 --> 00:02:21,640
这么一个过程

47
00:02:21,640 --> 00:02:23,840
就看到右边的这个图

48
00:02:23,840 --> 00:02:25,120
这个动态的图

49
00:02:25,120 --> 00:02:27,200
把刚才的XYZ

50
00:02:27,200 --> 00:02:30,920
然后通过一个计算图进行表达起来

51
00:02:30,920 --> 00:02:32,640
那既然是AI框架

52
00:02:32,640 --> 00:02:35,200
它肯定要帮做一些反向

53
00:02:35,200 --> 00:02:37,240
或者在上一个系列里面

54
00:02:37,240 --> 00:02:39,440
去讲到的自动微分的操作

55
00:02:39,440 --> 00:02:41,440
那有了这个backward

56
00:02:41,440 --> 00:02:45,000
然后去试着retain_graph=True的时候

57
00:02:45,000 --> 00:02:48,520
就会自动的帮去构建反向的图

58
00:02:48,520 --> 00:02:50,360
再看一次动画

59
00:02:50,360 --> 00:02:53,880
就会自动的帮把反向的图构建起来

60
00:02:53,880 --> 00:02:56,880
那这个就是AI框架编程

61
00:02:56,880 --> 00:03:00,280
最基本或者最典型的一种方式

62
00:03:00,280 --> 00:03:03,120
有了这种新的编程方式之后

63
00:03:03,120 --> 00:03:05,800
后来AI框架慢慢的去演变

64
00:03:05,800 --> 00:03:08,920
然后形成了一个比较系统的一个图

65
00:03:08,920 --> 00:03:10,080
那右边的这个图

66
00:03:10,080 --> 00:03:13,000
就是上一节展开的粉粉绿绿的图

67
00:03:13,000 --> 00:03:15,040
只是上一节里面做了一个分层

68
00:03:15,040 --> 00:03:17,320
这里面就没有做太多的分层

69
00:03:17,320 --> 00:03:21,360
而是从上到下看一下整个AI系统或者AI框架

70
00:03:21,360 --> 00:03:25,280
怎么做编程风格或者编程代码之间的一个区分

71
00:03:25,280 --> 00:03:29,200
现在最明显的就是区分前端和后端

72
00:03:29,200 --> 00:03:32,080
那可能统一表是优化层

73
00:03:32,080 --> 00:03:33,680
运行时底层库

74
00:03:33,680 --> 00:03:37,280
这些都作为一个整体的后端去看待

75
00:03:37,280 --> 00:03:40,880
这也是公司或者我跟其他同事

76
00:03:40,880 --> 00:03:42,440
在交流的过程当中

77
00:03:42,440 --> 00:03:44,640
大家都比较认可的一种方式

78
00:03:44,680 --> 00:03:47,280
前端就会一些高层次的语言

79
00:03:47,280 --> 00:03:49,000
例如python作为前端

80
00:03:49,000 --> 00:03:50,920
那可能有一些会使用lua

81
00:03:50,920 --> 00:03:53,480
还有R还有C++或者Javascript

82
00:03:53,480 --> 00:03:55,280
作为前端语言

83
00:03:55,280 --> 00:03:58,040
这个就是对上的API层

84
00:03:58,040 --> 00:04:01,240
那后端就会提供一些底层的编程模型

85
00:04:01,240 --> 00:04:02,760
编程语言的开发

86
00:04:02,760 --> 00:04:06,280
后端可能还会提供一些高性能可复用的模块

87
00:04:06,280 --> 00:04:11,320
或者通过前端的方式去驱动后端怎么去执行

88
00:04:11,320 --> 00:04:14,080
也就是说我前端的表达

89
00:04:14,080 --> 00:04:16,600
其实跟我的后端是相关的

90
00:04:16,600 --> 00:04:20,880
而不是说前端的表达跟后端是完全解耦的

91
00:04:20,880 --> 00:04:23,880
这里面有一个叫做统一表示

92
00:04:23,880 --> 00:04:26,000
在第二代神经网络里面

93
00:04:26,000 --> 00:04:29,280
可能更多的去使用计算图或者数据流图

94
00:04:29,280 --> 00:04:30,920
作为统一的表示

95
00:04:30,920 --> 00:04:34,360
既然有了这个计算图进行统一表示

96
00:04:34,360 --> 00:04:37,720
肯定前端要适配好这个统一的表示

97
00:04:37,720 --> 00:04:39,280
后端下面的优化

98
00:04:39,280 --> 00:04:42,160
也需要根据我的计算图进行优化

99
00:04:42,160 --> 00:04:45,280
所以整个编程的风格编程的范式

100
00:04:45,280 --> 00:04:47,760
不仅仅是指前端的代码

101
00:04:47,760 --> 00:04:50,200
或者前端的高层语言的表示

102
00:04:50,200 --> 00:04:52,760
更多的是指整个AI框架

103
00:04:52,760 --> 00:04:54,000
我的设计理念

104
00:04:54,000 --> 00:04:55,280
我的架构理念

105
00:04:57,040 --> 00:05:01,240
下面以PyTorch和TensorFlow两个框架为例

106
00:05:01,240 --> 00:05:05,600
看一下什么是命令式编程和声明式编程

107
00:05:05,600 --> 00:05:07,360
两种不同的编程方式

108
00:05:08,200 --> 00:05:09,960
PyTorch最特别的一点

109
00:05:10,000 --> 00:05:13,240
就是使用动态图去表示它的神经网络

110
00:05:13,240 --> 00:05:16,920
然后使得整个框架的应用性非常好

111
00:05:16,920 --> 00:05:18,720
但是我个人认为

112
00:05:18,720 --> 00:05:21,400
其实并不完全是因为动态图

113
00:05:21,400 --> 00:05:25,080
导致它的AI框架的应用性特别高

114
00:05:25,080 --> 00:05:28,560
更多的是因为它的命令式编程的方式

115
00:05:28,560 --> 00:05:31,440
导致它的AI框架的应用性特别高

116
00:05:31,440 --> 00:05:33,480
来看一看了命令式编程

117
00:05:33,480 --> 00:05:34,640
它的英文又叫做

118
00:05:34,640 --> 00:05:36,920
Imperative Programming

119
00:05:36,920 --> 00:05:37,920
那在英文里面

120
00:05:37,920 --> 00:05:39,320
还有另外一种表述

121
00:05:39,320 --> 00:05:41,120
叫做define-by-run

122
00:05:41,120 --> 00:05:44,120
也就是程序定义完之后就去执行了

123
00:05:44,120 --> 00:05:46,040
那这种执行方式

124
00:05:46,040 --> 00:05:48,480
在AI框架的一个架构图里面

125
00:05:48,480 --> 00:05:49,800
是比较直接的

126
00:05:49,800 --> 00:05:53,080
也就是我前端通过前端的一些语言

127
00:05:53,080 --> 00:05:55,320
高级语言去表示完之后

128
00:05:55,320 --> 00:05:58,040
然后会把它变成一个计算图

129
00:05:58,040 --> 00:05:59,800
这是一个概念上的计算图

130
00:05:59,800 --> 00:06:02,160
而不是真正意义上的计算图

131
00:06:02,160 --> 00:06:05,440
然后程序就会做一些自动微分

132
00:06:05,440 --> 00:06:07,720
如果选择了自动微分auto grad

133
00:06:07,720 --> 00:06:09,400
那就可能会做自动微分

134
00:06:09,400 --> 00:06:10,240
那如果没选

135
00:06:10,240 --> 00:06:12,680
可能自动微分这一层都没掉了

136
00:06:12,680 --> 00:06:15,360
然后就去直接根据代码

137
00:06:15,360 --> 00:06:17,840
进行一个调度和执行

138
00:06:17,840 --> 00:06:18,680
就是这么简单

139
00:06:18,680 --> 00:06:20,560
就从上面直接下来了

140
00:06:20,560 --> 00:06:22,880
所以会用前端的语言

141
00:06:22,880 --> 00:06:25,760
直接去驱动后端的算子执行

142
00:06:25,760 --> 00:06:27,080
非常直接

143
00:06:27,080 --> 00:06:29,160
我定义了一个乘号

144
00:06:29,160 --> 00:06:29,760
在这里面

145
00:06:29,760 --> 00:06:32,400
我定义了一个a = x * y

146
00:06:32,400 --> 00:06:34,440
b = a + z之后

147
00:06:34,440 --> 00:06:36,960
程序把这个图定义下来

148
00:06:36,960 --> 00:06:38,800
变成一个计算图之后

149
00:06:38,800 --> 00:06:42,000
也是根据我a = x * y

150
00:06:42,000 --> 00:06:43,680
先执行完这个计算

151
00:06:43,680 --> 00:06:45,600
然后再执行这个计算

152
00:06:45,600 --> 00:06:49,480
直接是define-by-run的这种方式去执行的

153
00:06:49,480 --> 00:06:52,920
那它的一个特点就是非常方便

154
00:06:52,920 --> 00:06:55,320
我调试灵活度很高

155
00:06:55,320 --> 00:06:58,400
那缺点就是缺乏对算法的统一描述

156
00:06:58,400 --> 00:07:01,280
还有缺乏编译期间的优化

157
00:07:01,280 --> 00:07:03,520
那优点其实都很清楚了

158
00:07:03,520 --> 00:07:05,600
来聊聊缺点

159
00:07:05,600 --> 00:07:08,520
其实现在设计Mindspore的时候

160
00:07:08,520 --> 00:07:12,120
也会考虑到很多这方面的因素和原因

161
00:07:12,120 --> 00:07:15,880
首先动态图确实应用性很高

162
00:07:15,880 --> 00:07:18,480
但是如果要做高阶微分的时候

163
00:07:18,480 --> 00:07:20,960
如果没有了一个统一的表示

164
00:07:20,960 --> 00:07:22,840
对算法的统一表示

165
00:07:22,840 --> 00:07:24,200
虽然在PyTorch里面

166
00:07:24,200 --> 00:07:26,760
虽然说是说使用了一个计算图

167
00:07:26,760 --> 00:07:28,960
或者完整表示的一个图

168
00:07:28,960 --> 00:07:31,760
而是一个虚拟的统一表示的图

169
00:07:31,760 --> 00:07:34,160
这个图只是一个简单的概念

170
00:07:34,160 --> 00:07:37,160
在后面在真正的调度和执行的时候

171
00:07:37,160 --> 00:07:40,520
还是跟前端的语言相绑定的

172
00:07:40,520 --> 00:07:41,960
也就是刚才说的

173
00:07:41,960 --> 00:07:44,000
我定义完了一个语言之后

174
00:07:44,000 --> 00:07:46,200
我定义完一行代码程序之后

175
00:07:46,200 --> 00:07:48,280
我后面就会进行一个执行

176
00:07:48,280 --> 00:07:49,640
只是为了方便理解

177
00:07:49,640 --> 00:07:52,960
可能把它变成一个统一的表示的计算图

178
00:07:54,040 --> 00:07:57,040
那第二个刚才看到前面一个图

179
00:07:57,040 --> 00:07:59,440
其实有很多关于运行时的

180
00:07:59,440 --> 00:08:01,320
还有优化层的一些内容

181
00:08:01,320 --> 00:08:04,040
那个内核代码的优化和编译

182
00:08:04,160 --> 00:08:07,040
但是实际上到了PyTorch里面

183
00:08:07,040 --> 00:08:09,400
刚才的那些层数都没有了

184
00:08:09,400 --> 00:08:11,600
也就是缺乏编译器的优化

185
00:08:11,600 --> 00:08:12,480
编译的优化

186
00:08:12,600 --> 00:08:15,280
是对统一表示进行优化的

187
00:08:15,280 --> 00:08:16,480
因为计算机里面

188
00:08:16,480 --> 00:08:18,360
其实要做IR

189
00:08:18,360 --> 00:08:19,520
这个就是统一表示

190
00:08:19,520 --> 00:08:21,640
就是编译器的其中一个概念

191
00:08:21,640 --> 00:08:23,720
叫做IR中间表示

192
00:08:23,720 --> 00:08:25,920
那缺乏了编译器的统一的优化

193
00:08:25,920 --> 00:08:27,320
就会导致我的性能

194
00:08:27,320 --> 00:08:29,400
可能并不是非常友好

195
00:08:29,400 --> 00:08:31,680
所以面对于可能在更多

196
00:08:31,680 --> 00:08:33,600
设备上的去执行的时候

197
00:08:33,640 --> 00:08:36,240
PyTorch的性能并不是很高

198
00:08:36,240 --> 00:08:39,320
现在在很多推理部署平台

199
00:08:39,320 --> 00:08:41,400
都会有自己的一个框架

200
00:08:41,400 --> 00:08:43,000
或者我在推理部署的时候

201
00:08:43,320 --> 00:08:45,000
会把PyTorch的代码

202
00:08:45,000 --> 00:08:46,760
转成ONNX的代码

203
00:08:46,760 --> 00:08:49,920
然后ONNX在对接其他硬件平台

204
00:08:49,920 --> 00:08:51,840
这个就是动态图

205
00:08:51,840 --> 00:08:53,440
或者命令式编程

206
00:08:53,440 --> 00:08:56,160
PyTorch它带来的一些缺点

207
00:08:57,640 --> 00:09:00,680
下面来看看声明式编程

208
00:09:00,680 --> 00:09:02,200
声明式编程的英文

209
00:09:02,400 --> 00:09:05,120
叫做Declarative programming

210
00:09:05,120 --> 00:09:07,320
然后它的另外一种说法

211
00:09:07,320 --> 00:09:09,080
就是defined-and-run

212
00:09:09,080 --> 00:09:11,680
然后最出名的代表就是TensorFlow

213
00:09:11,680 --> 00:09:15,200
是基于一个完全的静态图去执行的

214
00:09:15,200 --> 00:09:16,400
静态图去执行

215
00:09:16,400 --> 00:09:18,800
就非常依赖于统一表示

216
00:09:18,800 --> 00:09:21,520
就是神经网络的中间表达计算图

217
00:09:21,520 --> 00:09:23,920
这一层前端跟后端

218
00:09:23,920 --> 00:09:24,800
不像PyTorch

219
00:09:24,800 --> 00:09:28,040
它们中间其实是耦合性是非常高的

220
00:09:28,040 --> 00:09:30,400
那PyTorch这种声明式编程

221
00:09:30,400 --> 00:09:31,760
或者静态图

222
00:09:31,760 --> 00:09:33,680
它的解耦程度是比较高的

223
00:09:33,680 --> 00:09:37,360
就是前端跟后端不完全依赖和绑定

224
00:09:37,360 --> 00:09:39,960
后端更多的是根据计算图

225
00:09:39,960 --> 00:09:42,080
进行一个优化和调度执行

226
00:09:42,080 --> 00:09:45,200
那前端可能更多的是对计算图

227
00:09:45,200 --> 00:09:46,880
进行一些表示

228
00:09:46,880 --> 00:09:48,760
定义了计算图之后

229
00:09:48,760 --> 00:09:50,360
再定义我的前端

230
00:09:50,360 --> 00:09:52,480
所以用户学TensorFlow的时候

231
00:09:52,480 --> 00:09:55,080
就会觉得为什么它这么难学

232
00:09:55,080 --> 00:09:58,320
为什么要学那么多额外的Python的知识

233
00:09:58,320 --> 00:10:00,560
这个就是它的一个问题了

234
00:10:00,560 --> 00:10:01,760
那在执行方面

235
00:10:01,760 --> 00:10:04,760
就是前端的语言表达不直接执行

236
00:10:04,760 --> 00:10:06,200
刚才编程式风格的

237
00:10:06,200 --> 00:10:07,680
就是我定义完前端之后

238
00:10:07,680 --> 00:10:08,920
后端直接执行

239
00:10:08,920 --> 00:10:10,640
现在不直接执行

240
00:10:10,640 --> 00:10:13,120
而是通过一个IR去表示

241
00:10:13,120 --> 00:10:15,200
就是统一的计算去表示

242
00:10:15,200 --> 00:10:17,240
那对数据流图进行优化

243
00:10:17,240 --> 00:10:20,000
在执行也就是刚才比PyTorch

244
00:10:20,000 --> 00:10:22,200
多了很多一个计算图的优化

245
00:10:22,200 --> 00:10:23,080
内存的管理

246
00:10:23,080 --> 00:10:24,840
计算图的调度和执行

247
00:10:24,840 --> 00:10:27,160
还有类和代码的优化和编译

248
00:10:27,160 --> 00:10:29,320
还有子表达式的一些优化

249
00:10:30,320 --> 00:10:31,840
内容的多了非常多

250
00:10:31,960 --> 00:10:34,000
可能又多了中间好几层

251
00:10:34,000 --> 00:10:35,680
所以它的优点就是

252
00:10:35,680 --> 00:10:38,040
执行之前得到整个程序

253
00:10:38,040 --> 00:10:41,160
或者整个神经网络的全表达

254
00:10:41,160 --> 00:10:42,960
然后可以做一些性能的优化

255
00:10:43,160 --> 00:10:44,120
编译的优化

256
00:10:44,120 --> 00:10:46,040
就会导致性能非常高

257
00:10:46,040 --> 00:10:48,240
可能还可以对接到多的硬件

258
00:10:48,240 --> 00:10:49,760
就不同的硬件上面

259
00:10:49,760 --> 00:10:52,520
但是缺点就是不灵活

260
00:10:52,520 --> 00:10:54,120
假设现在用Python

261
00:10:54,120 --> 00:10:55,880
表达了一行语句

262
00:10:55,880 --> 00:10:59,160
但是计算图没有跟得上来

263
00:10:59,160 --> 00:11:01,600
或者Python的某个操作方式

264
00:11:01,600 --> 00:11:04,320
没有复用某个if else的表达方式

265
00:11:04,320 --> 00:11:06,160
这个计算图没法表达

266
00:11:06,160 --> 00:11:08,400
那就很不方便去编程

267
00:11:08,400 --> 00:11:10,200
它的调试也不好

268
00:11:10,200 --> 00:11:12,080
如果的Python出现了控制流

269
00:11:12,080 --> 00:11:14,360
或者新的一个数学表示

270
00:11:14,360 --> 00:11:15,680
那对于计算图来说

271
00:11:15,680 --> 00:11:16,840
要适配过来

272
00:11:16,840 --> 00:11:18,840
就会导致很强的约束

273
00:11:19,000 --> 00:11:21,640
一旦有约束就确实不好用

274
00:11:21,640 --> 00:11:23,880
这个就是Tensorflow的问题

275
00:11:24,080 --> 00:11:27,320
既然有了两种极端的编程的

276
00:11:27,320 --> 00:11:29,680
范式和编程的问题

277
00:11:29,680 --> 00:11:32,960
所以后面其实或者后来之秀

278
00:11:32,960 --> 00:11:33,800
例如Mindspore

279
00:11:33,800 --> 00:11:35,520
还有PyTorch JIT

280
00:11:35,520 --> 00:11:38,600
都采用了一种融合的方式

281
00:11:38,600 --> 00:11:40,200
融合的方式有两种

282
00:11:40,200 --> 00:11:41,240
主要是有两种

283
00:11:41,240 --> 00:11:43,960
一种叫做分阶段的编程

284
00:11:43,960 --> 00:11:45,000
Multi-stage

285
00:11:45,000 --> 00:11:46,920
另外一种就是即时编程

286
00:11:46,920 --> 00:11:48,920
叫Just-in-time JIT

287
00:11:48,920 --> 00:11:51,080
那PyTorch又独立的开发了一个

288
00:11:51,080 --> 00:11:52,480
JIT的一个后端

289
00:11:52,480 --> 00:11:55,160
然后对PyTorch进行一个加速执行的

290
00:11:55,200 --> 00:11:57,040
为了就是更好的去适配到

291
00:11:57,040 --> 00:11:58,160
第三方的框架

292
00:11:58,160 --> 00:12:00,240
或者第三方的硬件上面

293
00:12:00,240 --> 00:12:02,200
让它执行效率更高

294
00:12:03,880 --> 00:12:05,840
而Mindspore采用的是一种

295
00:12:05,840 --> 00:12:07,640
函数式的编程方式

296
00:12:07,640 --> 00:12:09,800
其实跟刚才讲的那两种

297
00:12:10,000 --> 00:12:11,560
又有一点不一样

298
00:12:11,800 --> 00:12:13,600
就不在这里面展开

299
00:12:13,600 --> 00:12:15,640
Mindspore的具体的编程方式

300
00:12:15,640 --> 00:12:18,040
但是可能会在后面去介绍

301
00:12:18,040 --> 00:12:19,680
怎么去使用Mindspore的时候

302
00:12:19,680 --> 00:12:20,800
给大家介绍一下

303
00:12:20,800 --> 00:12:22,600
Mindspore的函数式编程

304
00:12:22,600 --> 00:12:23,880
有什么不一样

305
00:12:26,160 --> 00:12:27,440
好了

306
00:12:27,440 --> 00:12:29,360
又到了大家喜闻乐见的时候

307
00:12:29,360 --> 00:12:31,080
这是这一个系列里面

308
00:12:31,080 --> 00:12:32,320
最后的一节课了

309
00:12:32,320 --> 00:12:33,520
那这一节课里面

310
00:12:33,640 --> 00:12:34,720
一起回顾了

311
00:12:34,720 --> 00:12:37,120
深度学习的编程方式

312
00:12:37,120 --> 00:12:38,040
有两种

313
00:12:38,040 --> 00:12:39,760
第一种就是声明式编程

314
00:12:39,760 --> 00:12:41,680
另外一种是命令式编程

315
00:12:41,840 --> 00:12:44,560
其实未来可能可以看见的

316
00:12:44,560 --> 00:12:47,760
肯定是以命令式编程的应用性为主

317
00:12:47,960 --> 00:12:49,520
因为命令式的编程方式

318
00:12:49,520 --> 00:12:53,280
实在是太方便开发者去使用了

319
00:12:53,320 --> 00:12:55,200
但是也不会放弃

320
00:12:55,200 --> 00:12:57,120
声明式的一个编译优化

321
00:12:57,120 --> 00:12:59,440
相融合的一个优点

322
00:12:59,440 --> 00:13:02,480
所以它会做一个两者的结合

323
00:13:04,240 --> 00:13:04,680
好了

324
00:13:04,680 --> 00:13:05,400
谢谢各位

325
00:13:05,400 --> 00:13:06,320
拜了个拜

326
00:13:07,200 --> 00:13:08,040
卷的不行了

327
00:13:08,040 --> 00:13:08,880
卷的不行了

328
00:13:08,880 --> 00:13:10,320
记得一键三连加关注

329
00:13:10,680 --> 00:13:12,080
所有的内容都会开源

330
00:13:12,080 --> 00:13:14,000
在下面这条链接里面

331
00:13:14,255 --> 00:13:15,255
拜了个拜

