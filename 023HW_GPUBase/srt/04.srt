1
00:00:00,975 --> 00:00:03,650
字幕生成：慎独    校对：游吟外星人

2
00:00:05,045 --> 00:00:06,045
Hello，大家好

3
00:00:06,045 --> 00:00:07,950
一直拖更的ZOMI又回来了

4
00:00:07,950 --> 00:00:09,765
今天还是在GPU详解

5
00:00:09,765 --> 00:00:11,877
看看英伟达GPU的

6
00:00:11,877 --> 00:00:15,750
不同年代不同型号的架构有什么区别

7
00:00:15,750 --> 00:00:18,750
今天主要给大家分享的内容

8
00:00:18,750 --> 00:00:21,750
就是从Fermi到Pascal架构

9
00:00:21,750 --> 00:00:23,008
如果大家对

10
00:00:23,008 --> 00:00:25,150
英伟达GPU的基础概念不太了解的话

11
00:00:25,150 --> 00:00:28,350
可以看回上一节对大家的汇报和分享

12
00:00:28,350 --> 00:00:31,750
今天从2010年的Fermi架构开始

13
00:00:31,750 --> 00:00:36,150
去看看英伟达GPU每一代的架构有什么不一样

14
00:00:36,150 --> 00:00:40,750
实际上在这一节里面我给大家去汇报的内容

15
00:00:40,750 --> 00:00:41,750
主要分开几个

16
00:00:41,750 --> 00:00:43,750
第一个是总体概览一下

17
00:00:43,750 --> 00:00:46,750
GPU历史架构的总体情况

18
00:00:46,750 --> 00:00:48,350
接着看看Fermi架构

19
00:00:48,350 --> 00:00:50,550
然后了解一下Kepler架构

20
00:00:50,550 --> 00:00:53,750
最后Maxwell 麦克斯韦架构

21
00:00:53,750 --> 00:00:57,150
在结束之前看一个非常重要的内容

22
00:00:57,150 --> 00:00:58,113
就是Pascal架构

23
00:00:58,113 --> 00:01:00,520
Pascal架构也是开启了

24
00:01:00,520 --> 00:01:03,950
英伟达的一个AI星辰之旅

25
00:01:06,275 --> 00:01:07,258
下面来看看

26
00:01:07,258 --> 00:01:11,075
英伟达GPU的架构发展的总体情况

27
00:01:11,075 --> 00:01:13,113
这里面有几个比较重要的年份

28
00:01:13,113 --> 00:01:16,913
就是2010年的Fermi架构的提出

29
00:01:16,913 --> 00:01:20,713
开启了英伟达GPU架构演进的十几年

30
00:01:20,713 --> 00:01:23,513
里面对AI领域有比较重要的历史突破

31
00:01:23,513 --> 00:01:25,764
或者历史性意义的里程碑的架构

32
00:01:25,764 --> 00:01:29,763
就是从2016年的Pascal架构开始

33
00:01:29,763 --> 00:01:31,713
这里面对AI、人工神经网络

34
00:01:31,713 --> 00:01:34,113
或者深度学习有比较重要历史意义的

35
00:01:34,113 --> 00:01:36,713
就是这里面的的Volta架构 伏特架构

36
00:01:36,713 --> 00:01:40,713
然后针对产品对AI产品卖的最好的

37
00:01:40,713 --> 00:01:43,713
就是这里面的Ampere架构 安倍架构

38
00:01:43,713 --> 00:01:46,313
当然，这里面是一个整体的情况

39
00:01:46,313 --> 00:01:48,913
今天主要是聚焦于Fermi、Kepler

40
00:01:48,913 --> 00:01:50,313
Maxwell，还有Pascal

41
00:01:50,313 --> 00:01:52,300
还有Volta架构这5个

42
00:01:52,300 --> 00:01:54,913
而这里面有几个历史性的意义

43
00:01:54,913 --> 00:01:58,113
就是Fermi架构是首个完整的

44
00:01:58,113 --> 00:02:00,113
GPU的计算的架构

45
00:02:00,113 --> 00:02:02,513
里面就提出了非常多新的概念

46
00:02:02,513 --> 00:02:05,313
这些新的概念很多都沿用到至今

47
00:02:05,313 --> 00:02:07,513
Pascal架构里面比较重要的一个提出

48
00:02:07,513 --> 00:02:10,113
就是NVLink，提出了第一代NVLink

49
00:02:10,113 --> 00:02:15,513
当时候双向的互联带宽是160GB每秒

50
00:02:15,513 --> 00:02:18,313
而Volta架构最重要的一个提出

51
00:02:18,313 --> 00:02:19,713
就是Tensor Core

52
00:02:19,713 --> 00:02:23,113
专门针对神经网络矩阵卷积

53
00:02:23,113 --> 00:02:25,913
进行加速的第一代核心

54
00:02:27,484 --> 00:02:29,340
现在来看看第一个重要的内容

55
00:02:29,340 --> 00:02:31,142
就是Fermi架构

56
00:02:31,142 --> 00:02:33,913
右边就是英伟达Fermi的一个架构图

57
00:02:33,913 --> 00:02:35,513
最大可以支持16个SM

58
00:02:35,513 --> 00:02:38,313
每个SM有32个CUDA核

59
00:02:38,529 --> 00:02:41,313
也是从Fermi架构开始取消了SP这个概念

60
00:02:41,313 --> 00:02:43,113
引用了CUDA Core的概念

61
00:02:43,113 --> 00:02:44,620
在整个Fermi架构里面

62
00:02:44,620 --> 00:02:46,313
一共有512个CUDA Core

63
00:02:46,313 --> 00:02:49,113
也就是16个SM乘以32个CUDA Core

64
00:02:49,313 --> 00:02:52,313
就一共得到512个CUDA的核心

65
00:02:52,313 --> 00:02:54,713
其实CUDA的核心就是实际的计算单元

66
00:02:54,713 --> 00:02:58,113
整个GPU的架构当时还是以游戏作为主打

67
00:02:58,113 --> 00:03:00,113
所以这里面会有多个GPC

68
00:03:00,113 --> 00:03:02,513
也就是之前提到的图形图像处理簇

69
00:03:02,513 --> 00:03:04,713
在每个图形图像处理簇里面

70
00:03:04,713 --> 00:03:06,263
又有一个光栅引擎

71
00:03:06,263 --> 00:03:08,113
也就是Raster Engine

72
00:03:08,492 --> 00:03:09,913
因为计算核心太多了

73
00:03:09,913 --> 00:03:11,713
所以这里面的L2 Cache

74
00:03:11,713 --> 00:03:13,062
会放在中间

75
00:03:13,062 --> 00:03:15,313
数据可以快速的传输到上面的CUDA Core

76
00:03:15,313 --> 00:03:16,913
还有下面的CUDA Core

77
00:03:16,913 --> 00:03:19,213
下面打开Fermi架构的SM

78
00:03:19,213 --> 00:03:21,313
右边这个就是整体的SM

79
00:03:21,313 --> 00:03:23,313
每个SM有32个CUDA Core

80
00:03:23,313 --> 00:03:24,913
每排有8个

81
00:03:24,913 --> 00:03:26,113
一共有两排

82
00:03:26,113 --> 00:03:27,513
这里面SFU

83
00:03:27,513 --> 00:03:29,998
就是之前提到的特殊处理单元

84
00:03:29,998 --> 00:03:31,166
大部分并行的计算

85
00:03:31,166 --> 00:03:32,713
都是发生在CUDA Core里面

86
00:03:32,713 --> 00:03:34,313
进行处理的

87
00:03:34,313 --> 00:03:36,113
而打开每一个CUDA Core

88
00:03:36,113 --> 00:03:37,913
可以看到中间这个图

89
00:03:37,913 --> 00:03:39,713
首先指令或者线程

90
00:03:39,713 --> 00:03:41,913
会给到CUDA Core里面去真正的执行

91
00:03:41,913 --> 00:03:42,913
而这里面执行

92
00:03:42,913 --> 00:03:44,713
可以选择FP Unit

93
00:03:44,713 --> 00:03:47,513
这里面具体的是指FP32去执行

94
00:03:47,513 --> 00:03:49,713
也可以选择INT8去执行

95
00:03:49,713 --> 00:03:52,313
但这里面的执行不能是并行的

96
00:03:52,313 --> 00:03:53,513
它在CUDA Core里面

97
00:03:53,513 --> 00:03:54,113
只能说

98
00:03:54,113 --> 00:03:55,113
选择FP Unit去执行

99
00:03:55,113 --> 00:03:57,113
或者选择INT Unit去执行

100
00:03:57,206 --> 00:03:59,913
有了具体的硬件肯定会有对应的软件

101
00:03:59,913 --> 00:04:01,313
而英伟达里面对应的软件

102
00:04:01,313 --> 00:04:02,313
就是CUDA

103
00:04:02,313 --> 00:04:03,713
CUDA里面就分层

104
00:04:03,713 --> 00:04:04,913
之前提到了分层

105
00:04:04,913 --> 00:04:07,113
线程、Block（线程块）

106
00:04:07,113 --> 00:04:09,307
还有Grid（网格）三个层次

107
00:04:09,416 --> 00:04:11,713
每个层次都有对应不同的硬件

108
00:04:11,713 --> 00:04:15,113
例如Thread它可以共享局部的Memory

109
00:04:15,113 --> 00:04:17,513
而线程块用的是Shared Memory

110
00:04:17,513 --> 00:04:18,513
共享内存（指Shared Memory）

111
00:04:18,513 --> 00:04:20,913
最后到Grid也是SM之间

112
00:04:20,913 --> 00:04:23,913
可以共享全局内存

113
00:04:26,113 --> 00:04:27,513
在两年之后

114
00:04:27,513 --> 00:04:28,513
2012年的时候

115
00:04:28,513 --> 00:04:30,913
英伟达就发布了下一代的架构

116
00:04:30,913 --> 00:04:31,913
叫做Kepler

117
00:04:31,913 --> 00:04:33,313
Kepler架构

118
00:04:33,313 --> 00:04:34,513
Kepler架构里面

119
00:04:34,713 --> 00:04:35,913
SM改变了

120
00:04:35,913 --> 00:04:38,313
这里面SM就变成了SMX

121
00:04:38,313 --> 00:04:40,913
但是所代表的意义其实没有太多的变化

122
00:04:40,913 --> 00:04:43,913
它还是Streaming Multiprocessor的意义

123
00:04:43,913 --> 00:04:45,554
在整个Kepler架构里面

124
00:04:45,554 --> 00:04:47,113
硬件上面最直接的

125
00:04:47,113 --> 00:04:50,113
就是拥有了双精度的运算单元

126
00:04:50,113 --> 00:04:52,113
也就是从Kepler架构开始

127
00:04:52,113 --> 00:04:56,513
英伟达GPU慢慢地进入到了HPC这个领域

128
00:04:56,513 --> 00:04:58,913
就是高性能计算机这个领域

129
00:04:58,913 --> 00:05:00,913
例如我国就有太湖之光

130
00:05:00,913 --> 00:05:01,913
天河这种

131
00:05:01,913 --> 00:05:04,913
就属于高性能计算机里面的领域

132
00:05:04,913 --> 00:05:08,513
而现在80%的高性能计算机集群

133
00:05:08,513 --> 00:05:11,313
都会带有GPU进行加速

134
00:05:11,313 --> 00:05:14,501
也是因为这里面拥有了双精度浮点运算

135
00:05:14,667 --> 00:05:16,113
第三个重要的改进

136
00:05:16,113 --> 00:05:18,113
就是在Kepler架构里面

137
00:05:18,113 --> 00:05:19,913
就提出了GPU Direct

138
00:05:19,913 --> 00:05:21,513
传统的GPU数据处理

139
00:05:21,513 --> 00:05:24,713
需要多次经过CPU的内存拷贝

140
00:05:24,713 --> 00:05:26,713
为了降低访存的延迟

141
00:05:26,713 --> 00:05:28,313
还有数据重新搬运的问题

142
00:05:28,313 --> 00:05:30,113
就出现了GPU Direct

143
00:05:30,113 --> 00:05:32,513
绕过了CPU或者Host里面的

144
00:05:32,513 --> 00:05:33,313
System Memory

145
00:05:33,313 --> 00:05:35,713
（不再）必须要跟CPU里面的内存进行交互

146
00:05:35,713 --> 00:05:38,513
而是直接通过GPU跟GPU之间

147
00:05:38,513 --> 00:05:40,113
进行直接的数据交互

148
00:05:40,113 --> 00:05:41,113
进一步的提升了

149
00:05:41,113 --> 00:05:43,375
数据的处理和数据的带宽

150
00:05:43,525 --> 00:05:45,713
现在回顾一下Fermi架构里面

151
00:05:45,713 --> 00:05:48,113
SM一共有32个

152
00:05:48,113 --> 00:05:49,713
在Kepler架构里面

153
00:05:49,713 --> 00:05:53,113
SM就高达了192个

154
00:05:53,113 --> 00:05:56,113
整体的核心多了非常多

155
00:05:56,113 --> 00:05:57,513
它的并行处理的能力

156
00:05:57,513 --> 00:05:59,313
也是多了很多很多

157
00:06:01,113 --> 00:06:02,113
过了两年之后

158
00:06:02,113 --> 00:06:03,513
也就是2014年

159
00:06:03,513 --> 00:06:04,513
英伟达又提出了

160
00:06:04,513 --> 00:06:05,713
下一代的GPU架构

161
00:06:05,713 --> 00:06:08,313
Maxwell 麦克斯韦架构

162
00:06:08,313 --> 00:06:11,159
麦克斯韦架构其实没有太多的变化

163
00:06:11,159 --> 00:06:13,513
从SM里面具体的看一下

164
00:06:13,513 --> 00:06:14,513
在Fermi架构里面

165
00:06:14,513 --> 00:06:16,513
SM的核心有32个

166
00:06:16,513 --> 00:06:17,513
在Kepler架构里面

167
00:06:17,513 --> 00:06:19,113
SM的核心有192个

168
00:06:19,113 --> 00:06:20,713
而在Maxwell架构里面

169
00:06:20,713 --> 00:06:22,513
SMX这个名字

170
00:06:22,713 --> 00:06:25,513
又回归到SM这个名字里面了

171
00:06:25,513 --> 00:06:28,713
而整体的核心个数就变回了128个

172
00:06:28,713 --> 00:06:30,113
英伟达其实发现了

173
00:06:30,113 --> 00:06:31,763
核心数没必要太多

174
00:06:31,763 --> 00:06:32,879
但是线程数

175
00:06:32,879 --> 00:06:34,806
可以超配可以更多

176
00:06:35,513 --> 00:06:36,713
到了2016年

177
00:06:36,713 --> 00:06:38,713
来到了Pascal架构

178
00:06:39,550 --> 00:06:42,513
Pascal是法国的物理学家和数学家

179
00:06:42,513 --> 00:06:44,852
高中学到的大气压

180
00:06:44,852 --> 00:06:46,801
也是以Pascal作为单位的

181
00:06:46,801 --> 00:06:48,171
在整体的架构里面

182
00:06:48,171 --> 00:06:50,313
SM进行了一个精简

183
00:06:50,313 --> 00:06:52,313
每个SM里面包含的内容

184
00:06:52,313 --> 00:06:53,513
也是越来越少

185
00:06:53,513 --> 00:06:54,713
但是总体来说

186
00:06:54,713 --> 00:06:56,713
因为纳米的制程提升了

187
00:06:56,713 --> 00:06:59,513
所以它里面包含的SM也会增加

188
00:06:59,513 --> 00:07:02,906
接下来打开Pascal架构里面的SM

189
00:07:02,906 --> 00:07:05,313
单个SM里面就有64个

190
00:07:05,313 --> 00:07:06,913
FP32的CUDA Core

191
00:07:06,913 --> 00:07:09,113
这里面CUDA Core非常非常的多

192
00:07:09,113 --> 00:07:10,557
相比起Maxwell里面的

193
00:07:10,557 --> 00:07:11,769
128个CUDA Core

194
00:07:11,769 --> 00:07:14,384
还有Kepler架构里面的192个

195
00:07:14,559 --> 00:07:16,258
确实Pascal架构里面的CUDA Core

196
00:07:16,258 --> 00:07:17,313
少了很多

197
00:07:17,313 --> 00:07:19,513
而且这里面分开两个区域

198
00:07:19,513 --> 00:07:21,713
一个是左边的区域

199
00:07:21,713 --> 00:07:23,713
一个是右边的区域

200
00:07:23,713 --> 00:07:26,713
每个区域有32个CUDA Core

201
00:07:26,713 --> 00:07:29,113
这样的好处再往下看看

202
00:07:29,113 --> 00:07:31,513
为什么把CUDA Core区分成两块

203
00:07:31,713 --> 00:07:32,913
是因为这里面看到

204
00:07:32,913 --> 00:07:33,713
Register File

205
00:07:33,713 --> 00:07:35,877
就是寄存器的数量保持不变

206
00:07:35,877 --> 00:07:37,113
分开两个

207
00:07:37,113 --> 00:07:38,113
这样的话就可以保证

208
00:07:38,113 --> 00:07:38,913
CUDA Core

209
00:07:38,913 --> 00:07:40,513
每个执行具体的线程

210
00:07:40,513 --> 00:07:42,513
可以使用更多的寄存器

211
00:07:42,513 --> 00:07:43,182
而SM

212
00:07:43,182 --> 00:07:45,513
单个SM可以并发的去执行

213
00:07:45,513 --> 00:07:46,513
更多的线程

214
00:07:46,513 --> 00:07:47,113
更多的Warp

215
00:07:47,113 --> 00:07:48,113
更多的Block

216
00:07:48,113 --> 00:07:50,113
进一步的加强了英伟达GPU的

217
00:07:50,113 --> 00:07:51,513
并行处理能力

218
00:07:52,113 --> 00:07:53,913
刚才提到了在Pascal架构里面

219
00:07:53,913 --> 00:07:55,313
非常重要的一点

220
00:07:55,313 --> 00:07:56,713
就是右边的图

221
00:07:56,713 --> 00:07:58,313
里面就提出了

222
00:07:58,313 --> 00:08:00,513
第一代的NV LINK

223
00:08:00,513 --> 00:08:03,713
带宽高达了160GB每秒

224
00:08:03,713 --> 00:08:07,113
相当于当时PCIe里面的三倍

225
00:08:07,438 --> 00:08:09,113
实现了单个节点

226
00:08:09,113 --> 00:08:11,513
也就是单台服务器里面的GPU

227
00:08:11,513 --> 00:08:13,713
可以进行数据的互联

228
00:08:14,513 --> 00:08:16,113
减少了数据传输的延迟

229
00:08:16,113 --> 00:08:17,513
也减少了数据之间的

230
00:08:17,513 --> 00:08:20,713
大量的通过PCIe回传到CPU的内存里面

231
00:08:20,713 --> 00:08:22,913
进行重复的搬运性的工作

232
00:08:22,913 --> 00:08:25,713
实现了整个网络的拓扑互联

233
00:08:25,713 --> 00:08:28,713
这个建树也是非常非常的重要

234
00:08:28,713 --> 00:08:30,313
现在ZOMI负责训练大模型的

235
00:08:30,313 --> 00:08:32,513
过程当中就发现了

236
00:08:32,513 --> 00:08:33,713
带宽会成为整个

237
00:08:33,713 --> 00:08:35,313
分布式训练系统里面的

238
00:08:35,313 --> 00:08:36,313
主要的瓶颈

239
00:08:39,713 --> 00:08:40,513
一年之后

240
00:08:40,513 --> 00:08:42,113
也就是在2017年

241
00:08:42,113 --> 00:08:44,313
英伟达又提出了新一代的架构

242
00:08:44,313 --> 00:08:45,913
叫做Volta

243
00:08:45,913 --> 00:08:47,513
叫做Voltage架构

244
00:08:47,513 --> 00:08:50,513
Volta是第一个发现用化学的方式

245
00:08:50,513 --> 00:08:52,113
产生电流原理的

246
00:08:52,113 --> 00:08:54,283
意大利科学家Volta

247
00:08:54,283 --> 00:08:55,913
进行命名的

248
00:08:55,913 --> 00:08:56,713
而这一代架构

249
00:08:56,713 --> 00:08:58,513
也叫做Volta架构

250
00:08:58,513 --> 00:08:59,913
在整体Volta架构里面

251
00:08:59,913 --> 00:09:02,709
就引用了非常非常多的新鲜的玩意

252
00:09:02,709 --> 00:09:05,513
在整体架构里面将CUDA Core进行拆分

253
00:09:05,513 --> 00:09:07,913
把FPU和ALU拆分

254
00:09:07,913 --> 00:09:09,113
取消了CUDA Core

255
00:09:09,113 --> 00:09:10,313
就是以后就没有

256
00:09:10,313 --> 00:09:13,113
CUDA Core这个整体的硬件的概念

257
00:09:13,113 --> 00:09:14,913
但是CUDA Core这个软件的概念

258
00:09:14,913 --> 00:09:16,313
都可以保留下来

259
00:09:16,313 --> 00:09:17,113
这样的好处

260
00:09:17,113 --> 00:09:18,313
就实现一条指令

261
00:09:18,313 --> 00:09:20,913
可以同时执行不同的计算

262
00:09:20,913 --> 00:09:21,713
第二个特性

263
00:09:21,713 --> 00:09:24,113
就是提出了独立的线程的调度

264
00:09:24,113 --> 00:09:25,713
改进了整个SIMT的模型

265
00:09:25,713 --> 00:09:28,913
也就是单指令多线程的架构

266
00:09:28,913 --> 00:09:31,513
使得每个线程都有自己的独立的

267
00:09:31,513 --> 00:09:32,513
Programming Counter

268
00:09:32,513 --> 00:09:33,913
就是独立的PC

269
00:09:33,913 --> 00:09:35,765
还有自己的Stack（自己的栈）

270
00:09:36,313 --> 00:09:37,513
第三个就是

271
00:09:37,513 --> 00:09:39,513
针对AI提出来的

272
00:09:39,513 --> 00:09:42,313
Tensor Core第一代的张量核心

273
00:09:42,313 --> 00:09:43,713
针对深度学习

274
00:09:43,713 --> 00:09:45,713
提供了专门针对卷积

275
00:09:45,713 --> 00:09:46,713
或者矩阵乘

276
00:09:46,713 --> 00:09:47,978
进行计算加速

277
00:09:48,106 --> 00:09:49,913
当然这里面还有其他黑科技

278
00:09:49,913 --> 00:09:51,513
例如NV-Link的第二代

279
00:09:51,513 --> 00:09:54,513
还有提出了MPS这个概念

280
00:09:54,513 --> 00:09:57,913
多进程服务更好的去适配到云厂商里面

281
00:09:57,913 --> 00:09:59,513
进行多用户的租赁

282
00:09:59,513 --> 00:10:00,513
或者多用户的处理

283
00:10:00,513 --> 00:10:02,313
和多用户的排队

284
00:10:02,313 --> 00:10:03,513
接下来看看

285
00:10:03,513 --> 00:10:06,113
Volta架构里面具体的SM

286
00:10:06,113 --> 00:10:08,913
首先这里面的SM的内容

287
00:10:08,913 --> 00:10:10,913
就比之前多了非常多

288
00:10:10,913 --> 00:10:12,913
因为把CUDA Core拆分出来了

289
00:10:12,913 --> 00:10:13,913
一个SM里面就有

290
00:10:13,913 --> 00:10:15,313
就有4个Warp Scheduler

291
00:10:15,313 --> 00:10:16,913
1234

292
00:10:16,913 --> 00:10:18,713
刚好上下各两个

293
00:10:18,713 --> 00:10:20,850
一共有64个FP32的核

294
00:10:20,913 --> 00:10:22,913
每一组有16个

295
00:10:22,913 --> 00:10:24,913
64个INT32的核

296
00:10:24,913 --> 00:10:26,913
每一组也是共有16个

297
00:10:26,913 --> 00:10:28,313
针对双精度FP64

298
00:10:28,313 --> 00:10:30,913
一共有32个FP64的核

299
00:10:30,913 --> 00:10:33,313
也就是每一组有8个

300
00:10:33,313 --> 00:10:34,713
当然了这里面很重要的

301
00:10:34,713 --> 00:10:36,913
就是Tensor Core的提出

302
00:10:36,913 --> 00:10:38,713
Tensor Core一共有8个

303
00:10:38,713 --> 00:10:39,913
当然还有其他的

304
00:10:39,913 --> 00:10:41,313
32个LD/ST Unit

305
00:10:41,313 --> 00:10:42,913
还有4个SFU

306
00:10:42,913 --> 00:10:45,765
就是特殊的数据处理单元

307
00:10:45,970 --> 00:10:48,553
这里面把FP32和INT

308
00:10:48,553 --> 00:10:50,113
两组运算单元独立出来

309
00:10:50,113 --> 00:10:52,113
出现在整个流水线里面

310
00:10:52,313 --> 00:10:53,649
就使得SM可以

311
00:10:53,649 --> 00:10:55,713
同时执行FP16，同时执行INT

312
00:10:55,713 --> 00:10:57,313
同时执行FP32

313
00:10:57,313 --> 00:10:58,513
还有Tensor Core

314
00:10:58,513 --> 00:11:01,113
使得吞吐进一步的增加

315
00:11:01,113 --> 00:11:03,513
每个Cycle都可以同时执行

316
00:11:03,513 --> 00:11:05,313
上面的这些指令

317
00:11:05,313 --> 00:11:08,913
也就是每个时钟周期可以执行的数量更多了

318
00:11:08,913 --> 00:11:11,513
可以执行计算量更大了

319
00:11:12,463 --> 00:11:14,713
下面打开Volta架构里面

320
00:11:14,713 --> 00:11:17,113
最重要的一个单元Tensor Core

321
00:11:17,285 --> 00:11:20,213
在AI的计算里面最常见的是

322
00:11:20,213 --> 00:11:22,813
卷积或者矩阵乘的方式

323
00:11:22,813 --> 00:11:25,213
其实在之前的GPU里面

324
00:11:25,213 --> 00:11:27,213
需要编码成FMA

325
00:11:27,213 --> 00:11:28,613
Fused-Multiply-Add（融合乘加运算）

326
00:11:28,613 --> 00:11:30,613
把乘加运算合并起来

327
00:11:30,613 --> 00:11:32,613
而这个时候其实硬件层面

328
00:11:32,613 --> 00:11:34,413
需要把数据来回的搬运

329
00:11:34,413 --> 00:11:37,213
从寄存器搬到ALU执行一个乘法

330
00:11:37,213 --> 00:11:40,613
然后再从寄存器搬到ALU执行一个加法

331
00:11:40,613 --> 00:11:42,613
然后再放回寄存器里面

332
00:11:42,613 --> 00:11:44,813
整个数据是来回的搬运的

333
00:11:44,813 --> 00:11:46,413
那现在一个Tensor Core

334
00:11:46,413 --> 00:11:50,213
一个指令就可以执行4×4×4的一个GEMM

335
00:11:50,213 --> 00:11:52,413
也就是64个FMA

336
00:11:52,413 --> 00:11:55,613
极大地减少了系统内存的开销

337
00:11:55,613 --> 00:11:56,613
硬件的开销

338
00:11:56,613 --> 00:11:59,413
虽然这里面的矩阵数是FP16

339
00:11:59,413 --> 00:12:01,813
但是输出可以是FP32

340
00:12:01,813 --> 00:12:02,813
相当于就提供了

341
00:12:02,813 --> 00:12:05,213
64个FP32的ALU的计算能力

342
00:12:05,213 --> 00:12:08,013
能耗上也是非常的具有优势

343
00:12:08,013 --> 00:12:09,013
一个时钟周期内

344
00:12:09,013 --> 00:12:11,813
可以执行更多的矩阵的运算了

345
00:12:11,813 --> 00:12:14,013
而在整个Volta架构的硬件里面

346
00:12:14,013 --> 00:12:15,813
可以看到后来卖的

347
00:12:15,813 --> 00:12:19,213
就不仅仅是经常理解到的一张卡了

348
00:12:19,213 --> 00:12:21,813
而是卖的一个DGX的Station

349
00:12:21,813 --> 00:12:23,013
一个工作站

350
00:12:23,013 --> 00:12:24,213
每个工作站上面

351
00:12:24,213 --> 00:12:25,613
就可以贴合8块

352
00:12:25,613 --> 00:12:26,813
或者4块

353
00:12:26,813 --> 00:12:28,613
Volted架构的芯片

354
00:12:29,995 --> 00:12:31,813
刚才看到的每一块芯片

355
00:12:31,813 --> 00:12:33,613
就是通过这种方式

356
00:12:33,613 --> 00:12:35,813
贴合在整个机柜上面

357
00:12:35,813 --> 00:12:38,013
而整个机柜上面也可以累加起来

358
00:12:38,013 --> 00:12:40,413
后面有非常多的风口

359
00:12:40,413 --> 00:12:41,013
而这里面

360
00:12:41,013 --> 00:12:45,613
又有很多CPU贴合在整个机器上面

361
00:12:45,613 --> 00:12:47,613
这个就是具体的形态

362
00:12:48,688 --> 00:12:49,288
好了

363
00:12:49,288 --> 00:12:51,413
今天的内容就到这里为止

364
00:12:51,413 --> 00:12:54,213
回顾了英伟达从2010年

365
00:12:54,213 --> 00:12:56,413
到2017年里面的

366
00:12:56,413 --> 00:12:58,413
主要的GPU架构的发展

367
00:12:58,413 --> 00:13:00,213
从Fermi发展到Volta

368
00:13:00,213 --> 00:13:03,013
从Fermi到Volta架构经历了7年

369
00:13:03,013 --> 00:13:04,413
而这几个架构里面

370
00:13:04,413 --> 00:13:06,013
ZOMI觉得最重要的几个概念

371
00:13:06,013 --> 00:13:07,813
就是从Fermi架构提出了

372
00:13:07,813 --> 00:13:10,013
首个完整的GPU计算架构

373
00:13:10,013 --> 00:13:12,213
接着到2016年的Pascal

374
00:13:12,213 --> 00:13:13,813
提出了第一代的NVLink

375
00:13:13,813 --> 00:13:16,952
到2017年提出的第一代的Tensor Core

376
00:13:16,952 --> 00:13:19,613
是非常重要的一些概念

377
00:13:19,613 --> 00:13:21,813
也就是因为从2016年的

378
00:13:21,813 --> 00:13:23,413
NVLink和Tensor Core开始

379
00:13:23,413 --> 00:13:26,833
英伟达慢慢地在AI的道路上越走越强

380
00:13:26,833 --> 00:13:29,013
今天分享的内容就到这里为止

381
00:13:29,013 --> 00:13:29,813
谢谢各位

382
00:13:29,813 --> 00:13:30,740
拜了个拜

383
00:13:31,128 --> 00:13:32,719
卷的不行了

384
00:13:32,719 --> 00:13:34,599
记得一键三连加关注哦

385
00:13:34,599 --> 00:13:37,586
所有的内容都会开源在下面这条链接里面

386
00:13:38,025 --> 00:13:39,025
拜了个拜

