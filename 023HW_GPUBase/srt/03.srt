1
00:00:00,960 --> 00:00:03,648
字幕生成：慎独    校对：游吟外星人

2
00:00:04,925 --> 00:00:05,880
Hello，大家好

3
00:00:05,880 --> 00:00:07,680
我是小时候缺钙

4
00:00:07,680 --> 00:00:09,640
长大了缺发的ZOMI

5
00:00:12,469 --> 00:00:15,005
今天来到一个新的内容

6
00:00:15,005 --> 00:00:16,325
GPU的详解

7
00:00:16,325 --> 00:00:18,285
英伟达的GPU

8
00:00:18,863 --> 00:00:21,903
在整个英伟达的GPU架构系列里面

9
00:00:22,023 --> 00:00:23,703
来了解一下

10
00:00:23,703 --> 00:00:26,463
接下来会给大家展开哪些内容

11
00:00:26,613 --> 00:00:28,615
这个就是今天的主角

12
00:00:28,615 --> 00:00:30,031
GPU的基础概念

13
00:00:30,031 --> 00:00:34,073
有了对GPU的软硬件的基础概念之后

14
00:00:34,073 --> 00:00:37,353
接下来就会简单的去看一看

15
00:00:37,353 --> 00:00:39,553
从Fermi到Volta的架构

16
00:00:39,553 --> 00:00:42,233
从Turing到Hopper的架构

17
00:00:42,233 --> 00:00:44,953
最后看一下最近这几年

18
00:00:44,953 --> 00:00:48,193
针对AI而衍生出来的Tensor Core

19
00:00:48,193 --> 00:00:50,993
还有大带宽的NVLink

20
00:00:51,973 --> 00:00:54,393
在GPU基础概念里面

21
00:00:54,553 --> 00:00:57,753
主要分开三大块给大家去汇报

22
00:00:57,753 --> 00:01:00,033
第一块就是GPU的基础概念

23
00:01:00,033 --> 00:01:01,873
第二个就是是CUDA

24
00:01:01,873 --> 00:01:03,873
CUDA是一个并行的计算平台

25
00:01:03,873 --> 00:01:06,033
或者并行的编程体系也好

26
00:01:06,033 --> 00:01:07,956
给大家展开CUDA里面的

27
00:01:07,956 --> 00:01:09,713
Grid、Block、Thread

28
00:01:09,713 --> 00:01:13,033
线程、网格、块相关的结构

29
00:01:13,033 --> 00:01:14,167
最后看看

30
00:01:14,167 --> 00:01:17,033
英伟达的算力是怎么计算的

31
00:01:17,033 --> 00:01:18,073
有了这个算力之后

32
00:01:18,073 --> 00:01:21,353
在大模型训练的时候就很好的去计算

33
00:01:21,353 --> 00:01:23,153
算力峰值

34
00:01:23,153 --> 00:01:25,033
算一下算力利用率

35
00:01:25,033 --> 00:01:26,353
到底是怎么样的

36
00:01:26,733 --> 00:01:28,633
在进入实际内容之前

37
00:01:28,633 --> 00:01:31,713
来回顾一下GPU跟CPU

38
00:01:31,713 --> 00:01:33,153
之间的一个关系

39
00:01:33,313 --> 00:01:34,713
从左下角的这个图

40
00:01:34,713 --> 00:01:35,473
可以看到

41
00:01:35,473 --> 00:01:37,033
其实CPU里面的ALU

42
00:01:37,033 --> 00:01:38,713
或者所谓的计算单元

43
00:01:38,713 --> 00:01:40,073
并不是那么多

44
00:01:40,073 --> 00:01:42,473
更重要的是讲究逻辑的控制

45
00:01:42,473 --> 00:01:45,313
所以橙色这块控制单元会比较大

46
00:01:45,313 --> 00:01:47,473
而且Cache也会比较大

47
00:01:47,473 --> 00:01:49,913
右边这块就是GPU的简单的架构

48
00:01:49,913 --> 00:01:51,473
绿色代表计算单元

49
00:01:51,473 --> 00:01:52,313
从GPU的架构

50
00:01:52,313 --> 00:01:53,273
你们可以看到

51
00:01:53,273 --> 00:01:55,353
里面有大量的计算单元

52
00:01:55,353 --> 00:01:57,073
而真正的控制和Cache

53
00:01:57,193 --> 00:01:58,833
其实是非常少的一部分

54
00:01:58,833 --> 00:01:59,233
当然了

55
00:01:59,233 --> 00:02:00,353
大家都有DRAM

56
00:02:00,353 --> 00:02:01,313
而在GPU里面

57
00:02:01,313 --> 00:02:02,353
实际上这个DRAM

58
00:02:02,393 --> 00:02:05,073
叫做HBM高带宽存储器

59
00:02:05,478 --> 00:02:08,753
下面来到第一个真正的内容

60
00:02:08,753 --> 00:02:12,833
就是英伟达的硬件的基本的概念

61
00:02:12,833 --> 00:02:15,273
从上到下逐个的去看一下

62
00:02:15,273 --> 00:02:16,233
首先可以看到

63
00:02:16,233 --> 00:02:18,913
这里面有非常多的GPC

64
00:02:18,913 --> 00:02:21,393
有非常多的图像处理簇

65
00:02:21,393 --> 00:02:23,507
就是Graph Possessed Cluster

66
00:02:23,507 --> 00:02:24,505
一共有八个

67
00:02:24,505 --> 00:02:26,953
那每一个GPC可以看到

68
00:02:26,953 --> 00:02:30,073
里面又有非常多的TPC

69
00:02:30,073 --> 00:02:32,153
TPC就是英伟达的

70
00:02:32,153 --> 00:02:33,913
Texture Possessed Cluster

71
00:02:33,913 --> 00:02:35,593
纹理处理簇

72
00:02:35,593 --> 00:02:37,463
每个纹理处理簇里面

73
00:02:37,463 --> 00:02:39,993
又分为多个SM

74
00:02:39,993 --> 00:02:41,553
多个SM里面

75
00:02:41,553 --> 00:02:43,793
又分为很多Block和Thread

76
00:02:43,793 --> 00:02:46,673
现在来去综合的看一下 

77
00:02:46,673 --> 00:02:47,993
这些主要的概念

78
00:02:48,249 --> 00:02:50,798
在GPU里面最大的有一个GPC

79
00:02:51,073 --> 00:02:52,873
叫做图像处理簇

80
00:02:52,873 --> 00:02:55,993
然后又有了TPC纹理处理簇

81
00:02:55,993 --> 00:02:58,753
最小的单位有一个流处理器

82
00:02:58,753 --> 00:02:59,753
而另外的话

83
00:02:59,753 --> 00:03:01,033
在中间绿色的

84
00:03:01,033 --> 00:03:01,753
或者蓝色的

85
00:03:01,753 --> 00:03:02,696
刚才那个模块

86
00:03:02,696 --> 00:03:05,593
就是HBM高带宽存储器

87
00:03:05,593 --> 00:03:06,953
整体的包含关系

88
00:03:06,953 --> 00:03:09,633
就是GPC大于TCP大于SM

89
00:03:09,633 --> 00:03:10,553
而SM里面

90
00:03:10,673 --> 00:03:12,793
就有各种各样的CUDA Tensor Core

91
00:03:12,793 --> 00:03:14,793
各种各样的TR core也好

92
00:03:14,793 --> 00:03:17,593
专门的针对图形图像

93
00:03:17,593 --> 00:03:20,193
纹理或者AI张量

94
00:03:20,193 --> 00:03:22,153
进行处理的单元

95
00:03:22,193 --> 00:03:25,673
接下来要理解一个非常重要的概念

96
00:03:25,673 --> 00:03:27,033
叫做SM

97
00:03:31,463 --> 00:03:32,633
这里面的SM

98
00:03:32,873 --> 00:03:35,073
并不是意义上理解的SM

99
00:03:35,273 --> 00:03:37,633
是多流式处理器

100
00:03:37,633 --> 00:03:39,593
Streaming Multiprocessor

101
00:03:39,593 --> 00:03:43,033
这个概念其实是从G80

102
00:03:43,033 --> 00:03:45,731
也就是2006年的时候去提出的

103
00:03:45,731 --> 00:03:47,433
这里面的的核心组件

104
00:03:47,433 --> 00:03:48,913
主要是有CUDA Core

105
00:03:48,913 --> 00:03:50,913
就是计算的核心

106
00:03:50,913 --> 00:03:52,513
另外还会有一些共享的内存

107
00:03:52,513 --> 00:03:53,513
还有寄存器

108
00:03:53,513 --> 00:03:55,673
里面SM最核心的模块

109
00:03:55,793 --> 00:03:58,073
就是现成的执行的单元

110
00:03:58,073 --> 00:03:59,513
所以叫做CUDA Core

111
00:03:59,513 --> 00:04:00,473
这里面可以看到

112
00:04:00,473 --> 00:04:01,793
不管是INT32

113
00:04:01,793 --> 00:04:02,553
FP32

114
00:04:02,553 --> 00:04:03,353
FP64

115
00:04:03,353 --> 00:04:04,673
还是Tensor Core

116
00:04:04,673 --> 00:04:07,633
它最终都归根于执行单元

117
00:04:07,633 --> 00:04:09,153
或者最小的执行单位

118
00:04:09,153 --> 00:04:10,353
因此说

119
00:04:10,353 --> 00:04:13,073
SM是英伟达的整个的核心

120
00:04:13,073 --> 00:04:15,513
里面SM有非常多的计算单元

121
00:04:15,513 --> 00:04:17,843
所有计算单元都藏在SM里面

122
00:04:18,078 --> 00:04:20,153
SM里面可以并发的执行

123
00:04:20,153 --> 00:04:21,953
数百个线程

124
00:04:21,953 --> 00:04:24,753
每一个线程都会执行对应的指令

125
00:04:24,753 --> 00:04:26,993
或者在对应的硬件数据上面

126
00:04:26,993 --> 00:04:28,113
去进行处理的

127
00:04:28,113 --> 00:04:30,193
当然这些都是都是硬件上的概念

128
00:04:30,193 --> 00:04:31,513
而从软件上面来看

129
00:04:31,793 --> 00:04:33,713
特别是从CUDA的角度来看

130
00:04:33,713 --> 00:04:35,873
可以通过SM并发的执行

131
00:04:35,873 --> 00:04:36,953
数百个线程

132
00:04:36,953 --> 00:04:37,873
数百个Thread

133
00:04:37,873 --> 00:04:38,633
而每个Thread

134
00:04:38,633 --> 00:04:40,313
刚才其实在上一个章节里面

135
00:04:40,313 --> 00:04:41,033
提到

136
00:04:41,033 --> 00:04:42,713
线程其实是有等级的

137
00:04:42,713 --> 00:04:44,513
它有块、网格

138
00:04:44,513 --> 00:04:46,073
和最小的单位线程

139
00:04:46,233 --> 00:04:47,273
而每个线程块

140
00:04:47,273 --> 00:04:49,393
是放在同一个SM去执行

141
00:04:49,393 --> 00:04:50,444
而SM里面

142
00:04:50,633 --> 00:04:51,873
通过Register

143
00:04:51,873 --> 00:04:52,833
或者Cache

144
00:04:53,113 --> 00:04:54,673
去约束每个块

145
00:04:54,673 --> 00:04:55,793
线程的大小

146
00:04:55,793 --> 00:04:57,553
处理的数据就这么多了

147
00:04:57,553 --> 00:04:58,953
硬件单元就这么多了

148
00:04:58,953 --> 00:04:59,873
所以线程

149
00:05:00,033 --> 00:05:01,713
不能无限制的扩充

150
00:05:01,852 --> 00:05:02,993
现在来看一看

151
00:05:02,993 --> 00:05:04,793
其实SM刚才只是举了

152
00:05:04,793 --> 00:05:06,913
几个比较重要的模块

153
00:05:06,913 --> 00:05:09,113
实际上SM里面的模块非常多

154
00:05:09,113 --> 00:05:10,273
有CUDA Core

155
00:05:10,273 --> 00:05:11,833
向量的运算单元

156
00:05:11,833 --> 00:05:12,993
还有Tensor Core

157
00:05:12,993 --> 00:05:14,193
张量的运算单元

158
00:05:14,193 --> 00:05:16,633
专门针对AI进行加速的

159
00:05:16,633 --> 00:05:18,073
特别是矩阵乘

160
00:05:18,073 --> 00:05:19,953
另外针对超越函数

161
00:05:19,953 --> 00:05:21,353
或者一些复杂的数学操作

162
00:05:21,673 --> 00:05:25,673
就会有SFU特殊的函数单元

163
00:05:25,673 --> 00:05:26,953
专门对数学进行

164
00:05:26,953 --> 00:05:29,313
反平方根、正弦余弦的操作

165
00:05:29,313 --> 00:05:31,873
上面的1、2、3主要是针对具体的

166
00:05:31,873 --> 00:05:34,313
计算、执行来去看待的

167
00:05:34,313 --> 00:05:36,433
而下面看看第4个

168
00:05:36,433 --> 00:05:37,673
就是Warp Scheduler

169
00:05:37,673 --> 00:05:38,953
还有Dispatch Scheduler

170
00:05:38,953 --> 00:05:41,473
Warp Scheduler是线程束的一个调度器

171
00:05:41,473 --> 00:05:42,553
会把线程

172
00:05:42,553 --> 00:05:44,713
下发到具体的计算单元里面

173
00:05:44,713 --> 00:05:46,073
而针对线程

174
00:05:46,073 --> 00:05:47,513
它是个软件的概念

175
00:05:47,513 --> 00:05:48,313
实际上硬件

176
00:05:48,313 --> 00:05:50,193
是执行指令

177
00:05:50,193 --> 00:05:51,953
因此有了第5个单元

178
00:05:51,953 --> 00:05:52,953
Dispatch Unit

179
00:05:52,953 --> 00:05:54,913
专门针对指令进行分发

180
00:05:54,913 --> 00:05:55,753
分发到上面

181
00:05:55,753 --> 00:05:57,873
1、2、3的具体的执行单位

182
00:05:57,873 --> 00:06:00,513
最后就是访存和IO了

183
00:06:00,513 --> 00:06:01,433
就是Register File

184
00:06:01,433 --> 00:06:02,513
寄存器堆

185
00:06:02,513 --> 00:06:03,821
还有Load/Store

186
00:06:03,821 --> 00:06:03,833
访问IO的一些单元

187
00:06:03,833 --> 00:06:05,341
访问IO的一些单元

188
00:06:05,353 --> 00:06:07,593
专门针对数据进行处理

189
00:06:08,290 --> 00:06:10,090
刚才只是简单的去看看

190
00:06:10,090 --> 00:06:11,690
一个比较大的概念

191
00:06:11,690 --> 00:06:13,690
或者单元叫做SM

192
00:06:13,690 --> 00:06:15,570
现在再打开SM里面

193
00:06:15,730 --> 00:06:17,850
其实刚才提到的很多

194
00:06:17,850 --> 00:06:18,755
CUDA Core

195
00:06:18,810 --> 00:06:20,170
它的前身叫做

196
00:06:20,170 --> 00:06:21,850
SP流处理器

197
00:06:21,850 --> 00:06:25,090
也是SM里面最基本的处理的单元

198
00:06:25,303 --> 00:06:28,129
只不过在后来的英伟达的架构演进里

199
00:06:28,129 --> 00:06:30,529
慢慢的被CUDA Core所替代掉了

200
00:06:30,529 --> 00:06:31,129
而CUDA Core

201
00:06:31,129 --> 00:06:32,929
其实后面也慢慢的消亡了

202
00:06:32,929 --> 00:06:34,969
那GPU进行并行的计算

203
00:06:34,969 --> 00:06:37,289
因此会有非常多的SM

204
00:06:37,289 --> 00:06:39,369
和非常多的SP

205
00:06:39,675 --> 00:06:41,209
随着Fermi架构的提出

206
00:06:41,209 --> 00:06:43,809
SP就逐渐的被CUDA Core

207
00:06:43,809 --> 00:06:45,209
所代替掉了

208
00:06:45,209 --> 00:06:48,129
这里面是新的架构的名字的命名

209
00:06:48,129 --> 00:06:49,289
在一个SM里面

210
00:06:49,449 --> 00:06:51,689
具有非常多的CUDA Core

211
00:06:51,689 --> 00:06:54,461
而打开其中一个CUDA Core来看看

212
00:06:54,729 --> 00:06:55,809
一个SM里面

213
00:06:55,889 --> 00:06:56,809
有两组

214
00:06:56,809 --> 00:06:58,529
各16个CUDA Core

215
00:06:58,529 --> 00:06:59,716
而每个CUDA Core

216
00:06:59,769 --> 00:07:01,689
就是右边的小模块

217
00:07:01,689 --> 00:07:03,569
包含一个浮点的运算单元

218
00:07:03,569 --> 00:07:04,298
还有一个

219
00:07:04,298 --> 00:07:06,249
整点的运算单元

220
00:07:06,398 --> 00:07:07,769
到了Volta架构的时候

221
00:07:07,929 --> 00:07:09,609
CUDA Core又慢慢的

222
00:07:09,609 --> 00:07:10,809
被取代掉了

223
00:07:10,809 --> 00:07:11,889
就没有了CUDA Core

224
00:07:11,929 --> 00:07:13,849
反而变成多个INT32的单元

225
00:07:13,849 --> 00:07:15,209
多个FP32单元

226
00:07:15,209 --> 00:07:17,409
后面还出现的Tensor Core

227
00:07:17,409 --> 00:07:19,635
之所以去掉CUDA Core这个概念

228
00:07:19,764 --> 00:07:21,254
是因为每个SM

229
00:07:21,409 --> 00:07:25,009
现在支持FP32和INT32的并发执行

230
00:07:25,009 --> 00:07:28,049
更好的提升运算的吞吐量

231
00:07:28,049 --> 00:07:29,689
提升系统的利用率

232
00:07:30,015 --> 00:07:31,529
在英伟达的硬件架构里面

233
00:07:31,649 --> 00:07:34,089
漏掉了中间非常重要的

234
00:07:34,089 --> 00:07:35,809
黄黄的这个模块

235
00:07:35,809 --> 00:07:37,969
这个模块叫做Warp线程簇

236
00:07:37,969 --> 00:07:38,969
从逻辑上来说

237
00:07:38,969 --> 00:07:39,889
所有的线程

238
00:07:39,889 --> 00:07:42,249
其实是并行或者并发的去执行的

239
00:07:42,249 --> 00:07:44,089
但是从硬件的角度来说

240
00:07:44,089 --> 00:07:45,489
不是所有的线程

241
00:07:45,489 --> 00:07:47,809
都能够在同一时刻

242
00:07:47,809 --> 00:07:48,729
去执行的

243
00:07:48,729 --> 00:07:49,769
因此英伟达

244
00:07:49,769 --> 00:07:51,569
就引用了Warp这个概念

245
00:07:51,569 --> 00:07:53,492
通过Warp去控制线程

246
00:07:53,628 --> 00:07:56,329
通过Warp对线程进行锁同步

247
00:07:56,329 --> 00:07:58,169
然后拆解成具体的指令

248
00:07:58,169 --> 00:08:00,449
给计算单元去执行

249
00:08:01,844 --> 00:08:04,324
现在来到第二个内容的概念

250
00:08:04,324 --> 00:08:05,924
看一下什么是CUDA

251
00:08:06,249 --> 00:08:08,714
CUDA其实是在2006年11月份的时候

252
00:08:08,714 --> 00:08:10,114
英伟达就推出来了

253
00:08:10,114 --> 00:08:11,114
围绕着CUDA

254
00:08:11,314 --> 00:08:13,834
英伟达其实做了非常多的工作

255
00:08:13,834 --> 00:08:17,354
例如推出了非常多的奇奇怪怪的数学库

256
00:08:17,354 --> 00:08:20,834
还推出了很多那些针对CUDA的加速库

257
00:08:20,834 --> 00:08:23,674
当然CUDA还可以支持OpenACC

258
00:08:23,674 --> 00:08:26,794
LVM、Fortran、Python等高级的语言

259
00:08:27,314 --> 00:08:30,385
现在来了解一下CUDA到底是什么

260
00:08:30,567 --> 00:08:32,794
其实CUDA可以非常方便的去控制

261
00:08:32,794 --> 00:08:36,074
GPU里面的各种并行的硬件

262
00:08:36,074 --> 00:08:36,674
而第二个

263
00:08:36,674 --> 00:08:38,994
我觉得它是一个编程的模型

264
00:08:38,994 --> 00:08:40,194
所谓的编程模型

265
00:08:40,194 --> 00:08:41,474
更好的理解

266
00:08:41,474 --> 00:08:43,314
ZOMI觉得它是一个编程的体系

267
00:08:43,314 --> 00:08:45,047
可以使用C或者C++

268
00:08:45,047 --> 00:08:47,274
去写一些高级的的并行的语言

269
00:08:47,274 --> 00:08:48,348
而在底层

270
00:08:48,474 --> 00:08:52,034
CUDA它其实是通过一个LLVM来去实现的

271
00:08:52,034 --> 00:08:53,354
一个CUDA的编译器

272
00:08:53,354 --> 00:08:54,474
非常方便开发者

273
00:08:54,474 --> 00:08:56,270
使用C、C++高级语言

274
00:08:56,270 --> 00:08:58,674
去进行开发CUDA的程序

275
00:08:58,674 --> 00:09:01,354
那第三个就是CUDA可以支持OpenCL

276
00:09:01,354 --> 00:09:01,834
Fortran

277
00:09:01,834 --> 00:09:03,274
还有Direct Compute

278
00:09:03,274 --> 00:09:05,554
这种第三方的语言或者应用程序的接口

279
00:09:05,554 --> 00:09:09,034
从而更好地去服务于更上层的

280
00:09:09,034 --> 00:09:10,114
Application

281
00:09:10,393 --> 00:09:12,034
有了英伟达这一层之后

282
00:09:12,034 --> 00:09:13,394
英伟达的硬件

283
00:09:13,394 --> 00:09:15,114
就可以不断的去演进

284
00:09:15,114 --> 00:09:18,554
从而保持着软硬件的一个独立的解耦

285
00:09:18,554 --> 00:09:20,874
更好地去实现一些硬件的发展

286
00:09:21,421 --> 00:09:23,394
下面再具体的打开一下CUDA

287
00:09:23,394 --> 00:09:25,274
相关的一个软件集成

288
00:09:25,274 --> 00:09:26,394
首先中间这一层

289
00:09:26,394 --> 00:09:27,394
叫做CUDA

290
00:09:27,394 --> 00:09:29,354
里面就包括了CUDA的Compiler

291
00:09:29,474 --> 00:09:31,834
还有CUDA C++的一个核心

292
00:09:31,834 --> 00:09:33,634
当然还有一些简单的工具

293
00:09:33,634 --> 00:09:35,754
在旁边就是CUDA的Driver

294
00:09:35,754 --> 00:09:36,514
它的驱动

295
00:09:36,514 --> 00:09:38,354
因为很多人可以理解为CUDA

296
00:09:38,554 --> 00:09:41,754
它可以非常方便去驱动英伟达的GPU

297
00:09:41,754 --> 00:09:43,754
里面会提供很多相关的API的库

298
00:09:43,914 --> 00:09:44,674
内存的管理

299
00:09:44,834 --> 00:09:46,754
还有图形图像的一些

300
00:09:46,754 --> 00:09:48,074
API的接口

301
00:09:48,074 --> 00:09:50,354
在网上就是CUDA-X Libraries了

302
00:09:50,354 --> 00:09:51,034
CUDA-X Libraries

303
00:09:51,034 --> 00:09:54,194
其实在针对AI或者神经网络深度学习

304
00:09:54,194 --> 00:09:55,154
这一个领域里面

305
00:09:55,434 --> 00:09:58,034
就推出了非常多的加速库了

306
00:09:58,034 --> 00:09:58,914
训练加速库了

307
00:09:58,914 --> 00:09:59,634
推理加速库了

308
00:09:59,634 --> 00:10:00,794
还有基础加速库

309
00:10:00,794 --> 00:10:01,834
就非常的多了

310
00:10:01,834 --> 00:10:03,354
在网上就是去对接

311
00:10:03,354 --> 00:10:04,834
一些AI框架了

312
00:10:04,834 --> 00:10:06,194
Pytorch、ONNX等各种各样

313
00:10:06,194 --> 00:10:08,154
非常多的AI框架

314
00:10:08,154 --> 00:10:10,983
再往上就是真正的AI的一些功能了

315
00:10:10,983 --> 00:10:14,034
这个整个就是CUDA的软硬件生态站

316
00:10:14,874 --> 00:10:16,114
来到了第三个内容

317
00:10:16,114 --> 00:10:16,954
现在来看看

318
00:10:16,954 --> 00:10:18,234
CUDA的层次结构

319
00:10:18,234 --> 00:10:19,914
来到CUDA的层次结构里面

320
00:10:19,914 --> 00:10:21,034
很重要的一个概念

321
00:10:21,034 --> 00:10:22,074
之前讲到

322
00:10:22,074 --> 00:10:24,806
CUDA最重要一个执行单位就是线程

323
00:10:24,955 --> 00:10:26,874
里面每一条弯弯弯弯的

324
00:10:26,874 --> 00:10:28,674
都是线程

325
00:10:28,674 --> 00:10:30,794
而线程最主要最重要的是

326
00:10:30,794 --> 00:10:31,474
网格

327
00:10:31,474 --> 00:10:33,634
网格里面又分回很多的块

328
00:10:33,634 --> 00:10:35,474
块内又很多的线程

329
00:10:35,474 --> 00:10:36,594
这些块件的线程

330
00:10:36,714 --> 00:10:37,634
都是独立执行

331
00:10:37,634 --> 00:10:39,634
中间通过本地共享内存

332
00:10:39,634 --> 00:10:42,074
就Local Memory去交换数据

333
00:10:42,320 --> 00:10:44,034
接下来真正的理解一下

334
00:10:44,034 --> 00:10:46,354
线程内的一些层次的结构

335
00:10:46,354 --> 00:10:49,154
首先第一个概念就是所谓的kernel

336
00:10:49,314 --> 00:10:50,314
在了解kernel之前

337
00:10:50,394 --> 00:10:51,834
看两个概念

338
00:10:51,834 --> 00:10:53,074
一个就是Host

339
00:10:53,074 --> 00:10:54,594
Host主要是指CPU

340
00:10:54,594 --> 00:10:55,674
另外一个是Device

341
00:10:55,674 --> 00:10:57,754
Device主要是指GPU

342
00:10:57,754 --> 00:11:00,594
Host跟Device之间是交互去执行的

343
00:11:00,594 --> 00:11:02,954
Host跟Device之间可以进行通讯

344
00:11:02,954 --> 00:11:03,354
这样的话

345
00:11:03,354 --> 00:11:06,794
就可以非常方便数据进行传出传入

346
00:11:06,794 --> 00:11:08,514
通过Device，通过GPU

347
00:11:08,514 --> 00:11:09,794
做一些并行的操作

348
00:11:09,794 --> 00:11:10,794
并行操作完之后

349
00:11:10,914 --> 00:11:11,874
把数据的结果

350
00:11:11,874 --> 00:11:14,154
丢给Host CPU进行处理

351
00:11:14,154 --> 00:11:15,914
整体就是这么一个循环

352
00:11:15,914 --> 00:11:16,874
接下来看一下

353
00:11:16,874 --> 00:11:18,714
什么为之kernel

354
00:11:18,714 --> 00:11:19,634
kernel这个概念

355
00:11:19,794 --> 00:11:20,714
是在CUDA里面

356
00:11:20,714 --> 00:11:23,274
最核心的一个重要概念

357
00:11:23,490 --> 00:11:25,314
CUDA程序里面主要就是指

358
00:11:25,314 --> 00:11:27,634
调用CUDA的核函数

359
00:11:27,634 --> 00:11:29,034
也就是kernel函数

360
00:11:29,074 --> 00:11:31,154
来执行并行的计算

361
00:11:31,314 --> 00:11:33,834
这就是第一个讲到的概念

362
00:11:33,834 --> 00:11:35,114
而在第二个概念里面

363
00:11:35,194 --> 00:11:36,642
会等一下看到

364
00:11:36,794 --> 00:11:38,314
有很多Host的代码

365
00:11:38,434 --> 00:11:40,594
实际上是在CPU上面去执行的

366
00:11:40,594 --> 00:11:42,354
但是当数据遇到一些

367
00:11:42,354 --> 00:11:44,274
要并发并行处理的任务

368
00:11:44,274 --> 00:11:45,594
就会使用CUDA

369
00:11:45,594 --> 00:11:47,674
就会使用CUDA进行编程

370
00:11:47,674 --> 00:11:49,434
在编译完之后执行的时候

371
00:11:49,434 --> 00:11:51,154
可能有部分代码是在Host

372
00:11:51,154 --> 00:11:52,794
主机端去执行

373
00:11:52,794 --> 00:11:53,554
有部分代码

374
00:11:53,674 --> 00:11:56,154
就会在Device上面去执行

375
00:11:56,295 --> 00:11:59,194
而这个程序只要是CUDA执行的部分

376
00:11:59,194 --> 00:12:01,354
统一都叫做kernel

377
00:12:02,034 --> 00:12:03,034
接下来看两个

378
00:12:03,034 --> 00:12:04,554
真实的具体的代码的例子

379
00:12:04,685 --> 00:12:07,074
这个代码例子只是实现很简单的功能

380
00:12:07,074 --> 00:12:09,114
迭代很多次X+Y

381
00:12:09,114 --> 00:12:11,114
然后把结果存到Y里面

382
00:12:11,251 --> 00:12:13,674
因为CPU遵循冯诺依曼架构

383
00:12:13,674 --> 00:12:14,914
所以执行的时候

384
00:12:15,154 --> 00:12:16,794
都是线程级别的去执行

385
00:12:16,794 --> 00:12:18,274
一条一条指令的去执行

386
00:12:18,274 --> 00:12:19,554
因此在for循环里面

387
00:12:19,554 --> 00:12:21,194
就会执行很多次迭代

388
00:12:21,194 --> 00:12:22,874
所以有个迭代for n

389
00:12:23,234 --> 00:12:24,154
接下来看一下

390
00:12:24,154 --> 00:12:26,274
程序的主线有个main

391
00:12:26,274 --> 00:12:27,434
然后定一个数据

392
00:12:27,434 --> 00:12:28,914
开辟一个内存空间

393
00:12:28,914 --> 00:12:31,674
接着去定义初始化的数据

394
00:12:31,674 --> 00:12:32,874
然后调用add

395
00:12:32,874 --> 00:12:34,114
刚才那个函数

396
00:12:34,234 --> 00:12:36,234
执行想要的计算

397
00:12:36,234 --> 00:12:38,874
最后销毁内存

398
00:12:39,107 --> 00:12:41,274
看一下CUDA它是怎么执行的

399
00:12:41,274 --> 00:12:42,354
首先CUDA执行

400
00:12:42,354 --> 00:12:44,434
在前面加了个global

401
00:12:44,434 --> 00:12:45,268
在CUDA里面

402
00:12:45,268 --> 00:12:47,914
会有一个变量声明符global

403
00:12:48,081 --> 00:12:50,874
然后就去执行具体的函数了

404
00:12:50,874 --> 00:12:51,914
这里面就是具体的

405
00:12:51,914 --> 00:12:53,274
函数的执行方式

406
00:12:53,274 --> 00:12:55,394
接着在内存开辟的时候

407
00:12:55,554 --> 00:12:56,594
就直接声明CUDA

408
00:12:56,594 --> 00:12:58,509
在CUDA里面开辟一个内存

409
00:12:58,594 --> 00:12:59,754
然后在CUDA里面

410
00:12:59,754 --> 00:13:02,714
对这个数据进行具体的赋值

411
00:13:02,714 --> 00:13:03,874
接着调用add

412
00:13:03,874 --> 00:13:06,114
1 1 N x y

413
00:13:06,114 --> 00:13:07,074
把数据传进去

414
00:13:07,234 --> 00:13:08,634
就真正在执行的时候

415
00:13:08,634 --> 00:13:12,301
就会执行GPU里面的kernel的函数

416
00:13:12,474 --> 00:13:13,274
执行完之后

417
00:13:13,274 --> 00:13:15,554
就通过声明cudaFree

418
00:13:15,594 --> 00:13:17,331
去释放内存

419
00:13:18,038 --> 00:13:20,314
刚才在CPU里面去执行的是.cpp

420
00:13:20,314 --> 00:13:23,594
而在英伟达里面执行是.cu

421
00:13:23,594 --> 00:13:25,794
这里面可以看到其实有部分代码

422
00:13:25,794 --> 00:13:27,194
执行的是在CPU

423
00:13:27,194 --> 00:13:28,034
有部分代码

424
00:13:28,234 --> 00:13:30,434
就执行在CUDA里面

425
00:13:31,243 --> 00:13:31,829
回到正题

426
00:13:31,829 --> 00:13:33,874
看看线程的层次结构

427
00:13:33,874 --> 00:13:36,234
线程最外面的一个概念是Grid

428
00:13:36,234 --> 00:13:37,514
网格

429
00:13:37,514 --> 00:13:39,154
kernel在Device

430
00:13:39,154 --> 00:13:40,447
就GPU执行的时候

431
00:13:40,554 --> 00:13:43,674
实际上会启动非常多的线程

432
00:13:43,674 --> 00:13:45,994
就很多刚才一条条弯曲的曲线

433
00:13:45,994 --> 00:13:47,834
而一个kernel所启动的时候

434
00:13:48,114 --> 00:13:49,114
所有的线程

435
00:13:49,114 --> 00:13:51,234
都会封装在一个Grid里面

436
00:13:51,234 --> 00:13:52,354
同一个网格上面

437
00:13:52,514 --> 00:13:55,034
线程是共享全局内存的

438
00:13:55,034 --> 00:13:56,674
也是刚才声明的

439
00:13:56,674 --> 00:13:57,914
global的意思

440
00:13:57,914 --> 00:13:59,914
里面的数据都是共享的

441
00:13:59,914 --> 00:14:01,994
而Grid是作为第一层

442
00:14:02,434 --> 00:14:04,714
第二个内容就是线程块

443
00:14:04,714 --> 00:14:06,154
Thread Block

444
00:14:06,354 --> 00:14:08,034
实际上叫Block就行了

445
00:14:08,034 --> 00:14:10,074
刚才讲到了Grid的网格

446
00:14:10,074 --> 00:14:12,194
网格里面的概念就是Block

447
00:14:12,194 --> 00:14:15,874
一个Block就可以包括非常多的线程

448
00:14:15,874 --> 00:14:17,730
Block跟Block之间

449
00:14:17,874 --> 00:14:19,498
是并行的去执行

450
00:14:19,498 --> 00:14:21,132
也就是Block00跟Block10

451
00:14:21,132 --> 00:14:22,951
可以同时的进行执行

452
00:14:22,951 --> 00:14:24,634
那它们同时进行执行的时候

453
00:14:24,634 --> 00:14:27,674
就没有必要去让它们之间相互通讯

454
00:14:27,674 --> 00:14:29,114
而且也没有执行的顺序

455
00:14:29,114 --> 00:14:31,034
大家都并行的去执行

456
00:14:31,034 --> 00:14:32,194
这些Block里面的

457
00:14:32,194 --> 00:14:33,627
各自的线程就好了

458
00:14:33,911 --> 00:14:35,714
因此就会说

459
00:14:35,714 --> 00:14:38,514
Block里面的线程是可以同步的

460
00:14:38,514 --> 00:14:42,434
通过共享内存进行通讯和数据之间的传输

461
00:14:42,434 --> 00:14:44,625
而第三层就是来到了

462
00:14:44,625 --> 00:14:48,234
线程最小的一个逻辑计算单位

463
00:14:48,694 --> 00:14:50,354
在整个所有的Device

464
00:14:50,354 --> 00:14:52,354
就是CUDA的整个程序里面

465
00:14:52,354 --> 00:14:53,714
真正执行的

466
00:14:53,714 --> 00:14:55,434
都是由最小的这些Tread

467
00:14:55,434 --> 00:14:56,954
线程来执行

468
00:14:56,954 --> 00:14:59,154
而多个线程组成Block

469
00:14:59,154 --> 00:15:02,518
多个Block又组成Grid这种方式

470
00:15:03,049 --> 00:15:04,394
现在来对一下

471
00:15:04,394 --> 00:15:06,954
第一个在刚开始的概念里面

472
00:15:06,994 --> 00:15:08,834
就去讲了英伟达的硬件的结构

473
00:15:08,834 --> 00:15:10,834
还有硬件的一些相关的概念

474
00:15:10,874 --> 00:15:12,794
接着去看了一下CUDA

475
00:15:12,994 --> 00:15:15,074
现在把CUDA跟英伟达之间的

476
00:15:15,074 --> 00:15:16,994
硬件架构把它联系起来

477
00:15:16,994 --> 00:15:20,034
从软件看到执行的是具体的线程

478
00:15:20,034 --> 00:15:21,238
线程又包成Block

479
00:15:21,238 --> 00:15:22,238
Block又包成Grid

480
00:15:22,274 --> 00:15:23,714
而在硬件上面去执行

481
00:15:23,874 --> 00:15:26,034
线程实际上是执行

482
00:15:26,034 --> 00:15:27,474
在CUDA Core里面的

483
00:15:27,474 --> 00:15:29,914
每一个线程就对应的CUDA Core

484
00:15:29,914 --> 00:15:32,154
当然了线程的数量是超配的

485
00:15:32,154 --> 00:15:33,314
软件上超配

486
00:15:33,314 --> 00:15:35,634
硬件上面执行是有限的

487
00:15:35,634 --> 00:15:37,114
所以会通过刚才的Wrap

488
00:15:37,114 --> 00:15:38,714
进行一个调度和同步

489
00:15:38,714 --> 00:15:40,034
然后看看第二个

490
00:15:40,034 --> 00:15:41,394
就是第二层的目录

491
00:15:41,394 --> 00:15:42,234
就是Thread Block

492
00:15:42,234 --> 00:15:42,954
块

493
00:15:42,954 --> 00:15:45,594
块就是由SM去进行执行

494
00:15:45,634 --> 00:15:46,794
最后就是网格

495
00:15:46,794 --> 00:15:49,434
网格就是由大量的SM进行堆叠

496
00:15:49,434 --> 00:15:51,434
堆叠起来就变成TPC

497
00:15:51,434 --> 00:15:52,874
还有GPC了

498
00:15:53,994 --> 00:15:56,034
现在来到最后一个内容

499
00:15:56,034 --> 00:15:57,114
算力的计算

500
00:15:57,114 --> 00:15:58,834
特别是英伟达的算力

501
00:15:58,834 --> 00:16:00,234
是怎么去算出来的

502
00:16:00,234 --> 00:16:01,634
有了这个算力峰值之后

503
00:16:01,834 --> 00:16:02,834
就很好的去评量

504
00:16:02,834 --> 00:16:04,274
在训练大模型

505
00:16:04,274 --> 00:16:06,874
或者在执行大模型训练的过程当中

506
00:16:06,874 --> 00:16:10,034
怎么去评价算力的利用率

507
00:16:10,154 --> 00:16:11,954
现在看一下GPU的算力

508
00:16:12,074 --> 00:16:15,234
其实跟三个因素有非常强烈的关系

509
00:16:15,434 --> 00:16:17,554
第一个就是核心的个数

510
00:16:17,554 --> 00:16:20,114
特别是指SM的数量

511
00:16:20,114 --> 00:16:22,154
第二个是指核心的频率

512
00:16:22,154 --> 00:16:24,314
第三个就是指每个Core里面的

513
00:16:24,314 --> 00:16:26,674
单时钟周期内的一个算力

514
00:16:26,909 --> 00:16:29,154
整体的峰值就是由这三个数相乘

515
00:16:29,154 --> 00:16:33,074
而得到peakflops的一个具体的值了

516
00:16:33,274 --> 00:16:35,274
现在看看一下具体的计算公式

517
00:16:35,274 --> 00:16:38,154
Fclk就是指GPU时钟周期内的

518
00:16:38,154 --> 00:16:39,914
一个指令执行数量

519
00:16:39,914 --> 00:16:42,474
单位是FLOPS/Cycle

520
00:16:42,514 --> 00:16:44,795
第二个就是NSM

521
00:16:44,795 --> 00:16:46,415
具体SM就是英伟达的

522
00:16:46,415 --> 00:16:47,394
Streaming Multi-Processor

523
00:16:47,394 --> 00:16:48,794
里面的一个数量

524
00:16:48,794 --> 00:16:50,354
SM的一个核数

525
00:16:50,569 --> 00:16:52,314
第三个Freq

526
00:16:52,495 --> 00:16:54,398
是指运行的频率

527
00:16:55,034 --> 00:16:56,394
今天的内容就这么多

528
00:16:56,394 --> 00:16:58,194
现在总结一下

529
00:16:58,194 --> 00:17:00,754
首先GPU里面有非常多的概念

530
00:17:00,754 --> 00:17:03,714
重点展开了SM还有Core

531
00:17:03,714 --> 00:17:05,594
另外的话SM里面实际上

532
00:17:05,674 --> 00:17:08,634
还包含很多各种各样的一些核

533
00:17:08,634 --> 00:17:10,554
而CUDA Core里面也包含了

534
00:17:10,554 --> 00:17:12,194
非常多相关的概念

535
00:17:12,194 --> 00:17:14,154
这些其实都没有展开

536
00:17:14,154 --> 00:17:17,034
只是具体的看一下大概念的意思

537
00:17:17,250 --> 00:17:19,074
有了一个对硬件的了解

538
00:17:19,074 --> 00:17:20,954
从Core、SM到Driver

539
00:17:20,954 --> 00:17:22,714
最后了解了一下

540
00:17:22,714 --> 00:17:25,594
CUDA里面的一个线程分级的概念

541
00:17:25,594 --> 00:17:26,954
线程的分级Thread Block

542
00:17:26,954 --> 00:17:29,834
还有Grid、单线程块，还有网格

543
00:17:29,834 --> 00:17:33,234
最后就把硬件跟软件之间的关系

544
00:17:33,234 --> 00:17:34,634
给大家汇报完

545
00:17:34,634 --> 00:17:36,394
今天的内容就到这里为止

546
00:17:36,394 --> 00:17:37,114
谢谢各位

547
00:17:37,114 --> 00:17:38,234
拜了个拜

548
00:17:38,588 --> 00:17:40,268
卷的不行了

549
00:17:40,268 --> 00:17:41,708
记得一键三连加关注

550
00:17:42,068 --> 00:17:43,468
所有的内容都会开源

551
00:17:43,468 --> 00:17:45,388
在下面这条链接里面

552
00:17:45,668 --> 00:17:46,708
拜了个拜

