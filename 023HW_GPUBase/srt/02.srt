1
00:00:01,011 --> 00:00:03,699
字幕生成：慎独    校对：游吟外星人

2
00:00:05,033 --> 00:00:06,181
Hi，大家好

3
00:00:06,181 --> 00:00:07,952
蓦然回首，肚子依然在抖

4
00:00:07,952 --> 00:00:10,189
我是中年发福的ZOMI

5
00:00:14,092 --> 00:00:18,060
今天还是在整个AI芯片系列里面的GPU详解

6
00:00:18,060 --> 00:00:21,470
去看看GPU里面的AI编程的本质

7
00:00:21,470 --> 00:00:24,792
说白了英伟达的GPU它整体架构

8
00:00:24,792 --> 00:00:28,422
其实在近10年来已经发展了非常多代

9
00:00:28,422 --> 00:00:30,477
但是有一个最大的问题

10
00:00:30,477 --> 00:00:33,753
就是为什么GPU适用于整个AI的计算

11
00:00:33,753 --> 00:00:36,100
或者换一个方式来去问

12
00:00:36,100 --> 00:00:39,044
为什么AI的训练要使用GPU

13
00:00:39,044 --> 00:00:40,870
而不是使用CPU呢

14
00:00:40,973 --> 00:00:44,400
这个就是今天要给大家去汇报的一个内容

15
00:00:44,400 --> 00:00:46,600
看看GPU的AI编程的本质

16
00:00:47,024 --> 00:00:51,091
首先今天会分开4个内容给大家去汇报

17
00:00:51,091 --> 00:00:52,891
首先第一个就是来看看

18
00:00:52,891 --> 00:00:56,491
回顾一下卷积的计算到底是怎么卷的

19
00:00:56,491 --> 00:00:57,491
怎么算的

20
00:00:57,491 --> 00:00:58,492
第二个内容

21
00:00:58,492 --> 00:01:01,923
就是来到了GPU的线程分级

22
00:01:01,923 --> 00:01:03,963
那第三个就是回顾一下

23
00:01:03,963 --> 00:01:06,963
整个AI的计算模式和线程之间的关系

24
00:01:06,963 --> 00:01:09,363
把前面两个内容结合起来

25
00:01:09,500 --> 00:01:11,021
最后一个内容就是

26
00:01:11,021 --> 00:01:13,818
GPU既然能够对AI进行编程

27
00:01:13,818 --> 00:01:15,298
对AI进行加速

28
00:01:15,298 --> 00:01:17,938
那看看AI里面最重要的一个运算

29
00:01:17,938 --> 00:01:20,818
就是矩阵乘这个运算GMM

30
00:01:20,818 --> 00:01:23,098
怎么去提升算力利用率

31
00:01:23,098 --> 00:01:25,098
或者提升算法利用率的

32
00:01:25,947 --> 00:01:28,427
正式进入到第一个小内容里面

33
00:01:28,427 --> 00:01:30,227
回顾一下在之前的章节里面

34
00:01:30,227 --> 00:01:31,827
给大家去汇报的

35
00:01:31,827 --> 00:01:34,027
就是GPU的线程的机制

36
00:01:34,027 --> 00:01:37,787
那左边的这个就是A100的一个简单的架构图

37
00:01:37,787 --> 00:01:39,747
下面有分层的一些Cache

38
00:01:39,747 --> 00:01:41,587
或者分层的缓存

39
00:01:41,587 --> 00:01:45,307
针对每一个SM里面的具体的计算单元

40
00:01:45,307 --> 00:01:48,667
A100的SM里面就有非常多的warp

41
00:01:48,667 --> 00:01:51,147
那这些warp就是线程簇

42
00:01:51,147 --> 00:01:53,427
通过不同的SM里面有大量的warp

43
00:01:53,427 --> 00:01:57,467
而每个warp可以同时去并行执行多个线程

44
00:01:57,467 --> 00:02:00,387
这里面就有非常多的寄存器

45
00:02:00,387 --> 00:02:02,427
还有现成的调度器

46
00:02:02,427 --> 00:02:05,427
GPU内部提供大量的线程和多级的缓存

47
00:02:05,427 --> 00:02:08,587
使得GPU整体的吞吐率变得非常的高

48
00:02:08,587 --> 00:02:11,147
能够处理非常夸张并行的任务

49
00:02:11,147 --> 00:02:12,467
了解到这点之后

50
00:02:12,467 --> 00:02:14,987
就知道GPU的本质

51
00:02:14,987 --> 00:02:16,307
在于并行

52
00:02:16,307 --> 00:02:19,027
而CPU的本质是希望尽可能的去加快

53
00:02:19,027 --> 00:02:21,267
每一个指令的运算

54
00:02:21,267 --> 00:02:24,067
这就是它们之间的最大的一个不同

55
00:02:25,427 --> 00:02:27,267
在之前推理引擎系列里面

56
00:02:27,267 --> 00:02:30,187
其实ZOMI老师已经给大家去汇报过

57
00:02:30,187 --> 00:02:33,027
一些卷积计算的详细的内容

58
00:02:33,027 --> 00:02:34,947
今天简单的去回顾一下

59
00:02:34,947 --> 00:02:36,747
卷积是怎么卷的

60
00:02:36,747 --> 00:02:39,227
首先看一下下面的这个图

61
00:02:39,227 --> 00:02:41,414
左边的这一块内容

62
00:02:41,414 --> 00:02:41,424
就是一些卷积核模板

63
00:02:41,424 --> 00:02:44,574
就是一些卷积核模板

64
00:02:44,587 --> 00:02:45,867
或者kernel

65
00:02:45,867 --> 00:02:48,147
右边这一个就是图片

66
00:02:48,147 --> 00:02:50,787
需要处理需要带卷积的图片

67
00:02:50,787 --> 00:02:52,451
最后模板里面的内容

68
00:02:52,467 --> 00:02:55,027
就是卷积核每一个元素

69
00:02:55,027 --> 00:02:56,945
跟图片里面的每一个元素

70
00:02:56,947 --> 00:02:58,827
逐元素相乘再相加

71
00:02:58,827 --> 00:03:01,387
就得到最终的一个输出的

72
00:03:01,387 --> 00:03:02,067
feature map

73
00:03:02,067 --> 00:03:03,347
或者特征图

74
00:03:03,347 --> 00:03:04,227
实际上

75
00:03:04,227 --> 00:03:06,160
在NPU、GPU和CPU里面

76
00:03:06,187 --> 00:03:08,581
真正去执行卷积运算的时候

77
00:03:08,587 --> 00:03:10,787
不会通过刚才那种划窗的方式

78
00:03:10,907 --> 00:03:11,987
对图片

79
00:03:11,987 --> 00:03:13,827
对数据进行卷积

80
00:03:13,827 --> 00:03:16,667
而是把卷积变成image to clone

81
00:03:16,667 --> 00:03:19,787
把图片变成一个矩阵的向量

82
00:03:19,947 --> 00:03:22,747
去模拟去恢复卷机的运算

83
00:03:22,947 --> 00:03:24,307
原来的图片

84
00:03:24,307 --> 00:03:26,347
会把它进行一个重排

85
00:03:26,347 --> 00:03:27,867
把图片里面的一个窗口

86
00:03:27,867 --> 00:03:29,147
123789

87
00:03:29,147 --> 00:03:30,507
逐元素排成一排

88
00:03:30,507 --> 00:03:32,707
然后再按channel数往后排

89
00:03:32,707 --> 00:03:33,667
通过这种方式

90
00:03:33,827 --> 00:03:35,187
对图像

91
00:03:35,187 --> 00:03:37,147
进行一个矩阵的重排

92
00:03:37,267 --> 00:03:39,081
接着kernel

93
00:03:39,082 --> 00:03:40,121
把Filter

94
00:03:40,122 --> 00:03:42,227
把卷积核进行重排

95
00:03:42,387 --> 00:03:43,187
卷积核的重排

96
00:03:43,187 --> 00:03:44,187
其实跟刚才一样

97
00:03:44,187 --> 00:03:46,707
把每一个卷积核123456789

98
00:03:46,707 --> 00:03:49,667
然后重新的展开成为一行

99
00:03:49,667 --> 00:03:52,387
不断地去堆叠channel的数量

100
00:03:52,387 --> 00:03:55,389
然后每一列就是代表有多少个N

101
00:03:55,389 --> 00:03:55,427
有多少个卷积核

102
00:03:55,427 --> 00:03:57,109
有多少个卷积核

103
00:03:57,147 --> 00:03:57,947
通过这种方式

104
00:03:58,147 --> 00:03:59,347
就把图片

105
00:03:59,347 --> 00:03:59,787
feature map

106
00:03:59,787 --> 00:04:00,707
把kernel

107
00:04:00,867 --> 00:04:02,987
变成了两个大的矩阵

108
00:04:02,987 --> 00:04:04,787
通过矩阵相乘的方式

109
00:04:04,787 --> 00:04:06,947
得到最终的feature map

110
00:04:06,947 --> 00:04:07,827
这种方式

111
00:04:07,827 --> 00:04:10,147
就去模拟卷积的运算

112
00:04:10,467 --> 00:04:11,789
简单一句话说明

113
00:04:11,812 --> 00:04:13,227
就是卷积的运算

114
00:04:13,227 --> 00:04:15,907
转换成为两个矩阵相乘

115
00:04:16,147 --> 00:04:17,507
的一个求解

116
00:04:17,507 --> 00:04:19,507
最终得到卷积的输出

117
00:04:19,707 --> 00:04:20,764
那上面这个卷积

118
00:04:20,808 --> 00:04:22,848
原来这个是输入的图片

119
00:04:22,867 --> 00:04:24,667
这个右边的绿色

120
00:04:24,667 --> 00:04:26,107
卷积核

121
00:04:26,107 --> 00:04:27,427
通过两个相乘

122
00:04:27,427 --> 00:04:28,387
或者滑窗的方式

123
00:04:28,547 --> 00:04:29,907
得到feature map

124
00:04:29,907 --> 00:04:31,627
实际上在运行的时候

125
00:04:31,867 --> 00:04:33,587
会把输入的图片进行展开

126
00:04:33,587 --> 00:04:35,107
把卷积核进行展开

127
00:04:35,107 --> 00:04:36,787
然后通过矩阵相乘的方式

128
00:04:36,787 --> 00:04:38,787
最后一步就是clone to image

129
00:04:38,787 --> 00:04:40,107
逆变的过程

130
00:04:40,107 --> 00:04:42,067
恢复成为feature map

131
00:04:42,067 --> 00:04:43,867
就是在GPU或者NPU里面

132
00:04:43,867 --> 00:04:47,027
真正执行卷积的计算求解的方式

133
00:04:47,707 --> 00:04:49,403
了解完卷积的运算之后

134
00:04:49,427 --> 00:04:50,067
就知道

135
00:04:50,067 --> 00:04:51,387
AI的计算的本质

136
00:04:51,387 --> 00:04:52,827
其实就是矩阵乘

137
00:04:52,827 --> 00:04:53,907
那接着看看

138
00:04:53,907 --> 00:04:55,827
GPU的线程的分级

139
00:04:55,827 --> 00:04:57,707
线程的等级制度

140
00:04:57,707 --> 00:05:01,107
首先在整个AI的计算模式里面

141
00:05:01,107 --> 00:05:02,347
并不是所有的计算

142
00:05:02,347 --> 00:05:03,787
都可以线程独立的

143
00:05:03,787 --> 00:05:05,987
我简单的举几个例子

144
00:05:06,457 --> 00:05:07,297
Element-wise

145
00:05:07,307 --> 00:05:08,787
就是逐元素相加

146
00:05:08,787 --> 00:05:11,067
或逐元素相乘的这种操作

147
00:05:11,067 --> 00:05:12,107
之前的例子

148
00:05:12,107 --> 00:05:14,555
AX+Y简单的线性操作

149
00:05:14,568 --> 00:05:17,288
把一个元素进行一个简单的处理

150
00:05:17,307 --> 00:05:18,747
得到另外一个元素

151
00:05:18,747 --> 00:05:20,667
但是在真正卷积

152
00:05:20,667 --> 00:05:22,533
或者真正AI运算过程当中

153
00:05:22,547 --> 00:05:23,907
会有大量的卷积

154
00:05:23,907 --> 00:05:24,826
大量的卷积

155
00:05:24,841 --> 00:05:27,401
就涉及到元素之间

156
00:05:27,427 --> 00:05:28,627
其实是有交互的

157
00:05:28,627 --> 00:05:30,787
数据之间是有交互的

158
00:05:30,787 --> 00:05:32,187
我要算一个元素

159
00:05:32,187 --> 00:05:34,067
可能就需要周边的其他元素

160
00:05:34,067 --> 00:05:36,120
周边的数据进行配合

161
00:05:36,296 --> 00:05:38,707
第三种就是All to All的方式

162
00:05:38,707 --> 00:05:39,967
例如快速傅里叶变化

163
00:05:39,967 --> 00:05:40,947
傅里叶变换这种

164
00:05:40,947 --> 00:05:42,867
一个元素的求解

165
00:05:42,867 --> 00:05:44,227
得到另外一个元素

166
00:05:44,227 --> 00:05:45,598
数据与数据之间

167
00:05:45,641 --> 00:05:48,462
并不能够做到完全的线程独立

168
00:05:48,488 --> 00:05:51,907
下面以中间这个卷积作为例子

169
00:05:51,907 --> 00:05:52,961
看看Local Memory

170
00:05:52,987 --> 00:05:53,958
在GPU里面

171
00:05:54,027 --> 00:05:54,787
跟Thread

172
00:05:54,787 --> 00:05:55,707
跟线程

173
00:05:55,707 --> 00:05:57,347
是怎么去配合工作的

174
00:05:57,816 --> 00:06:00,306
首先现在有一个图片

175
00:06:00,347 --> 00:06:01,907
这个图片是一只猫猫

176
00:06:01,907 --> 00:06:02,787
第一步

177
00:06:02,787 --> 00:06:04,547
会用网格，就是Grid

178
00:06:04,547 --> 00:06:07,147
对整个图片进行覆盖

179
00:06:07,147 --> 00:06:08,747
切分成一个一个块

180
00:06:08,987 --> 00:06:10,547
其中就拿出来

181
00:06:10,547 --> 00:06:12,627
中间的一个块进行处理

182
00:06:12,723 --> 00:06:14,106
每一个Grid里面

183
00:06:14,195 --> 00:06:16,777
实际上还会分开不同的Block

184
00:06:16,787 --> 00:06:19,347
这里面的Block就有可能会重叠的

185
00:06:19,347 --> 00:06:20,187
在GPU里面

186
00:06:20,187 --> 00:06:22,307
Block会去独立的执行的

187
00:06:22,467 --> 00:06:24,582
在第三步的时候

188
00:06:24,627 --> 00:06:26,867
Block里面会有大量的线程

189
00:06:26,867 --> 00:06:28,147
大量的Thread

190
00:06:28,227 --> 00:06:30,147
通过本地的数据共享

191
00:06:30,387 --> 00:06:32,917
或者叫做Local Data Memory

192
00:06:32,987 --> 00:06:34,467
来去进行计算

193
00:06:34,467 --> 00:06:35,547
每个元素

194
00:06:35,547 --> 00:06:36,947
或者每一个像素点

195
00:06:37,227 --> 00:06:39,747
都会给一个线程进行计算

196
00:06:39,947 --> 00:06:42,547
这个时候就变成了整个线程

197
00:06:42,547 --> 00:06:44,227
是分层分级的

198
00:06:44,227 --> 00:06:46,027
再简单的看看

199
00:06:46,027 --> 00:06:48,107
刚才讲到的一些概念

200
00:06:48,147 --> 00:06:50,427
首先网格

201
00:06:50,427 --> 00:06:51,227
就是Grid

202
00:06:51,227 --> 00:06:53,867
会表示所有需要执行的任务

203
00:06:53,867 --> 00:06:55,747
这里面会有大量的线程

204
00:06:55,747 --> 00:06:57,987
网格是一个最高层的概念

205
00:06:57,987 --> 00:06:58,867
网格之下

206
00:06:59,187 --> 00:07:02,107
就会分成非常多的Block

207
00:07:02,256 --> 00:07:03,519
每一个Block里面

208
00:07:03,547 --> 00:07:06,267
又有非常多的这种线程

209
00:07:06,267 --> 00:07:08,571
块中，Block之间的线程

210
00:07:08,587 --> 00:07:10,027
是互相独立的

211
00:07:10,027 --> 00:07:11,427
也就是Block A

212
00:07:11,427 --> 00:07:12,227
跟Block B

213
00:07:12,227 --> 00:07:13,507
之间里面的线程

214
00:07:13,507 --> 00:07:14,947
是独立执行的

215
00:07:14,947 --> 00:07:16,987
而Block里面的线程

216
00:07:16,987 --> 00:07:19,907
它们是共享本地内存数据的

217
00:07:19,907 --> 00:07:22,307
就共享Local Memory

218
00:07:22,429 --> 00:07:23,053
这个时候

219
00:07:23,067 --> 00:07:25,107
就在每一个Block里面

220
00:07:25,107 --> 00:07:26,427
每一个块里面

221
00:07:26,427 --> 00:07:29,439
就可以执行相同相关的操作

222
00:07:29,583 --> 00:07:31,907
刚才其实忽略了一个点

223
00:07:31,907 --> 00:07:35,107
就是网格里面的Block里面的块

224
00:07:35,227 --> 00:07:38,027
其实在GPU里面是超额分配的

225
00:07:38,027 --> 00:07:39,307
通过超额的分配

226
00:07:39,307 --> 00:07:40,507
大量的这些Block

227
00:07:40,507 --> 00:07:42,747
而Block里面又有大量的线程

228
00:07:42,747 --> 00:07:43,427
每个线程

229
00:07:43,547 --> 00:07:44,867
就可以对像素

230
00:07:44,867 --> 00:07:46,307
进行大量的操作

231
00:07:46,307 --> 00:07:47,958
而Block的超量的分配

232
00:07:48,107 --> 00:07:49,747
就可以通过计算

233
00:07:49,747 --> 00:07:52,227
去掩盖延时的问题

234
00:07:52,366 --> 00:07:53,387
不管是网格也好

235
00:07:53,387 --> 00:07:54,667
不管是线程也好

236
00:07:54,667 --> 00:07:56,227
都会在Block里面

237
00:07:56,227 --> 00:07:57,787
真正的去执行

238
00:07:58,129 --> 00:07:59,787
回到带宽的这个问题

239
00:07:59,787 --> 00:08:01,627
因为在之前的课程里面

240
00:08:01,627 --> 00:08:03,947
其实给大家大量的去强调过

241
00:08:03,947 --> 00:08:05,627
并行的能力是最重要的

242
00:08:05,627 --> 00:08:07,147
而并行其实是为了解决

243
00:08:07,147 --> 00:08:08,387
带宽慢

244
00:08:08,387 --> 00:08:09,941
带宽时延长（的问题）

245
00:08:09,968 --> 00:08:11,346
到底需要多少线程

246
00:08:11,427 --> 00:08:12,947
是由计算的复杂度

247
00:08:12,947 --> 00:08:13,867
来去决定的

248
00:08:13,867 --> 00:08:15,803
对于Element-wise这个操作来说

249
00:08:15,907 --> 00:08:17,187
每增加一个线程

250
00:08:17,187 --> 00:08:18,667
这意味着需要

251
00:08:18,686 --> 00:08:22,067
对数据进行一次新的加载

252
00:08:22,260 --> 00:08:24,947
因为在GPU里面是并行的去加载

253
00:08:24,947 --> 00:08:27,347
或者并行的去提供线程的

254
00:08:27,347 --> 00:08:29,587
于是增加线程的数量

255
00:08:29,587 --> 00:08:31,747
并不会对实际的运算

256
00:08:31,747 --> 00:08:33,267
或者对实际的时延

257
00:08:33,267 --> 00:08:34,895
产生任何的影响

258
00:08:34,940 --> 00:08:37,347
就变成数据的规模在合理的范围内增大

259
00:08:37,347 --> 00:08:38,347
并不会影响

260
00:08:38,347 --> 00:08:39,987
实际算法的效率

261
00:08:40,076 --> 00:08:41,827
并不会在计算的过程当中

262
00:08:41,987 --> 00:08:43,627
觉得它会变慢了

263
00:08:43,722 --> 00:08:44,591
同样的原因

264
00:08:44,613 --> 00:08:46,041
因为线程是分级的

265
00:08:46,067 --> 00:08:48,507
在对于卷积的这一类运算的时候

266
00:08:48,667 --> 00:08:50,507
每增加一个线程

267
00:08:50,627 --> 00:08:52,147
对于数据的读取

268
00:08:52,147 --> 00:08:52,947
因为是并行的

269
00:08:52,947 --> 00:08:54,587
它的影响也不会太大

270
00:08:54,940 --> 00:08:56,651
这个时候GPU的执行效率

271
00:08:56,667 --> 00:08:58,827
跟AI的计算模式之间

272
00:08:58,827 --> 00:09:00,667
就非常的匹配了

273
00:09:00,667 --> 00:09:03,147
可以看到里面的算法的复杂度

274
00:09:03,147 --> 00:09:04,107
或者算法的规模

275
00:09:04,107 --> 00:09:05,467
或者强度的规模

276
00:09:05,467 --> 00:09:06,747
都是O(1)

277
00:09:06,747 --> 00:09:08,587
非常的和谐

278
00:09:08,741 --> 00:09:11,507
因此在GPU里面通过线程的分层分级

279
00:09:11,507 --> 00:09:12,787
能够很好的匹配到

280
00:09:12,787 --> 00:09:15,347
算法的一些计算的强度

281
00:09:16,142 --> 00:09:18,034
现在来到了第三个内容

282
00:09:18,088 --> 00:09:19,208
去详细的打开

283
00:09:19,208 --> 00:09:20,208
AI的计算模式

284
00:09:20,208 --> 00:09:22,168
跟线程之间的关系

285
00:09:22,298 --> 00:09:23,771
刚才在第一个小内容里面

286
00:09:23,812 --> 00:09:24,652
给大家汇报过

287
00:09:24,652 --> 00:09:26,292
实际上卷积的计算

288
00:09:26,292 --> 00:09:28,012
可以用矩阵乘来代替

289
00:09:28,012 --> 00:09:29,332
脱离卷积的这个概念

290
00:09:29,332 --> 00:09:30,692
真正的在kernel

291
00:09:30,692 --> 00:09:32,212
在GPU执行的阶段下

292
00:09:32,212 --> 00:09:33,812
看看矩阵乘

293
00:09:33,932 --> 00:09:35,372
跟AI计算模式

294
00:09:35,372 --> 00:09:36,412
或者矩阵乘

295
00:09:36,412 --> 00:09:37,372
跟GPU之间

296
00:09:37,372 --> 00:09:38,732
是怎么去进行更好的

297
00:09:38,732 --> 00:09:40,052
去融合和交互的

298
00:09:40,052 --> 00:09:41,972
红色矩阵的每一行

299
00:09:41,972 --> 00:09:43,909
跟蓝色矩阵的每一列

300
00:09:43,932 --> 00:09:45,612
进行一个乘加的操作之后

301
00:09:45,746 --> 00:09:47,146
就得到输出的

302
00:09:47,146 --> 00:09:49,586
橙色这块里面的一个元素

303
00:09:49,711 --> 00:09:52,172
首先要提取红色矩阵里面的

304
00:09:52,172 --> 00:09:53,652
第一行的元素

305
00:09:53,652 --> 00:09:55,092
还有蓝色矩阵

306
00:09:55,092 --> 00:09:56,612
第一列的元素

307
00:09:56,670 --> 00:10:00,012
红色和蓝色每一个元素进行相乘

308
00:10:00,012 --> 00:10:00,932
相乘完之后

309
00:10:01,132 --> 00:10:03,772
再逐个元素进行相加

310
00:10:03,910 --> 00:10:07,052
从而得到最终的橙色的值

311
00:10:07,073 --> 00:10:07,968
在GPU里面

312
00:10:07,972 --> 00:10:09,692
因为会提供一个FFM

313
00:10:09,692 --> 00:10:10,812
或者MAC的操作

314
00:10:10,812 --> 00:10:11,932
就是把乘加

315
00:10:11,932 --> 00:10:13,252
变成一个具体的指令

316
00:10:13,252 --> 00:10:15,652
所以每一次乘法和加法

317
00:10:15,652 --> 00:10:17,172
是一起去执行的

318
00:10:17,372 --> 00:10:18,612
但是在矩阵乘里面

319
00:10:18,732 --> 00:10:20,732
不仅是要算一行一列

320
00:10:20,732 --> 00:10:22,532
可能以这一行

321
00:10:22,532 --> 00:10:23,452
算第二列

322
00:10:23,452 --> 00:10:24,052
第三列

323
00:10:24,052 --> 00:10:24,732
第四列

324
00:10:24,732 --> 00:10:25,572
第五列

325
00:10:25,572 --> 00:10:27,412
然后得出第一个元素

326
00:10:27,412 --> 00:10:28,092
第二个元素

327
00:10:28,092 --> 00:10:29,132
第三个元素

328
00:10:29,132 --> 00:10:30,212
第四个元素

329
00:10:30,212 --> 00:10:32,132
第五个元素的值

330
00:10:32,324 --> 00:10:35,012
在整个矩阵乘里面，红色的这一行

331
00:10:35,012 --> 00:10:36,692
加载了一次

332
00:10:36,692 --> 00:10:39,252
但是蓝色去加载了5次

333
00:10:39,252 --> 00:10:40,852
每一次都有5个元素

334
00:10:40,852 --> 00:10:43,132
因此它执行了25次计算

335
00:10:43,509 --> 00:10:45,812
随着相乘的矩阵的大小的提高

336
00:10:45,812 --> 00:10:49,224
算力的需求就不断的提高了

337
00:10:49,378 --> 00:10:51,252
这个时候就知道矩阵的运算

338
00:10:51,372 --> 00:10:53,932
其实还是有一个算术的强度的

339
00:10:54,126 --> 00:10:55,692
需要进行的矩阵越大

340
00:10:55,692 --> 00:10:57,252
算术的强度就越大

341
00:10:57,252 --> 00:10:59,252
需要搬运数据的量就会增大

342
00:10:59,252 --> 00:11:01,612
这个时候算术的强度的比

343
00:11:01,612 --> 00:11:03,292
也会相对应的增大

344
00:11:04,532 --> 00:11:05,852
来到最后一个内容

345
00:11:05,852 --> 00:11:07,372
去看看算术的强度

346
00:11:07,372 --> 00:11:08,572
去对比一下表格

347
00:11:08,572 --> 00:11:09,652
在矩阵乘里面

348
00:11:09,652 --> 00:11:10,932
整体的算力的利用率

349
00:11:10,932 --> 00:11:12,812
或者算术的强度

350
00:11:12,991 --> 00:11:14,004
在这个图里面

351
00:11:14,031 --> 00:11:17,332
横坐标就是矩阵大小的增加

352
00:11:17,332 --> 00:11:19,612
纵坐标就是算术的强度

353
00:11:19,612 --> 00:11:21,652
或者计算的强度

354
00:11:21,732 --> 00:11:22,572
从横坐标里面

355
00:11:22,572 --> 00:11:23,332
可以看到

356
00:11:23,332 --> 00:11:24,132
矩阵的大小

357
00:11:24,132 --> 00:11:25,732
从1到64

358
00:11:25,732 --> 00:11:27,168
大小不断地增大

359
00:11:27,183 --> 00:11:29,972
计算强度成线性的增加

360
00:11:30,213 --> 00:11:33,252
现在打横画一条橙色的线

361
00:11:33,387 --> 00:11:35,147
这条线就代表GPU里面的

362
00:11:35,147 --> 00:11:35,160
浮点运算的计算的强度

363
00:11:35,160 --> 00:11:36,932
浮点运算的计算的强度

364
00:11:36,932 --> 00:11:39,132
从橙色和蓝色这条线

365
00:11:39,212 --> 00:11:39,932
可以看到

366
00:11:39,932 --> 00:11:41,892
中间有一个交叉点

367
00:11:41,892 --> 00:11:42,812
交叉点的位置

368
00:11:42,972 --> 00:11:44,972
就是GPU的浮点的运算

369
00:11:44,972 --> 00:11:48,332
跟矩阵的计算强度的一个交点

370
00:11:48,587 --> 00:11:50,492
为了满足整个GPU的计算强度

371
00:11:50,492 --> 00:11:51,892
也就是让SM

372
00:11:51,892 --> 00:11:53,572
或者让计算单元

373
00:11:53,572 --> 00:11:54,932
不断的去执行的时候

374
00:11:55,052 --> 00:11:57,132
要求整个矩阵的大小

375
00:11:57,252 --> 00:11:59,092
大概是在50左右

376
00:11:59,252 --> 00:12:00,612
这个时候就会满足

377
00:12:00,612 --> 00:12:02,999
整个计算的强度

378
00:12:02,999 --> 00:12:05,079
硬件就会忙碌起来

379
00:12:05,213 --> 00:12:06,401
在理想的情况下为了让机器

380
00:12:06,414 --> 00:12:07,467
为了让机器

381
00:12:07,492 --> 00:12:08,412
或者GPU

382
00:12:08,572 --> 00:12:10,932
在运算跟搬运数据的同时

383
00:12:11,092 --> 00:12:12,412
保持平衡

384
00:12:12,412 --> 00:12:13,652
就是百分百的速度

385
00:12:13,652 --> 00:12:14,732
去整体的运算

386
00:12:14,732 --> 00:12:16,412
这个就是所说的

387
00:12:16,412 --> 00:12:18,145
GPU它是个吞吐机的意义

388
00:12:18,653 --> 00:12:21,012
现在继续把蓝色的这条线

389
00:12:21,012 --> 00:12:22,532
不断的扩大

390
00:12:22,720 --> 00:12:25,012
刚才还是在这个点里面

391
00:12:25,012 --> 00:12:26,612
就是交集是50

392
00:12:26,612 --> 00:12:27,371
这个时候

393
00:12:27,387 --> 00:12:29,598
在矩阵大小为50的时候

394
00:12:29,612 --> 00:12:31,492
能够充分的发挥GPU

395
00:12:31,492 --> 00:12:33,812
在FP32的计算的强度

396
00:12:33,947 --> 00:12:36,652
当矩阵的大小不断的增加的时候

397
00:12:36,652 --> 00:12:37,772
GPU里面的内存

398
00:12:37,892 --> 00:12:39,132
就会空闲下来了

399
00:12:39,132 --> 00:12:40,252
所谓空闲

400
00:12:40,252 --> 00:12:43,172
不是指它的内存容量降低了

401
00:12:43,172 --> 00:12:44,932
或者它不需要那么多内存容量了

402
00:12:44,932 --> 00:12:47,332
而是指内存的搬运

403
00:12:47,332 --> 00:12:48,252
越来越慢了

404
00:12:48,252 --> 00:12:49,372
内存数据的刷新

405
00:12:49,372 --> 00:12:50,612
变得越来越慢了

406
00:12:50,693 --> 00:12:53,052
因为在GPU里面的计算单元里面

407
00:12:53,052 --> 00:12:55,572
需要花费更多的时间

408
00:12:55,612 --> 00:12:57,212
去对矩阵

409
00:12:57,212 --> 00:12:59,252
进行执行和运算

410
00:12:59,333 --> 00:12:59,988
AI的计算

411
00:12:59,999 --> 00:13:02,492
需要找到一个更好的平衡点

412
00:13:02,547 --> 00:13:04,092
去匹配更大的矩阵运算

413
00:13:04,092 --> 00:13:06,332
去找到更好的计算的强度

414
00:13:06,573 --> 00:13:09,292
新的这条线就是英伟达GPU里面

415
00:13:09,292 --> 00:13:11,092
后来提出了Tensor Core

416
00:13:11,092 --> 00:13:13,852
专门针对矩阵进行运算

417
00:13:13,852 --> 00:13:15,812
去提高计算的强度

418
00:13:15,812 --> 00:13:17,412
使得内存的搬运

419
00:13:17,412 --> 00:13:20,012
跟得上数据的运算的速度

420
00:13:20,012 --> 00:13:22,772
不管是用于AI加速的GPU

421
00:13:22,772 --> 00:13:25,212
还是用于AI加速的NPU

422
00:13:25,252 --> 00:13:26,132
TPU也好

423
00:13:26,132 --> 00:13:29,932
都在寻找中间最好的平衡点

424
00:13:30,067 --> 00:13:32,531
在满足计算强度的提高的同时

425
00:13:32,533 --> 00:13:34,292
还能够让矩阵越大越好

426
00:13:34,292 --> 00:13:35,412
因为在神经网络里面

427
00:13:35,532 --> 00:13:38,132
处理的是非常大的一个矩阵的运算

428
00:13:38,132 --> 00:13:39,932
而不是小矩阵的运算

429
00:13:40,720 --> 00:13:43,865
回到右边的表格里面

430
00:13:43,946 --> 00:13:46,692
看到现在其实多了一个功能

431
00:13:46,692 --> 00:13:48,132
或者多了一个模块

432
00:13:48,132 --> 00:13:49,372
叫做Tensor Core

433
00:13:49,372 --> 00:13:52,412
专门针对矩阵去提出的一个DSA

434
00:13:52,840 --> 00:13:54,412
如果使用L1的缓存

435
00:13:54,572 --> 00:13:56,652
整个计算的强度是32

436
00:13:56,652 --> 00:13:57,492
使用L2的缓存

437
00:13:57,612 --> 00:13:59,332
计算的强度是156

438
00:13:59,467 --> 00:14:01,612
所以需要根据多级的缓存

439
00:14:01,652 --> 00:14:03,572
缓存到底在哪里去搭配

440
00:14:03,572 --> 00:14:04,892
张量的核心

441
00:14:04,892 --> 00:14:06,092
使得整个张量

442
00:14:06,212 --> 00:14:08,052
在小矩阵或者大矩阵里面

443
00:14:08,052 --> 00:14:10,532
都能够更加高效的去执行运算

444
00:14:10,866 --> 00:14:12,252
最后看一个图

445
00:14:12,252 --> 00:14:13,372
这里面就表明

446
00:14:13,372 --> 00:14:14,692
在L1的缓存

447
00:14:14,692 --> 00:14:15,492
L2的缓存

448
00:14:15,505 --> 00:14:17,305
还有HBM的缓存里面

449
00:14:17,346 --> 00:14:19,438
针对矩阵的运算的强度

450
00:14:19,438 --> 00:14:20,838
都有一个交叉点

451
00:14:21,012 --> 00:14:22,212
这个时候就明白

452
00:14:22,252 --> 00:14:24,572
为什么说数据在哪里

453
00:14:24,612 --> 00:14:25,892
非常的重要

454
00:14:25,892 --> 00:14:27,412
比算力的强度

455
00:14:27,438 --> 00:14:29,758
或者float数更加重要

456
00:14:29,866 --> 00:14:30,866
简单的来说

457
00:14:30,892 --> 00:14:31,812
就是数据

458
00:14:31,812 --> 00:14:33,931
假设已经搬运到L1的缓存里面

459
00:14:33,932 --> 00:14:35,372
这个时候可以执行一些

460
00:14:35,372 --> 00:14:37,292
更小规模的矩阵的运算

461
00:14:37,292 --> 00:14:39,012
例如是小的卷积核

462
00:14:39,212 --> 00:14:40,572
对于NLP这种大的

463
00:14:40,572 --> 00:14:41,772
transformer的结构

464
00:14:42,052 --> 00:14:44,132
我可能会把数据搬运到L2

465
00:14:44,132 --> 00:14:46,212
然后通过L2的Cache去读取

466
00:14:46,212 --> 00:14:46,932
然后去执行

467
00:14:46,932 --> 00:14:49,092
因为数据跟读取之间

468
00:14:49,092 --> 00:14:50,972
是有一个关系比例的

469
00:14:50,972 --> 00:14:53,092
数据如果大量都在搬运

470
00:14:53,092 --> 00:14:55,532
大量都在等待计算的时候

471
00:14:55,532 --> 00:14:58,663
就会导致计算跟通讯之间不平衡

472
00:14:58,771 --> 00:15:01,932
因此找到计算的强度的点

473
00:15:01,972 --> 00:15:03,652
对于系统的优化

474
00:15:03,652 --> 00:15:05,612
变得非常重要

475
00:15:06,307 --> 00:15:06,867
好

476
00:15:06,867 --> 00:15:08,067
现在来总结一下

477
00:15:08,067 --> 00:15:10,667
为什么GPU更适用于AI计算

478
00:15:10,867 --> 00:15:12,945
真的是在乎算力吗？

479
00:15:12,959 --> 00:15:14,999
可能真的不是说非常在乎

480
00:15:14,999 --> 00:15:16,399
算力到底有多少

481
00:15:16,399 --> 00:15:17,399
而更应该关注

482
00:15:17,399 --> 00:15:19,652
内存、带宽跟时延之间

483
00:15:19,691 --> 00:15:21,879
跟算力的一个匹配度

484
00:15:22,133 --> 00:15:24,119
之所以说算力不是最重要的

485
00:15:24,119 --> 00:15:26,198
是因为有一个计算强度在

486
00:15:26,279 --> 00:15:27,759
不管算的多快

487
00:15:27,759 --> 00:15:29,479
内存来不及搬运

488
00:15:29,533 --> 00:15:32,199
增加的算力都是徒劳无功的

489
00:15:32,547 --> 00:15:36,159
另外带宽并没有时延那么重要

490
00:15:36,159 --> 00:15:39,079
因为时延会非常非常的长

491
00:15:39,079 --> 00:15:40,679
为了解决时延的问题

492
00:15:40,799 --> 00:15:42,479
GPU提供了大量的线程

493
00:15:42,505 --> 00:15:44,945
这里面线程都是超配的

494
00:15:45,200 --> 00:15:48,279
尽管GPU给提供了非常多的线程

495
00:15:48,279 --> 00:15:50,279
但这些线程其实不是

496
00:15:50,279 --> 00:15:52,719
所有都能够独立的去运作的

497
00:15:52,719 --> 00:15:54,639
它们之间需要有一个配合

498
00:15:54,800 --> 00:15:57,239
可能针对同一个数据有不同的线程

499
00:15:57,239 --> 00:15:59,159
对它进行一个读写的操作

500
00:15:59,187 --> 00:16:03,239
因此对线程进行了一个分层分级的概念

501
00:16:03,239 --> 00:16:04,239
通过分层分级

502
00:16:04,359 --> 00:16:06,799
把线程跟线程之间划分出来

503
00:16:06,799 --> 00:16:08,479
哪一些可以独立的操作

504
00:16:08,479 --> 00:16:09,839
哪一些可以独立的运行

505
00:16:09,839 --> 00:16:12,159
哪一些需要相互进行配合

506
00:16:12,360 --> 00:16:14,119
GPU增加大量的线程

507
00:16:14,173 --> 00:16:15,786
去解决时延的问题之后

508
00:16:15,879 --> 00:16:17,639
最后一点就需要去解决

509
00:16:17,639 --> 00:16:20,599
计算跟带宽之间的一个平衡点

510
00:16:21,013 --> 00:16:23,679
在一些小型或者计算密集型的任务里面

511
00:16:23,679 --> 00:16:25,879
为了获得更高的效率

512
00:16:25,879 --> 00:16:27,519
或者更高的计算的方式

513
00:16:27,519 --> 00:16:29,039
于是就对内存

514
00:16:29,067 --> 00:16:30,479
提出了新的需求

515
00:16:30,479 --> 00:16:32,999
也就是分层分级的内存

516
00:16:32,999 --> 00:16:35,039
从L1、L2到HBM

517
00:16:35,307 --> 00:16:37,919
最后一点就是提升单芯片里面的

518
00:16:37,919 --> 00:16:39,439
浮点运算的能力

519
00:16:39,439 --> 00:16:41,319
GPU针对AI计算

520
00:16:41,319 --> 00:16:43,039
就提出了Tensor Core了

521
00:16:43,107 --> 00:16:44,999
这样GPU就能够最大限度的

522
00:16:45,039 --> 00:16:47,799
去提升整体的系统线程

523
00:16:47,799 --> 00:16:48,639
内存

524
00:16:48,639 --> 00:16:50,999
还有所有组件的效率问题

525
00:16:50,999 --> 00:16:52,559
而这些所有的问题

526
00:16:52,559 --> 00:16:54,319
都取决于数据

527
00:16:54,319 --> 00:16:55,239
从哪里来

528
00:16:55,239 --> 00:16:57,719
数据到底是怎么运算的

529
00:16:58,583 --> 00:16:59,077
好了

530
00:16:59,079 --> 00:17:01,279
今天的内容就到这里为止

531
00:17:01,279 --> 00:17:02,319
今天回答了

532
00:17:02,319 --> 00:17:04,519
为什么GPU适用于AI计算

533
00:17:04,519 --> 00:17:06,119
是因为在GPU里面

534
00:17:06,279 --> 00:17:07,879
通过超配的线程

535
00:17:07,879 --> 00:17:09,479
来掩盖整体的时延

536
00:17:09,479 --> 00:17:10,799
还有多级的缓存

537
00:17:10,799 --> 00:17:12,065
多级的Cache

538
00:17:12,279 --> 00:17:14,239
去平衡计算和带宽的GAP

539
00:17:14,386 --> 00:17:16,159
最后提出了Tensor Core

540
00:17:16,159 --> 00:17:18,199
来增加峰值的算力

541
00:17:18,199 --> 00:17:19,719
通过这三个手段

542
00:17:19,719 --> 00:17:20,839
使得整个GPU

543
00:17:21,199 --> 00:17:23,519
非常适用于AI的计算

544
00:17:23,679 --> 00:17:24,479
谢谢各位

545
00:17:24,841 --> 00:17:26,479
卷的不行了

546
00:17:26,521 --> 00:17:28,174
记得一键三连加关注

547
00:17:28,321 --> 00:17:29,681
所有的内容都会开源

548
00:17:29,681 --> 00:17:31,521
在下面这条链接里面

549
00:17:31,881 --> 00:17:32,841
拜了个拜

