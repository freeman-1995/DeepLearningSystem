1
00:00:00,000 --> 00:00:05,440
字幕校对:米哈游天下第一

2
00:00:05,440 --> 00:00:07,560
哈喽大家好,我是ZOMI

3
00:00:07,560 --> 00:00:10,680
今天还是在AI芯片基础的一个内容

4
00:00:10,680 --> 00:00:14,520
不过今天迎来了一个新的简单的内容

5
00:00:14,520 --> 00:00:18,040
就是图形图像处理器GPU

6
00:00:18,040 --> 00:00:19,920
今天不会介绍太多的内容

7
00:00:19,920 --> 00:00:22,120
不过将会在下一个系列里面

8
00:00:22,120 --> 00:00:24,640
会有非常多GPU的深度的展开

9
00:00:24,640 --> 00:00:27,760
第一个会去看看整个GPU的硬件的基础

10
00:00:27,760 --> 00:00:28,840
就是它的工作原理

11
00:00:28,840 --> 00:00:31,280
还有一个编程的本质

12
00:00:31,280 --> 00:00:33,280
接着会以英伟达作为例子

13
00:00:33,280 --> 00:00:35,000
看看GPU的整体的架构

14
00:00:35,000 --> 00:00:36,440
从fermi的架构到hopper

15
00:00:36,440 --> 00:00:38,200
现在的H100

16
00:00:38,200 --> 00:00:40,400
里面可能会有一个小的插曲

17
00:00:40,400 --> 00:00:42,520
就是现在用的非常多的Tensor Core

18
00:00:42,520 --> 00:00:43,720
还有NVLink

19
00:00:43,720 --> 00:00:46,880
最后回到GPU最开始的初衷

20
00:00:46,880 --> 00:00:50,520
它用来做一些图形图像的流水线的处理

21
00:00:50,520 --> 00:00:52,640
里面就会详细的去看一看

22
00:00:52,640 --> 00:00:55,160
GPU里面的图形流水线的基础

23
00:00:55,160 --> 00:00:56,720
还有它的逻辑模块划分

24
00:00:56,720 --> 00:00:59,280
最后再到图形图像处理的一些算法

25
00:00:59,280 --> 00:01:01,720
到硬件是怎么形成

26
00:01:01,720 --> 00:01:04,400
整个英伟达整个GPU的帝国的

27
00:01:04,400 --> 00:01:08,480
现在还是回到通用图形图像处理器

28
00:01:08,480 --> 00:01:10,080
AI芯片的基础

29
00:01:10,080 --> 00:01:12,200
今天要给大家去分享的几个内容

30
00:01:12,200 --> 00:01:14,080
有四部分

31
00:01:14,080 --> 00:01:16,360
第一部分就是GPU的发展历史

32
00:01:16,360 --> 00:01:17,320
还有它的组成

33
00:01:17,320 --> 00:01:19,160
跟CPU其实是一样的

34
00:01:19,160 --> 00:01:21,280
接着简单的去看看

35
00:01:21,280 --> 00:01:24,520
GPU和CPU的一个具体的区别

36
00:01:24,560 --> 00:01:25,880
然后看看

37
00:01:25,880 --> 00:01:28,960
AI的发展为什么需要GPU

38
00:01:28,960 --> 00:01:33,135
最后看看GPU的应用场景了

39
00:01:33,135 --> 00:01:33,160
那事不宜迟

40
00:01:33,160 --> 00:01:34,175
那事不宜迟

41
00:01:34,200 --> 00:01:37,800
现在马上开始今天的主要内容

42
00:01:39,360 --> 00:01:41,920
第一个就是GPU的发展历史

43
00:01:41,920 --> 00:01:44,560
The history of the GPU

44
00:01:45,320 --> 00:01:46,920
看一下GPU发展

45
00:01:46,920 --> 00:01:48,920
其实经历了三个阶段

46
00:01:48,920 --> 00:01:52,320
第一个阶段主要是指初代的GPU

47
00:01:52,480 --> 00:01:54,120
这个时候GPU其实自身

48
00:01:54,120 --> 00:01:57,040
是不具备软件的编程能力的

49
00:01:57,040 --> 00:01:58,760
它没有软件自己的编程能力

50
00:01:58,760 --> 00:02:00,080
那时候CUDA还没有

51
00:02:00,080 --> 00:02:02,600
大家是不能够基于GPU来去做一些

52
00:02:02,600 --> 00:02:04,280
编程写自己的代码的

53
00:02:04,280 --> 00:02:06,480
它只是作为一个CPU的

54
00:02:06,480 --> 00:02:08,040
另外一个计算单元

55
00:02:08,040 --> 00:02:11,120
能够把部分的三维图形图像的功能

56
00:02:11,360 --> 00:02:13,600
从CPU里面剥离出来

57
00:02:13,600 --> 00:02:16,560
针对图形图像这些软件算法

58
00:02:16,560 --> 00:02:19,440
做一些独立的硬件的加速

59
00:02:19,640 --> 00:02:22,640
那个时候有一个最出名的一个引擎

60
00:02:22,640 --> 00:02:24,360
就是几何处理引擎

61
00:02:24,360 --> 00:02:27,120
Geometry Engine作为整体的代表的

62
00:02:27,480 --> 00:02:31,040
下面看一下GPU发展的第二个阶段

63
00:02:31,240 --> 00:02:34,840
第二阶段就是从99年到2005年

64
00:02:35,040 --> 00:02:36,800
这个时候GPU就提供了

65
00:02:36,800 --> 00:02:39,080
更多的硬件的加速能力

66
00:02:39,080 --> 00:02:41,120
还有一些有限的编程性

67
00:02:41,120 --> 00:02:44,880
那时候的编程能力还是非常有限的

68
00:02:45,240 --> 00:02:48,600
而在第二代GPU里面的CUDA

69
00:02:48,720 --> 00:02:52,560
现在熟悉的CUDA还没有出现

70
00:02:52,560 --> 00:02:53,600
那看一下

71
00:02:53,800 --> 00:02:56,440
这个时候其实有两大厂商在竞逐

72
00:02:56,440 --> 00:02:58,640
第一个就是英伟达

73
00:02:58,640 --> 00:03:00,880
第二个就是ATI

74
00:03:00,880 --> 00:03:03,360
那后来ATI被AMD收购了

75
00:03:03,360 --> 00:03:05,680
叫做A卡和N卡

76
00:03:05,680 --> 00:03:08,120
而那个时候ZOMI去买显卡

77
00:03:08,280 --> 00:03:09,760
都是非常之纠结

78
00:03:09,760 --> 00:03:11,680
到底选A卡还是N卡

79
00:03:11,680 --> 00:03:13,760
下面来看一下具体的内容

80
00:03:14,080 --> 00:03:15,040
在99年的时候

81
00:03:15,200 --> 00:03:16,640
英伟达就发布了

82
00:03:16,680 --> 00:03:19,040
GeForce 256图形图像处理器

83
00:03:19,400 --> 00:03:22,640
这个时候真正的去把一些图形图像的

84
00:03:22,640 --> 00:03:24,440
快速变换的显示的功能

85
00:03:24,440 --> 00:03:26,440
从CPU里面分离出来

86
00:03:26,440 --> 00:03:29,640
实现了第一个真正意义的GPU

87
00:03:29,920 --> 00:03:32,360
而在2000年到2005年的时候

88
00:03:32,480 --> 00:03:34,160
是属于一个GPU硬件

89
00:03:34,160 --> 00:03:36,800
硬件高速发展的阶段

90
00:03:36,800 --> 00:03:38,240
整体的运算的速率

91
00:03:38,360 --> 00:03:39,160
就是Float数

92
00:03:39,440 --> 00:03:42,120
超过了CPU非常非常的多

93
00:03:42,120 --> 00:03:43,480
而且超过的比例

94
00:03:43,480 --> 00:03:45,280
也是呈指数性的增长

95
00:03:45,320 --> 00:03:46,960
当时候的图形硬件流水线

96
00:03:47,160 --> 00:03:48,840
被定义为流处理器

97
00:03:49,160 --> 00:03:50,960
当时候的图形硬件的流水线

98
00:03:51,160 --> 00:03:52,560
被称为流处理器

99
00:03:52,560 --> 00:03:53,880
在这些流处理器里面

100
00:03:54,240 --> 00:03:56,240
就出现了顶点可编程

101
00:03:56,240 --> 00:03:57,360
3D的顶点可编程

102
00:03:57,360 --> 00:03:58,640
图像的顶点可编程

103
00:03:58,640 --> 00:04:01,880
还有像素级别的有限可编程

104
00:04:01,880 --> 00:04:02,640
但整体来说

105
00:04:02,640 --> 00:04:04,000
它的编程能力有限

106
00:04:04,000 --> 00:04:07,960
这个时间段更多的是聚焦于硬件的加速

107
00:04:08,440 --> 00:04:12,000
下面就是英伟达和ATI的两块显卡

108
00:04:12,000 --> 00:04:14,000
左边这个就GeForce 6800

109
00:04:14,000 --> 00:04:16,880
右边就是ATI的分别的显卡

110
00:04:16,880 --> 00:04:18,960
而ATI不是被AMD收购了吗

111
00:04:18,960 --> 00:04:20,480
还是保持着红色

112
00:04:20,840 --> 00:04:22,720
接着来到GPU的发展

113
00:04:22,720 --> 00:04:24,040
第3个阶段

114
00:04:24,040 --> 00:04:26,200
从2006年到现在

115
00:04:26,480 --> 00:04:27,240
基本上的目标

116
00:04:27,400 --> 00:04:30,120
就是奔着软件可编程去发展的

117
00:04:30,720 --> 00:04:32,720
刚好也是从2006年开始

118
00:04:32,880 --> 00:04:33,960
英伟达跟ATI

119
00:04:33,960 --> 00:04:34,760
就是A卡

120
00:04:34,760 --> 00:04:35,920
或者AMD的显卡

121
00:04:36,160 --> 00:04:38,440
出现了一个比较大的分水岭

122
00:04:38,800 --> 00:04:41,920
2006年英伟达就推出了一个CUDA

123
00:04:42,280 --> 00:04:44,080
CUDA

124
00:04:44,640 --> 00:04:47,440
而ATI推出了CTM

125
00:04:47,760 --> 00:04:49,640
两个都推出了编程环境

126
00:04:49,640 --> 00:04:51,000
使得GPU打破了

127
00:04:51,000 --> 00:04:53,240
只能够对图形图像

128
00:04:53,480 --> 00:04:54,840
这些硬件进行加速

129
00:04:54,840 --> 00:04:56,440
而成为了真正并行的

130
00:04:56,440 --> 00:04:57,840
数据处理的加速器

131
00:04:57,840 --> 00:05:00,160
也要成为GPU

132
00:05:00,600 --> 00:05:01,640
在2008年的时候

133
00:05:01,760 --> 00:05:03,200
Apple苹果就推出了一个

134
00:05:03,200 --> 00:05:05,080
通用的并行计算平台

135
00:05:05,080 --> 00:05:06,600
OpenCL

136
00:05:06,600 --> 00:05:09,960
就是苹果推出的一个协议和编程语言

137
00:05:10,400 --> 00:05:11,120
跟CUDA不一样

138
00:05:11,240 --> 00:05:12,840
CUDA只能支持英伟达

139
00:05:12,840 --> 00:05:14,080
现在遇到很多客户

140
00:05:14,200 --> 00:05:15,080
希望NPU

141
00:05:15,240 --> 00:05:17,360
支持像CUDA这种编程方式

142
00:05:17,360 --> 00:05:18,520
但其实是不太可能

143
00:05:18,520 --> 00:05:19,720
或者不太现实的

144
00:05:19,720 --> 00:05:22,120
因为CUDA它只针对英伟达有效

145
00:05:22,120 --> 00:05:22,840
像OpenCL

146
00:05:23,000 --> 00:05:25,840
它这种就是跟具体的设备没有关系

147
00:05:25,840 --> 00:05:27,680
而且也是随着2008年

148
00:05:27,960 --> 00:05:29,800
苹果iPhone4的出现

149
00:05:30,280 --> 00:05:31,480
整个OpenCL

150
00:05:31,680 --> 00:05:33,360
已经成为移动端GPU的

151
00:05:33,360 --> 00:05:35,720
一个编程的业界标准

152
00:05:35,720 --> 00:05:38,160
这也是跟生态相关的

153
00:05:39,080 --> 00:05:41,520
简单的打开CUDA来去看看

154
00:05:41,520 --> 00:05:43,160
CUDA就是这一层

155
00:05:43,360 --> 00:05:45,920
基于英伟达的硬件之上的

156
00:05:45,920 --> 00:05:47,400
CUDA它有CUDA toolkit

157
00:05:47,760 --> 00:05:48,920
还有CUDA driver

158
00:05:49,120 --> 00:05:51,960
所以CUDA不仅仅是一个硬件的驱动

159
00:05:51,960 --> 00:05:55,600
它还是一个编程开发的软件工具站

160
00:05:55,600 --> 00:05:57,360
CUDA内部有自己的编译器

161
00:05:57,520 --> 00:05:59,640
有自己的debuggers profilers的一些工具

162
00:05:59,640 --> 00:06:01,520
当然还有一些C++的

163
00:06:01,720 --> 00:06:04,840
API的接口网上提供给去编程的

164
00:06:04,840 --> 00:06:07,280
这网上就是CUDA之上

165
00:06:07,280 --> 00:06:08,840
建立的一些库啦

166
00:06:08,840 --> 00:06:10,040
现在深度学习

167
00:06:10,040 --> 00:06:11,720
经常用到的各种加速库

168
00:06:11,720 --> 00:06:13,920
就是基于CUDA之上构建的

169
00:06:16,520 --> 00:06:17,600
了解完历史之后

170
00:06:17,720 --> 00:06:21,480
看看GPU跟CPU之间的一个差异

171
00:06:37,320 --> 00:06:38,040
1

172
00:06:41,280 --> 00:06:42,200
让我加速一下

173
00:06:49,560 --> 00:06:50,560
女士们 先生们

174
00:06:51,840 --> 00:06:53,080
Leonardo

175
00:06:55,080 --> 00:06:56,280
2.0

176
00:06:57,400 --> 00:07:00,040
当按下这个按钮

177
00:07:00,040 --> 00:07:02,800
2100毫升的空气

178
00:07:02,960 --> 00:07:04,640
通过这些排泄物

179
00:07:04,640 --> 00:07:05,600
通过这些排泄物

180
00:07:05,600 --> 00:07:08,120
通过这些排泄物

181
00:07:08,120 --> 00:07:09,520
通过这些排泄物

182
00:07:09,520 --> 00:07:11,560
底部是一块细胞

183
00:07:11,560 --> 00:07:14,400
这些细胞会飞到7公里的空间

184
00:07:14,400 --> 00:07:15,800
在80毫秒内

185
00:07:15,800 --> 00:07:16,920
达到目标

186
00:07:16,920 --> 00:07:17,920
希望

187
00:07:19,120 --> 00:07:19,920
当它们都完成了

188
00:07:19,920 --> 00:07:21,200
它就会在蒙娜丽莎上画

189
00:07:21,640 --> 00:07:24,760
GPU画画示范

190
00:07:25,480 --> 00:07:26,920
10

191
00:07:27,640 --> 00:07:28,520
9

192
00:07:29,000 --> 00:07:29,800
8

193
00:07:30,200 --> 00:07:31,040
7

194
00:07:31,400 --> 00:07:32,160
6

195
00:07:32,560 --> 00:07:33,280
5

196
00:07:33,680 --> 00:07:34,440
4

197
00:07:34,800 --> 00:07:35,560
3

198
00:07:35,960 --> 00:07:36,680
2

199
00:07:36,880 --> 00:07:37,680
1

200
00:07:44,520 --> 00:07:45,920
女士们 先生们

201
00:07:46,480 --> 00:07:48,960
科学课程现在结束了

202
00:07:50,160 --> 00:07:50,960
谢谢

203
00:07:56,200 --> 00:07:57,440
通过刚才那个视频

204
00:07:57,640 --> 00:07:59,760
看到了CPU跟GPU之间的

205
00:07:59,760 --> 00:08:02,200
一个具体的表现形式

206
00:08:02,200 --> 00:08:05,000
现在来看看图形计算单元了

207
00:08:05,000 --> 00:08:06,280
所谓的图形计算单元

208
00:08:06,520 --> 00:08:07,440
GPU的一开始

209
00:08:07,600 --> 00:08:08,640
其实它没有想过

210
00:08:08,640 --> 00:08:11,000
自己能够成为一个并行的处理器

211
00:08:11,000 --> 00:08:12,600
它主要是接替CPU

212
00:08:12,600 --> 00:08:14,960
对图形进行渲染

213
00:08:14,960 --> 00:08:16,120
因为图形

214
00:08:16,320 --> 00:08:18,520
非常多相同的元素

215
00:08:18,520 --> 00:08:20,680
非常多相同的像素点

216
00:08:20,680 --> 00:08:22,000
要对每个像素点

217
00:08:22,120 --> 00:08:23,680
都执行相同的运算

218
00:08:23,680 --> 00:08:26,520
于是就衍生了GPU

219
00:08:26,520 --> 00:08:29,200
去分担CPU的工作

220
00:08:30,760 --> 00:08:32,520
实际上左边这个

221
00:08:32,520 --> 00:08:35,160
就是之前多次给大家演示到的

222
00:08:35,160 --> 00:08:36,240
CPU的架构图

223
00:08:36,240 --> 00:08:37,040
右边的这个

224
00:08:37,040 --> 00:08:38,960
就是GPU的架构图

225
00:08:38,960 --> 00:08:40,240
从整体的架构图

226
00:08:40,240 --> 00:08:41,040
可以看到了

227
00:08:41,040 --> 00:08:43,400
GPU大部分都有绿色的这些ALU

228
00:08:43,400 --> 00:08:44,880
就是计算单元

229
00:08:44,880 --> 00:08:45,760
去组成的

230
00:08:45,760 --> 00:08:47,320
只有少量的控制单元

231
00:08:47,320 --> 00:08:49,800
和内存的存储单元

232
00:08:49,800 --> 00:08:51,640
反观左边的CPU

233
00:08:51,840 --> 00:08:54,440
不仅仅被cache占用了大量的空间

234
00:08:54,440 --> 00:08:57,360
而且有非常之复杂的控制逻辑电路

235
00:08:57,360 --> 00:08:58,920
还有其他额外的电路

236
00:08:58,960 --> 00:09:02,400
相比起来计算能力的提升

237
00:09:02,640 --> 00:09:05,600
只是CPU里面的很小的一部分

238
00:09:07,480 --> 00:09:10,280
继续打开GPU的整体的架构

239
00:09:10,280 --> 00:09:11,840
来看看GPU有什么不一样

240
00:09:12,360 --> 00:09:14,280
首先第一个就是GPU的cache

241
00:09:14,440 --> 00:09:16,160
是非常非常的少的

242
00:09:16,280 --> 00:09:19,080
第二个它有很少的控制单元

243
00:09:19,440 --> 00:09:22,280
不擅长处理if-else-while-for

244
00:09:22,320 --> 00:09:24,520
这种控制流语句

245
00:09:24,720 --> 00:09:27,760
第三个就是有非常多的

246
00:09:27,880 --> 00:09:33,440
ALU就是绿色的计算单元

247
00:09:33,440 --> 00:09:35,360
ALU已经成矩阵的排列

248
00:09:35,360 --> 00:09:37,120
非常非常的多

249
00:09:37,120 --> 00:09:38,520
而这里面很有意思的

250
00:09:38,520 --> 00:09:40,920
就是cache非常的少

251
00:09:40,920 --> 00:09:42,680
DRAM并不大

252
00:09:42,680 --> 00:09:45,760
但是里面却有非常多的计算单元

253
00:09:45,760 --> 00:09:48,080
这意味着有非常之长的Latency

254
00:09:48,080 --> 00:09:50,560
但是会有一个很high的throughput

255
00:09:51,000 --> 00:09:52,320
翻译过来就是

256
00:09:52,320 --> 00:09:54,560
时延可能会稍微相对长一点

257
00:09:54,600 --> 00:09:58,160
但是吞吐率却是非常非常的大

258
00:09:58,520 --> 00:10:01,320
第4个点就有非常多的线程

259
00:10:01,320 --> 00:10:03,600
每个线程去控制一个ALU

260
00:10:03,600 --> 00:10:05,040
或者控制多个ALU

261
00:10:05,040 --> 00:10:06,840
通过大量的线程的并发

262
00:10:06,840 --> 00:10:08,920
可以处理并行的内容

263
00:10:10,400 --> 00:10:13,760
刚才提到GPU的cache是非常少的

264
00:10:13,920 --> 00:10:15,680
CPU里面的cache很多

265
00:10:15,680 --> 00:10:18,640
是因为它能做很多大量的缓存

266
00:10:18,640 --> 00:10:19,760
缓存起来了

267
00:10:19,760 --> 00:10:21,800
提供后面的数据进行处理

268
00:10:21,920 --> 00:10:24,880
但是GPU的cache它的缓存

269
00:10:24,880 --> 00:10:27,520
并不是保存之后供后续进行访问的

270
00:10:27,520 --> 00:10:30,480
而是为线程提供服务的

271
00:10:30,600 --> 00:10:32,920
之前其实简单的提到过

272
00:10:33,120 --> 00:10:35,360
GPU它是一个SIMT的架构

273
00:10:35,520 --> 00:10:37,280
T就是线程

274
00:10:37,280 --> 00:10:39,080
很多时候大量的线程

275
00:10:39,080 --> 00:10:41,200
需要访问同一段数据

276
00:10:41,200 --> 00:10:43,440
也就意味着可能一段数据

277
00:10:43,600 --> 00:10:45,680
要执行非常多的计算

278
00:10:45,840 --> 00:10:47,360
这个时候缓存

279
00:10:47,360 --> 00:10:50,000
就会合并很多这种线程的访问

280
00:10:50,000 --> 00:10:52,400
然后再去通过多级流水

281
00:10:52,560 --> 00:10:54,160
去访问DRAM

282
00:10:54,160 --> 00:10:56,120
就下面的内存的单元

283
00:10:56,120 --> 00:10:57,520
访问外面的内存

284
00:10:57,520 --> 00:10:59,560
或者大家叫做显存也可以

285
00:10:59,560 --> 00:11:00,520
获取数据之后

286
00:11:00,640 --> 00:11:04,320
cache会统一分发到对应的线程上面

287
00:11:04,320 --> 00:11:07,600
所以它的作用不是为了保存

288
00:11:07,600 --> 00:11:09,080
后面要访问的数据

289
00:11:09,080 --> 00:11:13,440
而是方便线程进行合并读写

290
00:11:13,760 --> 00:11:16,560
既然了解完GPU的几个重要的特性

291
00:11:16,560 --> 00:11:19,880
现在来看看GPU适合处理哪些应用程序

292
00:11:19,960 --> 00:11:22,840
第一种就是密集型的计算程序

293
00:11:22,840 --> 00:11:24,880
程序上面有大量的计算

294
00:11:24,880 --> 00:11:26,040
大量相同的计算

295
00:11:26,040 --> 00:11:29,320
于是这个时候就需要GPU进行处理

296
00:11:29,320 --> 00:11:32,040
第二种就是易于并行的程序

297
00:11:32,040 --> 00:11:33,960
可能有非常多的计算

298
00:11:33,960 --> 00:11:34,920
非常多的数据

299
00:11:34,920 --> 00:11:36,880
都执行相同的计算

300
00:11:36,880 --> 00:11:39,680
于是就可以使用现在的GPU进行处理了

301
00:11:39,680 --> 00:11:42,280
当然早期的GPU是不具备这个能力的

302
00:11:43,040 --> 00:11:44,560
最后简单的看看

303
00:11:44,560 --> 00:11:47,280
现在AI整个规模化产业化发展

304
00:11:47,400 --> 00:11:49,600
其实是急需GPU的

305
00:11:49,720 --> 00:11:51,840
可以看到AI遇到GPU

306
00:11:51,840 --> 00:11:53,920
主要有比较大的一个时间节点

307
00:11:53,920 --> 00:11:55,600
就是在2012年

308
00:11:55,600 --> 00:11:58,120
Alex跟Hiton设计了整个AlexNet

309
00:11:58,120 --> 00:12:00,640
那AlexNet在ImageNet这个数据集里面

310
00:12:00,640 --> 00:12:04,040
获得非常之夸张的一个冠军的水平

311
00:12:04,040 --> 00:12:06,680
其他的算法确实已经没有太多的优势了

312
00:12:06,680 --> 00:12:10,280
而AlexNet主要是用GPU进行加速训练两周

313
00:12:10,280 --> 00:12:12,400
效果已经非常非常的好了

314
00:12:12,400 --> 00:12:16,520
另外一个就是谷歌使用1000台CPU服务器

315
00:12:16,520 --> 00:12:18,960
去训练谷歌的YouTube里面的视频

316
00:12:18,960 --> 00:12:21,880
做一个猫和狗的分别

317
00:12:21,880 --> 00:12:25,280
而同期吴恩达使用了三台GTX680 GPU

318
00:12:25,280 --> 00:12:27,160
就完成了相同的任务

319
00:12:27,160 --> 00:12:29,080
而这个时候大家开始意识到

320
00:12:29,080 --> 00:12:30,840
通过GPU加深度学习

321
00:12:30,840 --> 00:12:34,120
可以取代很多CPU复杂的操作

322
00:12:34,120 --> 00:12:35,840
节省很多的资源

323
00:12:35,840 --> 00:12:37,920
而整个AI的爆发期

324
00:12:37,920 --> 00:12:39,800
是随着产业的发展的

325
00:12:39,800 --> 00:12:42,080
那产业的发展有两大方面

326
00:12:42,080 --> 00:12:44,360
第一大方面就是整个AI的分网

327
00:12:44,360 --> 00:12:46,640
AI框架越来越多

328
00:12:46,640 --> 00:12:48,600
大部分的AI框架慢慢的开始兼容

329
00:12:48,600 --> 00:12:49,440
CPU

330
00:12:49,440 --> 00:12:51,200
可能部分的AI框架像theano

331
00:12:51,200 --> 00:12:52,560
还有一开始的Torch

332
00:12:52,560 --> 00:12:53,440
而不是PyTorch

333
00:12:53,440 --> 00:12:55,400
一开始都是只支持CPU的

334
00:12:55,400 --> 00:12:57,480
后来就越来越多的支持GPU了

335
00:12:57,480 --> 00:13:00,440
那第二种就是英伟达的AI Enterprise

336
00:13:00,440 --> 00:13:03,360
推出了一个系列的解决方案

337
00:13:03,360 --> 00:13:06,840
通过GPU更好的去支持整个AI的发展

338
00:13:06,840 --> 00:13:10,920
所以说AI的爆发跟GPU的爆发是相关联的

339
00:13:10,920 --> 00:13:13,520
跟英伟达的努力也是相关联的

340
00:13:14,840 --> 00:13:16,600
今天的内容就到这里为止

341
00:13:16,600 --> 00:13:20,240
了解了通用图形处理器GPU的整体的发展

342
00:13:20,240 --> 00:13:22,200
然后看了一下GPU的架构

343
00:13:22,200 --> 00:13:24,440
接下来会从硬件的基础

344
00:13:24,440 --> 00:13:25,520
英伟达的CPU架构

345
00:13:25,520 --> 00:13:28,240
还有GPU的图形处理流水线

346
00:13:28,240 --> 00:13:30,240
真正意义上的流处理器

347
00:13:30,240 --> 00:13:32,920
去深入的看看GPU

348
00:13:32,920 --> 00:13:33,640
谢谢各位

349
00:13:33,640 --> 00:13:34,560
拜了个拜

350
00:13:35,275 --> 00:13:37,351
卷的不行了卷的不行了

351
00:13:37,351 --> 00:13:38,567
记得一键三连加关注哦

352
00:13:38,567 --> 00:13:42,150
所有的内容都会开源在下面这条链接里面

353
00:13:42,150 --> 00:13:43,933
拜了个拜

