1
00:00:00,000 --> 00:00:04,000
Subtitle：PlusV98

2
00:00:05,075 --> 00:00:11,100
哈喽大家好 我是在大大的公司里面挖呀挖呀挖的ZOMI

3
00:00:20,000 --> 00:00:26,000
今天来到了AI芯片GPU详解里面的Tensor Core第三个内容深度剖析

4
00:00:26,000 --> 00:00:31,188
那这个所谓的深度剖析呢更多ZOMI个人的理解

5
00:00:31,188 --> 00:00:34,188
今天呢主要给大家去分享三个内容

6
00:00:34,188 --> 00:00:39,188
第一个呢就是Tensor Core回顾一下整个Tensor Core的具体执行的流程

7
00:00:39,188 --> 00:00:43,188
第二个呢就去看一下指令的流水

8
00:00:43,188 --> 00:00:46,188
特别是指Tensor Core的Instruction Pipeline

9
00:00:46,188 --> 00:00:50,481
第三个呢来一起去看看CUDA Thread

10
00:00:50,481 --> 00:00:55,289
就是CUDA的线程执行跟硬件具体是怎么去结合的

11
00:00:55,289 --> 00:01:00,025
这里面就会给大家带来一些非常之个人主观的技术意见

12
00:01:00,173 --> 00:01:05,173
如果大家觉得不对的也可以去吐槽和指正和指点

13
00:01:06,000 --> 00:01:08,000
现在呢来到了第一个内容

14
00:01:08,000 --> 00:01:10,000
去看看Tensor Core的执行

15
00:01:10,000 --> 00:01:12,525
一个4x4的矩阵A

16
00:01:12,525 --> 00:01:14,125
4x4的矩阵B

17
00:01:14,125 --> 00:01:17,125
再加上一个4x4的矩阵C

18
00:01:17,125 --> 00:01:22,125
那所谓混合精度呢就是在计算的过程当中呢使用FP16去计算

19
00:01:22,125 --> 00:01:27,125
但是存储的时候呢使用FP32或者FP16进行存储

20
00:01:27,125 --> 00:01:30,573
但是在数学实际计算的时候呢

21
00:01:30,573 --> 00:01:34,573
是把矩一行乘以矩阵的一列

22
00:01:34,573 --> 00:01:39,573
然后再加上单独一个元素得到D矩阵的第一个元素

23
00:01:39,573 --> 00:01:42,715
把第一行跟第一列进行相乘

24
00:01:42,715 --> 00:01:47,579
实际上在整个计算的时候呢会把第二行跟第二列进行相乘

25
00:01:47,579 --> 00:01:49,904
再加上对角线的C11

26
00:01:49,904 --> 00:01:52,656
再接下来还有更多的操作

27
00:01:52,656 --> 00:01:52,904
就是每一行跟每一列都需要相乘

28
00:01:52,904 --> 00:01:55,728
就是每一行跟每一列都需要相乘

29
00:01:55,728 --> 00:01:58,096
才能够得到所有的元素

30
00:01:58,096 --> 00:02:01,000
也就是第一行跟第一列相乘

31
00:02:01,000 --> 00:02:03,000
第一行跟第二列相乘

32
00:02:03,000 --> 00:02:04,000
第三列第四列

33
00:02:04,000 --> 00:02:07,871
英伟达的V100其实并不是一行的去计算

34
00:02:07,871 --> 00:02:10,871
而是整一个矩阵整一个矩阵的去计算的

35
00:02:10,871 --> 00:02:13,871
来看看下面官方给出来的模拟图

36
00:02:13,871 --> 00:02:15,604
PASCA呢就是上一代的架构

37
00:02:15,604 --> 00:02:16,604
没有Tensor Core之前的

38
00:02:16,604 --> 00:02:18,604
是一个元素跟一行进行相乘

39
00:02:18,604 --> 00:02:21,356
每个时钟周期呢执行四次相乘

40
00:02:21,356 --> 00:02:23,120
得到一列数据

41
00:02:23,120 --> 00:02:25,120
而在V100里面呢

42
00:02:25,120 --> 00:02:29,120
把整个矩阵A跟整个矩阵B进行相乘

43
00:02:29,120 --> 00:02:31,806
然后呢得到整一个矩阵的输出

44
00:02:31,806 --> 00:02:33,256
整体来说

45
00:02:33,256 --> 00:02:34,057
右边的这个Tensor Core呢

46
00:02:34,057 --> 00:02:35,529
在单个时钟周期内呢

47
00:02:35,529 --> 00:02:37,529
就能够执行四乘四乘四

48
00:02:37,529 --> 00:02:39,904
等于64次的FMA

49
00:02:39,904 --> 00:02:41,600
也就是乘加的计算操作

50
00:02:41,600 --> 00:02:43,509
它的吞吐呢

51
00:02:43,509 --> 00:02:46,261
会比左边的PASCA架构呢快了12倍

52
00:02:46,261 --> 00:02:48,261
那PASCA架构这种计算呢

53
00:02:48,261 --> 00:02:50,261
使用的是CUDA Core

54
00:02:50,261 --> 00:02:51,765
而V100呢就出现了Tensor Core

55
00:02:51,765 --> 00:02:54,710
专门去对矩阵进行加速

56
00:02:54,710 --> 00:02:58,000
现在呢来看一下V100的一个V架构

57
00:02:58,000 --> 00:03:00,214
刚才看到 一个时钟周期呢

58
00:03:00,214 --> 00:03:02,145
其实只能执行16个FMA

59
00:03:02,145 --> 00:03:05,000
但是呢v100的Tensor Core里面呢

60
00:03:05,000 --> 00:03:06,817
一个时钟周期内呢

61
00:03:06,817 --> 00:03:09,817
可以执行两个4x4x4的FMA的操作

62
00:03:09,817 --> 00:03:10,817
整体来说

63
00:03:10,817 --> 00:03:12,183
Tensor Core的计算吞吐呢

64
00:03:12,183 --> 00:03:15,183
比右边的CUDA Core要高12倍

65
00:03:15,183 --> 00:03:17,183
那下面呢再看一下

66
00:03:17,183 --> 00:03:19,843
在一个SM里面有四个Sub Core

67
00:03:19,843 --> 00:03:22,019
每个Sub Core里面呢有两个Tensor Core

68
00:03:22,019 --> 00:03:24,387
单个Tensor Core一个时钟周期内呢

69
00:03:24,387 --> 00:03:26,387
能执行64个FMA

70
00:03:26,387 --> 00:03:28,893
里面的一个SM呢 单个时钟周期

71
00:03:28,893 --> 00:03:29,000
就可以执行1024次FFMA了

72
00:03:29,000 --> 00:03:31,743
就可以执行1024次FFMA了

73
00:03:33,000 --> 00:03:35,000
现在来到了第二个内容

74
00:03:35,000 --> 00:03:37,569
Tensor Core的指令流水

75
00:03:37,569 --> 00:03:40,421
有两个不同的符号

76
00:03:40,421 --> 00:03:41,421
一个是加号

77
00:03:41,421 --> 00:03:42,421
一个是乘号

78
00:03:42,421 --> 00:03:44,133
要在Tensor Core里面呢

79
00:03:44,133 --> 00:03:46,885
去实现刚才的一条简单的矩阵相乘

80
00:03:46,885 --> 00:03:50,149
把A的一行呢乘以B矩阵的一列

81
00:03:50,149 --> 00:03:52,453
也就是下面对应的这条公式

82
00:03:52,453 --> 00:03:54,600
于是呢电路呢

83
00:03:54,600 --> 00:03:56,275
假设这里面只是我的臆想

84
00:03:56,275 --> 00:03:58,656
不一定Tensor Core里面就是这么去实现的

85
00:03:58,656 --> 00:04:03,072
可能会把A跟B进行相乘再相加

86
00:04:03,072 --> 00:04:06,000
通过这种虚拟硬件电路的方式呢

87
00:04:06,000 --> 00:04:06,072
去实现整个相关的硬件电路

88
00:04:06,072 --> 00:04:08,768
去实现整个相关的硬件电路

89
00:04:08,768 --> 00:04:10,768
下面呢再看一下

90
00:04:10,768 --> 00:04:12,768
实际上呢在中间的过程当中

91
00:04:12,768 --> 00:04:14,768
或者计算的过程当中呢

92
00:04:14,768 --> 00:04:16,128
绿色的是寄存器

93
00:04:16,128 --> 00:04:18,688
离不开相关的寄存器

94
00:04:18,688 --> 00:04:20,688
竖着呢是一个32位的

95
00:04:20,688 --> 00:04:22,688
输出呢也是32位的

96
00:04:22,688 --> 00:04:25,688
但是呢我中间去计算A和B矩阵呢

97
00:04:25,688 --> 00:04:27,688
它是可以是16位的

98
00:04:27,688 --> 00:04:29,688
而在乘加完之后呢

99
00:04:29,688 --> 00:04:32,286
它确实中间需要有一个简单的寄存器

100
00:04:32,286 --> 00:04:34,286
去存储中间的数据

101
00:04:34,286 --> 00:04:36,254
可以看到寄存器

102
00:04:36,254 --> 00:04:39,254
离实际的计算单元呢

103
00:04:39,254 --> 00:04:40,822
是非常的接近

104
00:04:40,822 --> 00:04:41,572
通过这种方式呢

105
00:04:41,572 --> 00:04:42,806
可以简单的实现

106
00:04:42,806 --> 00:04:46,006
A矩阵的一行乘以B矩阵的一列

107
00:04:46,006 --> 00:04:48,677
刚才提到的只是一行跟一列

108
00:04:48,677 --> 00:04:50,469
那原来在CUDA Core里面呢

109
00:04:50,469 --> 00:04:52,645
是做一个点跟一行进行相乘

110
00:04:52,645 --> 00:04:54,373
现在在V100里面呢

111
00:04:54,373 --> 00:04:57,061
它是一个矩阵跟另外一个矩阵

112
00:04:57,061 --> 00:05:00,061
直接相乘得到一个新的矩阵

113
00:05:00,061 --> 00:05:01,789
而刚才提到的

114
00:05:01,789 --> 00:05:05,789
只是其中一行跟其中一列进行相乘

115
00:05:05,789 --> 00:05:07,789
得到中间一个元素

116
00:05:07,789 --> 00:05:09,119
那更多的怎么办呢

117
00:05:09,119 --> 00:05:12,564
假设刚才展示的A的一行跟B的一列相乘呢

118
00:05:12,564 --> 00:05:14,062
得到一个元素

119
00:05:14,062 --> 00:05:15,337
那下面呢看一下

120
00:05:15,337 --> 00:05:16,662
把这么一个简单的元素呢

121
00:05:16,662 --> 00:05:17,662
进行一个组成

122
00:05:17,662 --> 00:05:21,662
我把A0I A1I A2I A3I

123
00:05:21,662 --> 00:05:26,662
A的每一行跟B的每一列进行相乘

124
00:05:26,662 --> 00:05:27,662
这个时候呢

125
00:05:27,662 --> 00:05:29,662
就可以得到整个矩阵的

126
00:05:29,662 --> 00:05:31,662
每一个元素

127
00:05:31,662 --> 00:05:32,662
那这个时候

128
00:05:32,662 --> 00:05:34,662
对应的A矩阵的寄存器呢

129
00:05:34,662 --> 00:05:35,662
就应该是一堆

130
00:05:35,662 --> 00:05:37,662
对应B矩阵的寄存器呢

131
00:05:37,662 --> 00:05:38,662
也应该是一堆

132
00:05:38,662 --> 00:05:41,086
而不像刚才的只有单一个了

133
00:05:42,000 --> 00:05:44,408
ZOMI对硬件呢不是非常了解

134
00:05:44,408 --> 00:05:46,408
我尝试从我个人理解的角度

135
00:05:46,408 --> 00:05:48,408
去给大家做一个简单的分享

136
00:05:48,408 --> 00:05:49,398
如果大家觉得有不对呢

137
00:05:49,398 --> 00:05:51,000
随时欢迎大家随时指正

138
00:05:51,500 --> 00:05:55,287
当一个元素 也就是Scalar的乘加操作的指令呢

139
00:05:55,287 --> 00:05:56,380
就像下面所示

140
00:05:56,380 --> 00:05:57,212
但实际上呢

141
00:05:57,212 --> 00:05:58,488
Tensor Core里面的Mod呢

142
00:05:58,488 --> 00:06:00,488
只有Fp16存储或者加的时候呢

143
00:06:00,488 --> 00:06:02,008
是用到Fp32的

144
00:06:02,008 --> 00:06:04,303
于是呢把刚才的一个Mod呢

145
00:06:04,303 --> 00:06:05,303
把它节省掉

146
00:06:05,303 --> 00:06:07,303
现在呢实现两个元素相乘呢

147
00:06:07,303 --> 00:06:09,573
就要把两条流水并行起来

148
00:06:09,573 --> 00:06:10,917
这个就指令的流水

149
00:06:10,917 --> 00:06:13,477
现在实现用A的一行乘以B的一列

150
00:06:13,477 --> 00:06:16,477
于是呢就有四条Pipeline的流水

151
00:06:16,477 --> 00:06:19,477
现在只是实现简单计算一个元素

152
00:06:19,477 --> 00:06:21,477
就需要四条流水

153
00:06:21,477 --> 00:06:23,127
通过上面的绿色指令流水

154
00:06:23,127 --> 00:06:24,663
计算出了D00

155
00:06:24,663 --> 00:06:25,879
通过黄色的流水呢

156
00:06:25,879 --> 00:06:27,223
计算出了D01

157
00:06:27,223 --> 00:06:28,503
接下来想要把

158
00:06:28,503 --> 00:06:30,231
所有的元素计算出来了

159
00:06:30,231 --> 00:06:32,791
就有大量的指令的流水去拼接

160
00:06:32,791 --> 00:06:34,791
现在去把四条流水拼接起来

161
00:06:34,791 --> 00:06:37,143
就简单的实现了一个矩阵的

162
00:06:37,143 --> 00:06:39,191
D01到D03的一个结果

163
00:06:39,191 --> 00:06:43,000
那现在其实还要把所有的都拼起来

164
00:06:43,000 --> 00:06:44,631
那整个指令的流水呢

165
00:06:44,631 --> 00:06:46,935
在一个屏幕呢已经放不下了

166
00:06:46,935 --> 00:06:48,560
看一下这里面的颜色呢

167
00:06:48,560 --> 00:06:50,330
其实在某一个时间段

168
00:06:50,330 --> 00:06:53,330
对数据的读写是有规律的

169
00:06:53,330 --> 00:06:56,330
Mod呢就是需要对数据读取出来

170
00:06:56,330 --> 00:06:57,330
读取出来之后呢

171
00:06:57,330 --> 00:06:59,330
下面这个算完Round呢

172
00:06:59,330 --> 00:07:00,762
就是数据的写入

173
00:07:00,762 --> 00:07:02,762
所以在某一个时刻呢

174
00:07:02,762 --> 00:07:05,237
对整个流水是有四个数据

175
00:07:05,237 --> 00:07:07,237
从计存器里面读到计算单元

176
00:07:07,237 --> 00:07:08,237
然后有一个数据呢

177
00:07:08,237 --> 00:07:10,237
存到计存器里面

178
00:07:10,237 --> 00:07:12,582
通过大量的指令流水呢

179
00:07:12,582 --> 00:07:15,132
实现了整个Tensor Core的计算

180
00:07:16,000 --> 00:07:18,790
这次呢ZOMI的语速呢放得非常的慢

181
00:07:18,790 --> 00:07:20,262
然后也没有那么多废话了

182
00:07:20,262 --> 00:07:22,262
现在呢来看一下第三个内容

183
00:07:22,262 --> 00:07:24,262
Tensor Core的线程的执行

184
00:07:24,262 --> 00:07:26,726
在整体CUDA的软件设计方面呢

185
00:07:26,726 --> 00:07:29,076
其实是希望能够去匹配

186
00:07:29,076 --> 00:07:31,076
英伟达计算和存储分层的整个结构

187
00:07:31,076 --> 00:07:34,076
那整体呢英伟达对于Tensor Core的定义呢

188
00:07:34,076 --> 00:07:36,281
其实主要是通过CUDA来提供一个

189
00:07:36,281 --> 00:07:37,369
范型的编程

190
00:07:37,369 --> 00:07:38,591
那这个所谓的范型编程

191
00:07:38,591 --> 00:07:40,393
可以抛开一边先不谈

192
00:07:40,393 --> 00:07:41,545
他们的一个术语呢

193
00:07:41,545 --> 00:07:43,545
是General Programming

194
00:07:43,545 --> 00:07:44,695
后面的我的例子呢

195
00:07:44,695 --> 00:07:46,409
会有一个简单的

196
00:07:46,409 --> 00:07:47,817
A乘以B等于C

197
00:07:47,817 --> 00:07:48,817
作为Demo

198
00:07:48,817 --> 00:07:49,817
就没有了刚才

199
00:07:49,817 --> 00:07:51,817
D等于A乘以B加C

200
00:07:51,817 --> 00:07:52,817
没有了这个概念

201
00:07:52,817 --> 00:07:54,817
就简单的一个矩阵层

202
00:07:55,000 --> 00:07:59,000
矩阵A呢跟矩阵B相乘

203
00:07:59,000 --> 00:08:01,000
得到矩阵C

204
00:08:01,000 --> 00:08:01,600
但实际上呢

205
00:08:01,600 --> 00:08:02,600
不可能把这么大的一个矩阵

206
00:08:02,600 --> 00:08:04,600
载到具体的Tensor Core里面

207
00:08:04,600 --> 00:08:05,600
因为Tensor Core

208
00:08:05,600 --> 00:08:07,508
只能从容纳四乘四的

209
00:08:07,508 --> 00:08:07,600
一个简单的计算

210
00:08:07,600 --> 00:08:08,508
一个简单的计算

211
00:08:08,508 --> 00:08:09,508
那这个时候呢

212
00:08:09,508 --> 00:08:11,650
会对矩阵进行切片

213
00:08:11,650 --> 00:08:12,650
放到Thread Board

214
00:08:12,650 --> 00:08:13,650
就是线程块里面

215
00:08:13,650 --> 00:08:14,650
接着呢

216
00:08:14,650 --> 00:08:16,420
再放在软件上面

217
00:08:16,420 --> 00:08:17,420
定义一个的Warp

218
00:08:17,420 --> 00:08:18,420
Warp的概念呢

219
00:08:18,420 --> 00:08:19,708
在之前已经给大家讲过了

220
00:08:19,708 --> 00:08:21,708
最后整个线程去执行的

221
00:08:21,708 --> 00:08:23,708
就是真正的Tensor Core

222
00:08:23,708 --> 00:08:26,708
下面呢逐层去打开具体的内容

223
00:08:27,483 --> 00:08:29,593
抛开CUDA和英伟达的所有概念呢

224
00:08:29,593 --> 00:08:30,593
看一个矩阵层

225
00:08:30,593 --> 00:08:32,593
也就是所谓的GEMM

226
00:08:32,593 --> 00:08:33,593
其实呢一次呢

227
00:08:33,593 --> 00:08:35,593
是计算一个小的矩阵块

228
00:08:35,593 --> 00:08:36,593
也就是把矩阵A呢

229
00:08:36,593 --> 00:08:37,593
拿出一个小块

230
00:08:37,593 --> 00:08:38,593
把矩阵B呢

231
00:08:38,593 --> 00:08:39,593
拿出一个小块

232
00:08:39,593 --> 00:08:41,593
算出来一个矩阵C

233
00:08:41,593 --> 00:08:42,593
那这个时候呢

234
00:08:42,593 --> 00:08:43,835
在整体的软件

235
00:08:43,835 --> 00:08:45,115
去编程的时候呢

236
00:08:45,115 --> 00:08:47,115
就会沿着每一个维度

237
00:08:47,115 --> 00:08:48,115
也就是沿着

238
00:08:48,115 --> 00:08:49,115
每个m啊k啊

239
00:08:49,115 --> 00:08:50,982
还有n啊进行切分

240
00:08:50,982 --> 00:08:51,913
具体呢就划分成为

241
00:08:51,913 --> 00:08:53,363
mTile跟mTile的

242
00:08:53,363 --> 00:08:55,363
一个独立的矩阵乘法

243
00:08:55,363 --> 00:08:57,000
通过这种累积

244
00:08:57,000 --> 00:08:59,000
n维跟n维还有k维的Tile呢

245
00:08:59,000 --> 00:09:00,425
把整个矩阵层累积起来

246
00:09:00,425 --> 00:09:01,425
计算出整个大的

247
00:09:01,425 --> 00:09:02,425
矩阵的结果

248
00:09:02,425 --> 00:09:03,425
在整个编程里面呢

249
00:09:03,425 --> 00:09:04,425
每一个维度呢

250
00:09:04,425 --> 00:09:05,425
就进行分开

251
00:09:05,425 --> 00:09:07,425
nB nB kB

252
00:09:07,425 --> 00:09:08,425
然后下面就算

253
00:09:08,425 --> 00:09:09,425
每一个小的

254
00:09:09,425 --> 00:09:12,425
nTile by nTile by kTile

255
00:09:12,425 --> 00:09:13,425
非常的拗口啊

256
00:09:13,425 --> 00:09:14,799
通过这种方式呢

257
00:09:14,799 --> 00:09:17,799
去逐层的拆分出来

258
00:09:17,799 --> 00:09:18,799
最终去运算的时候呢

259
00:09:18,799 --> 00:09:20,799
C等于A乘以B

260
00:09:20,799 --> 00:09:21,799
简单的最核心的

261
00:09:21,799 --> 00:09:22,799
就是这段话

262
00:09:22,799 --> 00:09:23,799
现在呢

263
00:09:23,799 --> 00:09:24,799
逐段的去拆出来

264
00:09:24,799 --> 00:09:25,799
在CUDA里面的GM呢

265
00:09:25,799 --> 00:09:27,799
主要是在线程块

266
00:09:27,799 --> 00:09:29,799
也就是Thread Block上面

267
00:09:29,799 --> 00:09:30,799
去进行并行

268
00:09:30,799 --> 00:09:31,799
那可以看到

269
00:09:31,799 --> 00:09:33,799
刚才的一个大的循环

270
00:09:33,799 --> 00:09:34,799
也就是外面的循环

271
00:09:34,799 --> 00:09:35,799
会把kB里面

272
00:09:35,799 --> 00:09:37,799
这个里面的所有的内容呢

273
00:09:37,799 --> 00:09:39,385
通过CUDA的线程块

274
00:09:39,385 --> 00:09:40,473
去承接

275
00:09:40,473 --> 00:09:42,473
进行一个并行的操作

276
00:09:43,000 --> 00:09:45,000
打开了线程块之后呢

277
00:09:45,000 --> 00:09:46,000
在线程块里面

278
00:09:46,000 --> 00:09:48,000
实际上具体的执行

279
00:09:48,000 --> 00:09:50,000
是分配到Warp level上面

280
00:09:50,000 --> 00:09:53,000
去执行具体的计算

281
00:09:53,000 --> 00:09:54,000
那这个时候呢

282
00:09:54,000 --> 00:09:56,000
会把A矩阵跟B矩阵

283
00:09:56,000 --> 00:09:59,000
加载到这里面的SMEM

284
00:09:59,000 --> 00:10:00,546
也就是共享内存里面

285
00:10:00,546 --> 00:10:01,546
结果矩阵C

286
00:10:01,546 --> 00:10:03,170
就会分布到不同的Warp上面

287
00:10:03,170 --> 00:10:04,170
每个Warp呢

288
00:10:04,170 --> 00:10:05,680
负责独立的一个计算

289
00:10:05,680 --> 00:10:06,680
那现在看看

290
00:10:06,680 --> 00:10:07,680
往左边看看

291
00:10:07,680 --> 00:10:09,430
具体的计算的公式

292
00:10:09,430 --> 00:10:10,680
对kb的大的循环呢

293
00:10:10,680 --> 00:10:11,680
已经分配到

294
00:10:11,680 --> 00:10:13,680
不同的线程块上面去执行

295
00:10:13,680 --> 00:10:15,680
那针对每一个CUDA Warp呢

296
00:10:15,680 --> 00:10:17,430
去真正执行的

297
00:10:17,430 --> 00:10:18,430
是下面最核心的

298
00:10:18,430 --> 00:10:20,430
kTile这一块循环

299
00:10:20,430 --> 00:10:22,430
接下来再打开一层

300
00:10:22,430 --> 00:10:24,105
对于Warp level这一层

301
00:10:24,105 --> 00:10:26,105
来看看具体是怎么执行的

302
00:10:26,105 --> 00:10:27,105
首先呢

303
00:10:27,105 --> 00:10:28,105
会把ATile跟BTile

304
00:10:28,105 --> 00:10:30,105
就是共享内存里面的数据呢

305
00:10:30,105 --> 00:10:31,388
加载到RF

306
00:10:31,388 --> 00:10:34,063
也就是具体的寄存器

307
00:10:34,063 --> 00:10:35,063
那AFragment跟BFragment

308
00:10:35,063 --> 00:10:38,063
就是寄存器里面具体的空间或者数据

309
00:10:38,063 --> 00:10:40,282
值得注意的就是

310
00:10:40,282 --> 00:10:42,282
数据加载要快于计算

311
00:10:42,282 --> 00:10:42,890
否则的话

312
00:10:42,890 --> 00:10:43,890
就会严重的影响

313
00:10:43,890 --> 00:10:45,890
整个计算的吞吐

314
00:10:45,890 --> 00:10:46,890
结果矩阵C呢

315
00:10:46,890 --> 00:10:47,890
会比较大

316
00:10:47,890 --> 00:10:48,890
所以会存储在

317
00:10:48,890 --> 00:10:49,890
线程参加执行的

318
00:10:49,890 --> 00:10:51,229
寄存器上面

319
00:10:51,229 --> 00:10:52,229
那说白了

320
00:10:52,229 --> 00:10:54,229
就是都存在寄存器上面

321
00:10:54,229 --> 00:10:56,696
共享内存里面的整体的格式

322
00:10:56,696 --> 00:10:57,696
所谓的Layout呢

323
00:10:57,696 --> 00:10:59,696
是以K为进行存储

324
00:10:59,696 --> 00:11:00,696
这种方式

325
00:11:00,696 --> 00:11:01,446
会使得

326
00:11:01,446 --> 00:11:02,524
线程执行的更加高效

327
00:11:02,524 --> 00:11:05,505
所以在整体的循环里面

328
00:11:05,505 --> 00:11:06,997
再打开一层

329
00:11:06,997 --> 00:11:07,997
具体的线程

330
00:11:07,997 --> 00:11:09,997
M跟N之间的相乘呢

331
00:11:09,997 --> 00:11:11,997
就是在CUDA file里面

332
00:11:11,997 --> 00:11:12,997
去执行的

333
00:11:12,997 --> 00:11:13,997
就具体的线程

334
00:11:13,997 --> 00:11:15,377
去执行了

335
00:11:15,377 --> 00:11:16,377
那线程控制不了

336
00:11:16,377 --> 00:11:17,377
大部分的时候呢

337
00:11:17,377 --> 00:11:18,377
还是控制Warp

338
00:11:18,377 --> 00:11:19,377
现在呢

339
00:11:19,377 --> 00:11:21,377
再打开看看Tensor Core

340
00:11:21,377 --> 00:11:23,116
上面的并行执行

341
00:11:23,116 --> 00:11:23,796
实际上呢

342
00:11:23,796 --> 00:11:25,116
Tensor Core并行执行

343
00:11:25,116 --> 00:11:26,116
就是执行刚才

344
00:11:26,116 --> 00:11:28,116
A乘以B等于C

345
00:11:28,116 --> 00:11:29,116
这么一个简单

346
00:11:29,116 --> 00:11:30,904
最核心最里面的一个操作

347
00:11:31,404 --> 00:11:33,404
Tensor Core是英伟达对应的硬件

348
00:11:33,404 --> 00:11:33,979
但是呢

349
00:11:33,979 --> 00:11:34,979
CUDA对应的的API呢

350
00:11:34,979 --> 00:11:35,979
是WMMA

351
00:11:35,979 --> 00:11:36,979
通过WMMA呢

352
00:11:36,979 --> 00:11:37,979
对外提供

353
00:11:37,979 --> 00:11:39,979
具体的计算的能力

354
00:11:39,979 --> 00:11:40,979
也就是MMA的

355
00:11:40,979 --> 00:11:41,979
Load Storage

356
00:11:41,979 --> 00:11:42,979
MMA Sync

357
00:11:42,979 --> 00:11:44,979
这几个API呢

358
00:11:44,979 --> 00:11:46,979
完成整个Tensor Core的计算

359
00:11:47,429 --> 00:11:48,429
现在呢

360
00:11:48,429 --> 00:11:49,429
看到

361
00:11:49,429 --> 00:11:51,429
GEMM矩阵的软硬件分层里面呢

362
00:11:51,429 --> 00:11:52,429
每一层都需要

363
00:11:52,429 --> 00:11:53,429
对数据进行复用

364
00:11:53,429 --> 00:11:55,429
在整一个块里面呢

365
00:11:55,429 --> 00:11:56,429
大矩阵呢

366
00:11:56,429 --> 00:11:57,429
会把大矩阵

367
00:11:57,429 --> 00:11:58,429
放在不同的

368
00:11:58,429 --> 00:11:59,429
全局内存里面

369
00:11:59,429 --> 00:12:00,429
在对于局部内存呢

370
00:12:00,429 --> 00:12:02,429
分块来进行存

371
00:12:02,429 --> 00:12:03,429
那分块里面呢

372
00:12:03,429 --> 00:12:05,429
会把一些具体的数据呢

373
00:12:05,429 --> 00:12:06,843
存在寄存器里面

374
00:12:06,843 --> 00:12:07,843
所以它整体呢

375
00:12:07,843 --> 00:12:08,843
每一层的数据

376
00:12:08,843 --> 00:12:10,843
都进行大量的去复用

377
00:12:10,843 --> 00:12:11,843
因为矩阵

378
00:12:11,843 --> 00:12:12,843
确实太大了

379
00:12:12,843 --> 00:12:13,843
有非常多的数据呢

380
00:12:13,843 --> 00:12:14,843
进行每一块的利用

381
00:12:14,843 --> 00:12:15,843
那现在呢

382
00:12:15,843 --> 00:12:16,843
GEMM

383
00:12:16,843 --> 00:12:17,843
具体的计算

384
00:12:17,843 --> 00:12:19,093
已经算完了

385
00:12:19,093 --> 00:12:20,093
但是算完

386
00:12:20,093 --> 00:12:21,093
每一次的结果呢

387
00:12:21,093 --> 00:12:22,518
都是很小的一块结果

388
00:12:22,518 --> 00:12:25,132
怎么把整个矩阵

389
00:12:25,132 --> 00:12:26,132
或者累积矩阵

390
00:12:26,132 --> 00:12:27,132
成的结果呢

391
00:12:27,132 --> 00:12:28,132
写回去

392
00:12:28,132 --> 00:12:31,132
外面的这个C里面呢

393
00:12:31,132 --> 00:12:31,882
这个时候呢

394
00:12:31,882 --> 00:12:32,882
其实刚才

395
00:12:32,882 --> 00:12:35,882
漏了一个收尾的工作

396
00:12:38,000 --> 00:12:39,000
现在看看

397
00:12:39,000 --> 00:12:41,000
WMMA的结果回存呢

398
00:12:41,000 --> 00:12:42,000
刚才结果呢

399
00:12:42,000 --> 00:12:43,000
会存在

400
00:12:43,000 --> 00:12:44,000
regest file

401
00:12:44,000 --> 00:12:45,000
CUDA通过提供

402
00:12:45,000 --> 00:12:46,612
这个WMMA的API

403
00:12:47,000 --> 00:12:49,464
这里面有个store matrix sync

404
00:12:49,464 --> 00:12:50,464
这个工作呢

405
00:12:50,464 --> 00:12:51,464
去把所有的数据呢

406
00:12:51,464 --> 00:12:52,464
都搬到

407
00:12:52,464 --> 00:12:53,464
共享内存

408
00:12:53,464 --> 00:12:54,464
也就是SMEM里面

409
00:12:54,464 --> 00:12:55,464
对于SMEM呢

410
00:12:55,464 --> 00:12:56,464
会做大量的

411
00:12:56,464 --> 00:12:57,674
累积的操作

412
00:12:57,674 --> 00:12:58,674
那累积的操作呢

413
00:12:58,674 --> 00:13:00,674
既然是一条计算

414
00:13:00,674 --> 00:13:01,674
会搬到

415
00:13:01,674 --> 00:13:02,674
具体的寄存器里面

416
00:13:02,674 --> 00:13:03,674
去进行计算

417
00:13:03,674 --> 00:13:04,674
最后呢

418
00:13:04,674 --> 00:13:05,674
把所有的数据

419
00:13:05,674 --> 00:13:06,674
这些数据

420
00:13:06,674 --> 00:13:07,674
累积起来

421
00:13:07,674 --> 00:13:08,674
然后放在

422
00:13:08,674 --> 00:13:09,674
全局内存里面

423
00:13:09,674 --> 00:13:10,674
通过全局内存呢

424
00:13:10,674 --> 00:13:11,674
把一个块一个块的

425
00:13:11,674 --> 00:13:12,674
拼接起来

426
00:13:12,674 --> 00:13:13,674
把数据呢

427
00:13:13,674 --> 00:13:14,674
恢复回来

428
00:13:15,000 --> 00:13:16,000
最后呢

429
00:13:16,000 --> 00:13:17,000
总结一下

430
00:13:17,000 --> 00:13:17,644
整个完整的

431
00:13:17,644 --> 00:13:18,644
GM的计算

432
00:13:18,644 --> 00:13:19,394
或者存储的

433
00:13:19,394 --> 00:13:20,394
数据流呢

434
00:13:20,394 --> 00:13:21,394
就是首先第一步

435
00:13:21,394 --> 00:13:22,394
会对矩阵

436
00:13:22,394 --> 00:13:22,987
进行分块

437
00:13:22,987 --> 00:13:23,691
放在

438
00:13:23,691 --> 00:13:24,587
global memory里面

439
00:13:24,587 --> 00:13:26,587
接着进行划分

440
00:13:26,587 --> 00:13:27,587
把具体的

441
00:13:27,587 --> 00:13:28,587
局部的数据

442
00:13:28,587 --> 00:13:29,587
或者fragment呢

443
00:13:29,587 --> 00:13:30,587
放在shared memory

444
00:13:30,587 --> 00:13:31,587
共享内存里面

445
00:13:31,587 --> 00:13:32,637
真正执行的时候

446
00:13:32,637 --> 00:13:34,018
是放在寄存器里面

447
00:13:34,018 --> 00:13:35,018
以Warp level

448
00:13:35,018 --> 00:13:36,018
进行执行

449
00:13:36,018 --> 00:13:37,318
在Tensor Core里面

450
00:13:37,318 --> 00:13:38,318
执行完之后呢

451
00:13:38,318 --> 00:13:39,318
会回传到

452
00:13:39,318 --> 00:13:40,318
SMEM里面

453
00:13:40,318 --> 00:13:41,318
不是存储到

454
00:13:41,318 --> 00:13:42,961
register file了

455
00:13:42,961 --> 00:13:46,000
因为是从Tensor Core的register file写出来

456
00:13:46,000 --> 00:13:46,725
然后呢

457
00:13:46,725 --> 00:13:47,300
进行一个

458
00:13:47,300 --> 00:13:48,300
累加或者

459
00:13:48,300 --> 00:13:49,300
融合算子的操作

460
00:13:49,300 --> 00:13:49,925
这里面可能会

461
00:13:49,925 --> 00:13:50,925
融合其他算子

462
00:13:50,925 --> 00:13:51,674
例如

463
00:13:51,674 --> 00:13:52,674
卷积加维度啊

464
00:13:52,674 --> 00:13:54,430
matmul加各种激活

465
00:13:54,430 --> 00:13:55,430
再往后呢

466
00:13:55,430 --> 00:13:56,430
就把所有的数据呢

467
00:13:56,430 --> 00:13:57,430
写回

468
00:13:57,430 --> 00:13:59,262
全局内存

469
00:14:00,000 --> 00:14:01,000
那今天的内容呢

470
00:14:01,000 --> 00:14:02,000
就到这里为止

471
00:14:03,250 --> 00:14:05,054
ZOMI主要跟大家分享了

472
00:14:05,054 --> 00:14:05,517
Tensor Core的

473
00:14:05,517 --> 00:14:06,517
具体的执行

474
00:14:06,517 --> 00:14:07,517
把每一个计算呢

475
00:14:07,517 --> 00:14:08,517
打开看一看

476
00:14:08,517 --> 00:14:09,517
具体的硬件

477
00:14:09,517 --> 00:14:10,517
指令流水

478
00:14:10,517 --> 00:14:11,517
是怎么去执行的

479
00:14:11,517 --> 00:14:12,517
有了指令流水之后

480
00:14:12,517 --> 00:14:13,517
CUDA的

481
00:14:13,517 --> 00:14:14,517
线程

482
00:14:14,517 --> 00:14:15,517
是怎么完成

483
00:14:15,517 --> 00:14:16,517
或者怎么跟

484
00:14:16,517 --> 00:14:17,517
指令流水配合的

