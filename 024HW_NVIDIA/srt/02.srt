1
00:00:00,000 --> 00:00:04,300
Subtitle：PlusV98

2
00:00:04,750 --> 00:00:07,000
大家好,我是ZOMI

3
00:00:07,000 --> 00:00:09,312
现在来到了Tensor Core的第二弹

4
00:00:09,312 --> 00:00:12,025
去看看Tensor Core的架构演进

5
00:00:12,025 --> 00:00:15,584
其实呢,在整个英伟达的GPU架构里面呢

6
00:00:15,584 --> 00:00:18,212
讲了非常多简单的概念

7
00:00:18,212 --> 00:00:20,192
特别是从Turing到Hopper架构呢

8
00:00:20,192 --> 00:00:23,159
整个Tensor Core呢是发展了非常多代

9
00:00:23,159 --> 00:00:23,393
那现在呢

10
00:00:23,393 --> 00:00:26,043
来到了整个Tensor Core的架构的演进

11
00:00:26,043 --> 00:00:30,401
去看看每一代不同的英伟达的GPU的架构

12
00:00:30,401 --> 00:00:34,000
现在从Volta架构到Turing,Ampere,Hopper架构

13
00:00:34,000 --> 00:00:35,900
去看看每一代的Tensor Core都

14
00:00:35,900 --> 00:00:37,732
有哪些不一样的点

15
00:00:39,400 --> 00:00:42,379
回顾一下,在第一代Volta架构

16
00:00:42,379 --> 00:00:43,492
里面的Tensor Core呢

17
00:00:43,492 --> 00:00:45,739
主要是4x4的矩阵

18
00:00:45,739 --> 00:00:49,699
输入是Fp16的A和B两个矩阵

19
00:00:49,699 --> 00:00:52,534
然后加上Fp32和Fp16的矩阵之后呢

20
00:00:52,534 --> 00:00:55,243
得到Fp16和Fp32的矩阵

21
00:00:55,243 --> 00:00:58,296
通过这种方式呢进行混合精度的计算

22
00:00:58,296 --> 00:01:00,619
真正线程执行的时候呢

23
00:01:00,619 --> 00:01:03,542
是把A矩阵的一行乘上B矩阵的一列

24
00:01:03,542 --> 00:01:05,654
再加上C矩阵的一个元素

25
00:01:05,654 --> 00:01:08,214
得到D矩阵里面的其中一个元素

26
00:01:08,214 --> 00:01:11,947
这个呢就是Tensor Core里面的FMA指令

27
00:01:11,947 --> 00:01:13,675
执行的具体的内

28
00:01:15,025 --> 00:01:18,431
现在来看看今天的主要的概念

29
00:01:18,431 --> 00:01:20,765
Tensor Core的历代的发展

30
00:01:20,765 --> 00:01:22,030
一共呢经历了4代

31
00:01:22,030 --> 00:01:24,099
看一下每一代有哪些不一样

32
00:01:24,974 --> 00:01:27,545
首先呢Tensor Core是从Volta架构开始的

33
00:01:27,545 --> 00:01:29,134
当初呢只是Fp32

34
00:01:29,134 --> 00:01:31,449
直到Turing架构呢,它支持的更多

35
00:01:31,449 --> 00:01:33,166
到现在的H100呢

36
00:01:33,166 --> 00:01:35,481
它基本上的支持格式呢就更多了

37
00:01:35,481 --> 00:01:37,721
每一代的支持格式也会越来越多

38
00:01:37,721 --> 00:01:40,942
可以预见下一代英伟达的GPU的产品呢

39
00:01:40,942 --> 00:01:43,545
也可能支持更多精度的指令

40
00:01:46,500 --> 00:01:50,158
现在呢来到了第一代英伟达的架构

41
00:01:50,158 --> 00:01:51,936
Volta架构里面的SM

42
00:01:51,936 --> 00:01:54,992
左边的这个就是SM的原来的架构图

43
00:01:54,992 --> 00:01:58,614
SM里面呢有4个子核,叫做Sub Core

44
00:01:58,614 --> 00:02:01,102
那左边的这一块呢就是一个

45
00:02:01,102 --> 00:02:02,926
右边的这一块呢又是一个

46
00:02:02,926 --> 00:02:04,314
一共呢有4个子核

47
00:02:04,314 --> 00:02:07,611
现在后面都会以右边的这个形式去呈现

48
00:02:07,611 --> 00:02:12,913
首先呢一个SM里面有L1的I $,L1的指令缓存

49
00:02:12,913 --> 00:02:16,945
对应呢有4个子核Sub Core,1234

50
00:02:17,027 --> 00:02:20,552
而Sub Core下面呢又有对应的L1的缓存

51
00:02:20,552 --> 00:02:23,277
还有Share Memory,就是共享内存

52
00:02:23,277 --> 00:02:26,752
值得注意的就是看看下面的箭头

53
00:02:26,752 --> 00:02:28,337
首先呢L1 Cache里面呢

54
00:02:28,337 --> 00:02:30,692
会把具体的指令呢发到Sub Core

55
00:02:30,692 --> 00:02:34,242
这里面的指令呢或者这里面的箭头呢是单向的

56
00:02:34,242 --> 00:02:37,008
Sub Core计算完之后呢,它的箭头呢是双向的

57
00:02:37,008 --> 00:02:40,648
也就是可以获取计算数据或者对数据呢

58
00:02:40,648 --> 00:02:42,000
进行重复的计算

59
00:02:43,145 --> 00:02:47,291
现在呢重新的去解析一下第一代的Tensor Core

60
00:02:47,291 --> 00:02:48,336
也就是Volta架构的Tensor Core

61
00:02:48,336 --> 00:02:52,043
首先呢SM呢它是主要负责对寄存器里面的整体

62
00:02:52,043 --> 00:02:54,992
逻辑进行读和写和计算的

63
00:02:55,108 --> 00:02:56,939
而这里面的每一个Sub Core呢

64
00:02:56,939 --> 00:03:00,755
就包括了Tensor Core,IP64,FP32,还有INT8

65
00:03:00,755 --> 00:03:03,670
当然还有特殊的处理单元MFU

66
00:03:03,670 --> 00:03:06,539
所以Sub Core呢不仅是指Tensor Core

67
00:03:06,539 --> 00:03:09,021
也不仅仅是指CUDA Core

68
00:03:09,021 --> 00:03:12,299
Cuda Core/Tensor Core/RT Core

69
00:03:12,299 --> 00:03:14,114
都包在Sub Core里面

70
00:03:14,414 --> 00:03:16,260
在每一个Sub Core的上面呢

71
00:03:16,260 --> 00:03:18,543
就是我鼠标的所在的这一块位置呢

72
00:03:18,600 --> 00:03:20,388
其实还有一个Warp Schedule

73
00:03:20,388 --> 00:03:23,826
专门针对现成的Warp进行调度的

74
00:03:23,826 --> 00:03:27,308
数据呢就存储在L1或者L0的Cache里面

75
00:03:28,233 --> 00:03:31,170
视频里谈到了针对第一代架构的Tensor Core呢

76
00:03:31,170 --> 00:03:35,429
每一个Sub Core都有一个4x4x4的Tensor Core

77
00:03:35,429 --> 00:03:39,480
而Warp Schedule呢向Tensor Core发送具体的矩阵乘法

78
00:03:39,480 --> 00:03:42,050
也就是GML,WAM的运算指令

79
00:03:42,050 --> 00:03:44,824
计算完之后呢,就会从寄存器

80
00:03:44,824 --> 00:03:46,648
也就是中间的这个位置

81
00:03:46,648 --> 00:03:50,261
去读取或者去接收ABC矩阵

82
00:03:50,261 --> 00:03:52,216
A矩阵和B矩阵呢是FP16

83
00:03:52,216 --> 00:03:54,520
C矩阵呢就是FP32或者FP16

84
00:03:54,520 --> 00:03:57,388
执行多次的4x4的矩阵乘法

85
00:03:57,388 --> 00:03:59,512
直到完成整个矩阵运算之后呢

86
00:03:59,512 --> 00:04:02,396
将所得的的矩阵呢写回去寄存器

87
00:04:02,396 --> 00:04:03,910
也就是Register File

88
00:04:03,910 --> 00:04:06,446
或者Share Memory里面

89
00:04:07,350 --> 00:04:09,556
接下来对左边的这个SM图呢

90
00:04:09,556 --> 00:04:12,625
进行再打开看一下SM的微架构

91
00:04:12,625 --> 00:04:14,644
首先呢也是从上往下看

92
00:04:14,644 --> 00:04:18,000
上面就是一个共享的L1缓存

93
00:04:18,000 --> 00:04:19,636
每个时钟周期呢可以

94
00:04:19,636 --> 00:04:21,451
执行4个WarpInstruction

95
00:04:21,451 --> 00:04:22,950
下属4个Sub Core是独立的

96
00:04:22,950 --> 00:04:25,195
里面的数据呢是不进行缓存的

97
00:04:25,195 --> 00:04:27,509
但Sub Core里面的有两个Tensor Core嘛

98
00:04:27,509 --> 00:04:29,899
两个Tensor Core的数据呢是

99
00:04:29,899 --> 00:04:32,366
可以共享的,再往下呢有一个共享内存

100
00:04:32,366 --> 00:04:34,411
共享内存每个时钟周期呢可以执行

101
00:04:34,411 --> 00:04:37,920
或者可以传输128B的数据

102
00:04:37,920 --> 00:04:40,391
当SMEM计算完这个全数矩阵之后呢

103
00:04:40,391 --> 00:04:43,723
就会把数据呢回传到L2的Cache里面

104
00:04:43,723 --> 00:04:46,041
最后呢就返回到host CPU

105
00:04:47,239 --> 00:04:50,178
现在继续展开一下刚才的一个Sub Core里

106
00:04:50,178 --> 00:04:52,725
面的微架构,Sub Core里面的微架构呢

107
00:04:52,725 --> 00:04:54,023
就很多的内容了,刚才

108
00:04:54,023 --> 00:04:56,231
看到的L1 Cache呢是在上面

109
00:04:56,231 --> 00:04:57,458
L1 Cache里面呢

110
00:04:57,458 --> 00:05:02,042
具体的执行的就会到L0 Cache或者叫Register File

111
00:05:02,042 --> 00:05:04,583
把一些数据呢传输到这

112
00:05:04,583 --> 00:05:07,762
具体的指令的分发呢是通过Warp Schedule的

113
00:05:07,762 --> 00:05:10,642
针对的计算呢通过Math Dispatch Unit

114
00:05:10,642 --> 00:05:11,560
分发到具体的

115
00:05:11,560 --> 00:05:14,584
FP64、BP32、FP32还有MMU

116
00:05:14,584 --> 00:05:16,588
这几个具体的执行单元器计算

117
00:05:16,588 --> 00:05:19,192
但是呢如果调用的是WMMA

118
00:05:19,192 --> 00:05:21,480
相关的API或者相关的指令呢

119
00:05:21,480 --> 00:05:23,688
Warp Schedule呢就直接去触发或者去执行

120
00:05:23,688 --> 00:05:25,289
Tensor Core里面的计算

121
00:05:25,312 --> 00:05:27,432
Tensor Core里面呢就有两个4x4x4

122
00:05:27,432 --> 00:05:30,537
的Tensor去每个时钟周期去执行

123
00:05:30,537 --> 00:05:35,000
最后呢就把数据呢回存到Register File,也就是寄存器里面

124
00:05:35,800 --> 00:05:40,800
寄存器再通过MIO的Data Pipe呢跟Shared Memory进行通讯

125
00:05:42,425 --> 00:05:44,420
这里面有大量的数据传输

126
00:05:44,420 --> 00:05:48,516
数据呢存在哪里非常的关键,于是呢现在打开一下

127
00:05:48,516 --> 00:05:52,516
看一下L1的缓存还有共享的内存之间的一个关系

128
00:05:53,325 --> 00:05:57,312
在Volta架构里面呢对比起P100呢有一个很大的改进点

129
00:05:57,312 --> 00:06:02,022
就是把L1的缓存呢跟共享内存的合并成为同一块空间

130
00:06:02,022 --> 00:06:08,486
共享内存的SMEM呢可以为整个SM呢提供高达96KB的存储空间

131
00:06:08,486 --> 00:06:14,693
针对L2也就是对应的Ever Cast呢也进行了更新,已经有了一个5%到15%的提升

132
00:06:14,693 --> 00:06:21,338
Volta架构里面的Sub Core也就V架构里面呢单独提供了一个Tensor Core的指令

133
00:06:21,338 --> 00:06:27,321
提供给Warp Schedule,Warp Schedule呢直接去执行,不需要通过Mesh Dispatch Unit去进行分发

134
00:06:27,321 --> 00:06:33,321
除了Tensor Core是专门针对AI框的矩阵进行计算之外呢,Volta架构还减少了指令的延迟

135
00:06:34,871 --> 00:06:39,296
现在来到了Tensor Core的第二代,去看一下Turing架构里面的Tensor Core

136
00:06:39,296 --> 00:06:42,875
那首先呢SM就不管了,直接打开Sub Core

137
00:06:42,875 --> 00:06:45,232
就是V核 在Turing架构里面呢

138
00:06:45,232 --> 00:06:50,992
tensor core除了原先的FP16呢,其实还增加了INT8和INT4多种类型

139
00:06:50,992 --> 00:06:56,400
另外的话还是FP16的FastPath每个时钟周期呢,可以执行32次

140
00:06:56,400 --> 00:07:01,400
而原来的IP到8个时钟周期内呢,可以执行单个多线程GEM的计算

141
00:07:01,400 --> 00:07:05,200
也就是计算频率或者计算计算吞吐就更高了

142
00:07:06,000 --> 00:07:11,800
第二代的Turing架构提出了,其实距离上一代的Volta架构呢只是距离了一年

143
00:07:11,800 --> 00:07:15,952
那现在看看第三代的Tensor Core有哪些巨大的改变

144
00:07:15,952 --> 00:07:22,373
首先呢,现在呢去澄清或者去给大家汇报的一个内容,就是多级的缓存或者多级数据的带宽

145
00:07:22,373 --> 00:07:30,373
首先呢,看到的就是NVLink,NVLink呢是针对单节点多卡之间进行数据互联的

146
00:07:30,373 --> 00:07:37,373
再往上,L2和DRAM呢,就是针对每一款每一块GPU卡里面的系统内存

147
00:07:37,373 --> 00:07:41,550
再往上呢,就是每一个SM里面的内存,首先有共享内存呢,有IL了

148
00:07:41,550 --> 00:07:44,100
针对具体的计算的就是Math了

149
00:07:44,100 --> 00:07:47,908
包括Tensor Core或者CUDA Core都是取决于Math

150
00:07:47,908 --> 00:07:53,725
而真正的A100呢,它最重要的改变就是Movement Efficiently

151
00:07:53,725 --> 00:07:57,175
就是数据搬运更加的快,有了一个三倍的提升

152
00:07:57,175 --> 00:07:59,176
现在看看它具体是怎么做的

153
00:08:00,301 --> 00:08:03,875
首先呢,看到NPA架构之前呢,包括Turing架构,Volta架构

154
00:08:03,875 --> 00:08:06,500
如果或者如果Tensor Core呢,想要使用共享内存

155
00:08:06,500 --> 00:08:12,592
就必须要把数据呢,从全局内存里面去加载到寄存器,也就是Register File

156
00:08:12,592 --> 00:08:17,621
然后再写进去共享内存,整体来说是数据要搬来搬去

157
00:08:17,621 --> 00:08:21,573
除了增加数据的搬运呢,其实还影响了时延

158
00:08:21,573 --> 00:08:26,149
于是呢,NPA架构呢,就提出了提供一个异步的内存拷贝机制

159
00:08:26,149 --> 00:08:31,141
通过一个具体的新的指令,叫做LDGSTS

160
00:08:31,141 --> 00:08:34,289
这个呢,指令叫做Load Global Storage Shared

161
00:08:34,289 --> 00:08:42,850
实现了全局内存不需要经过具体的寄存器,直接加载到共享内存里面

162
00:08:42,850 --> 00:08:44,420
那这个呢,看看具体怎么做

163
00:08:44,927 --> 00:08:49,875
首先,V100原来的操作方式,如果想要使用共享内存呢

164
00:08:49,875 --> 00:08:53,248
就需要从LU把共享内存的搬到寄存器堆里边

165
00:08:53,248 --> 00:08:54,433
也就是Register File里面呢,然后再给内存呢

166
00:08:54,433 --> 00:08:58,144
就会搬到F里面,具体给Tensor Core去执行

167
00:08:58,144 --> 00:09:00,547
那上面就是Tensor Core

168
00:09:00,547 --> 00:09:03,555
每一次数据搬运都会非常的占用时延

169
00:09:03,555 --> 00:09:08,188
在A100里面呢,A100里面呢,就提出了一个软件的Sync Copy

170
00:09:08,188 --> 00:09:13,350
异步的拷贝机制,通过新的指令,可以把L2 Cache里面的全局内存

171
00:09:13,350 --> 00:09:18,950
直接搬到SMEM共享内存里面,然后给Register File呢,直接的去执行

172
00:09:18,950 --> 00:09:21,850
每次数据搬运都会增加时延、功耗

173
00:09:22,525 --> 00:09:26,964
大家有没有注意到,A100跟V100,除了中间的这个传输之外呢

174
00:09:26,964 --> 00:09:33,108
其实有一点呢,漏掉了,就是在V100里面呢,需要4个读取的数据给到Warp

175
00:09:33,108 --> 00:09:34,647
Warp只需要读取两次

176
00:09:34,647 --> 00:09:38,790
这里面呢,又有另外一个技术,就是Ampere架构的Tensor Core

177
00:09:38,790 --> 00:09:42,211
就是一个Warp呢,就提供了32个线程,32个线程可以共享数据

178
00:09:42,211 --> 00:09:45,795
而Volta架构里面呢,整个Tensor Core只有8个线程

179
00:09:45,795 --> 00:09:50,300
这样的好处 可以在线程之间呢,减少矩阵的数据搬运

180
00:09:50,300 --> 00:09:51,329
因此呢,数据搬运的次数呢

181
00:09:51,329 --> 00:09:54,000
会比V100更少

182
00:09:54,000 --> 00:10:02,315
看一下具体的FFMA,也就是Fuse Fold MathMath and Add,就是矩阵乘加的操作

183
00:10:02,315 --> 00:10:05,045
解读一下上面的这个图啊

184
00:10:05,045 --> 00:10:09,150
绿色的这些小块呢,叫做Support,就是刚才提到的Support

185
00:10:09,150 --> 00:10:15,413
包括这里面的图呢,就是Tensor Core,而连续的蓝色的框呢,就表示寄存器Register

186
00:10:15,413 --> 00:10:18,155
当寄存器纯粹使用CUDA Core的时候呢

187
00:10:18,155 --> 00:10:24,278
会把所有的数据呢,都放在Register里面,每个Register呢,针对一个CUDA Core的数据呢,进行传输

188
00:10:24,278 --> 00:10:26,774
所以使用CUDA Core呢,是算得非常慢的

189
00:10:26,774 --> 00:10:29,552
在V100里面呢,每个Tensor Core呢,可以处理8个线程

190
00:10:29,552 --> 00:10:31,052
每个线程都有自己的寄存器

191
00:10:31,052 --> 00:10:35,235
所以在8个时钟周期内呢,可以执行1024个MAC的操作

192
00:10:35,235 --> 00:10:40,304
那下面呢,就是A100的TC,就是Tensor Core的指令啊

193
00:10:40,304 --> 00:10:44,307
可以看到,每个Tensor Core呢,可以32条线程,现在32条线程

194
00:10:44,307 --> 00:10:51,000
因此呢,可以在8个时钟周期内呢,去寄存2048个MAC,每个时钟周期处理其中一块的数据

195
00:10:51,675 --> 00:10:55,727
第三代的Tensor CoreAmpere架构里面呢,除了制造工艺提升之外呢

196
00:10:55,727 --> 00:11:01,809
还提供了更多的线程使得硬件执行更快,数据搬运的更少,每个时钟的吞吐呢,更大

197
00:11:03,450 --> 00:11:08,337
大家看完这个图之后呢,有没有一点感觉

198
00:11:08,337 --> 00:11:14,353
就是为什么出现了Tensor Core之后呢,会比单纯的使用CUDA Core执行的更快呢?

199
00:11:14,353 --> 00:11:16,319
针对矩阵计算

200
00:11:16,319 --> 00:11:18,401
具体呢,是因为线程啊,每一次针对Tensor Core

201
00:11:18,401 --> 00:11:22,526
都可以处理的更快,处理的更多,吞吐是不一样的

202
00:11:24,900 --> 00:11:28,664
现在来到了2022年提出的Hopper架构

203
00:11:28,664 --> 00:11:33,750
里面呢,就提出了第4代Tensor Core,第4代Tensor Core其实呢,有三个重要的改变的点

204
00:11:33,750 --> 00:11:38,459
现在来回顾一下,其实前三代的Tensor Core呢,都是基于Warp-Level进行编程的

205
00:11:38,459 --> 00:11:45,874
那英伟达架构A100里面呢,做了软件的异步加载之后呢,其实它还是基于Web-Level进行编程

206
00:11:45,874 --> 00:11:51,127
简单地来说呢,就是把数据呢,从HBM,就是全局内存,加载到寄存器

207
00:11:51,127 --> 00:11:54,766
再通过Warp Scheduleh去调用,后来完成整个矩阵的运算

208
00:11:54,766 --> 00:11:58,862
最后再把数据回传到寄存器,再不断地去运算

209
00:11:58,862 --> 00:12:01,256
这整一个过程呢,它其实有两个问题

210
00:12:01,256 --> 00:12:06,862
那第一个问题呢,就是数据的搬运和计算是严重地去偶合的

211
00:12:06,862 --> 00:12:11,048
线程加载矩阵数据的时候呢,其实会独立地去获取矩阵的地址

212
00:12:11,048 --> 00:12:14,505
会非常消耗继承器的数量还有存储的带宽

213
00:12:14,505 --> 00:12:19,435
第二个就是可扩展性受到约束,因为单个Warp里面的线程是有限的

214
00:12:19,435 --> 00:12:24,235
而单个Warp里面的线程有限呢,就会导致矩阵的计算的规格受到约束

215
00:12:24,235 --> 00:12:26,180
于是呢,在第4代Tensor Core里面呢

216
00:12:26,180 --> 00:12:34,180
就提出了一个TMA,叫做Tensor Memory Accelerator,就是增量内存加速的功能

217
00:12:34,180 --> 00:12:41,700
左边这个图呢,就是A100,就是上一代的安培架构里面的一个整体的SM的架构图

218
00:12:41,700 --> 00:12:44,356
右边呢,就是H100里面的SM的架构图

219
00:12:44,356 --> 00:12:48,610
可以看到基本上没有太多的差别,除了因为工艺制程的原因呢

220
00:12:48,610 --> 00:12:51,746
导致这里面的CUDA Core和Tensor Core的密度更高之外呢

221
00:12:51,746 --> 00:12:55,522
最重要的就是多了一个Tensor Memory Accelerator

222
00:12:55,522 --> 00:12:59,198
简称叫做TMA,也就是硬件的数据异步加载

223
00:12:59,198 --> 00:13:06,580
在A100里面呢,其实已经提出了一个软件的数据异步加载,这里面呢,TMA直接把它硬件化了

224
00:13:06,580 --> 00:13:11,188
非常方便把全局内存的数据呢,异步加载到共享内存

225
00:13:11,188 --> 00:13:14,580
直接给Register File 寄存器去读写

226
00:13:15,200 --> 00:13:20,185
H100之前的架构里面呢,大部分都是从左边的图所示啊

227
00:13:20,185 --> 00:13:23,897
只有Grid和Block之分,线程是没有办法控制的

228
00:13:23,897 --> 00:13:28,692
所以呢,因此呢,针对硬件呢,分别对应的是一个SM,SM对应的是Block

229
00:13:28,692 --> 00:13:33,044
而Grid呢,是对应整个Devices,也就是单块GPU

230
00:13:33,044 --> 00:13:38,420
局部的数据呢,只能通过Shared Memory在SM内进行共享

231
00:13:38,420 --> 00:13:41,172
跨SM之间呢,是不能够处理的

232
00:13:41,172 --> 00:13:48,556
而Hopper架构呢,直接在硬件上面呢,引入了一个交叉互联网络,也就是直接把数据拓展到4个SM

233
00:13:48,556 --> 00:13:50,924
SM之间呢,可以互相通讯

234
00:13:51,250 --> 00:13:58,250
于是在软件上或者CUDA上面呢,引用了一个新的概念,叫做TBC,也就是把4个SM聚合起来

235
00:13:58,250 --> 00:14:06,250
SM跟SM之间呢,可以高效访问他们互相之间的内存,所以这种呢,叫做分布式共享内存

236
00:14:06,250 --> 00:14:13,411
另外的话,你既然硬件有改变,所以软件有改变,软件呢,就提出了Warp group这种编程的模式

237
00:14:13,411 --> 00:14:17,411
对应的就引入了刚才说到的Flatbox Cluster这个概念

238
00:14:18,325 --> 00:14:22,014
更直观的从软件层面去看一下,有什么区别啊?

239
00:14:22,014 --> 00:14:22,325
左边这个呢,就是没有进行分布式共享内存的

240
00:14:22,325 --> 00:14:25,014
左边这个呢,就是没有进行分布式共享内存的

241
00:14:25,014 --> 00:14:30,014
每个Thread Block呢,就是对应的SM,里面可以共享自己的内存

242
00:14:30,014 --> 00:14:36,014
但是呢,SM跟SM之间呢,没有办法进行数据交互,于是呢,只能通过全局内存进行交互

243
00:14:36,014 --> 00:14:41,553
但是呢,在H100里面呢,引入了一个SM的Cluster,或者现成Block的Cluster

244
00:14:41,553 --> 00:14:44,773
通过硬件来实现分布式的共享内存

245
00:14:44,773 --> 00:14:47,773
SM跟SM之间的数据呢,可以互联

246
00:14:47,773 --> 00:14:50,923
不需要再次把数据呢,放在HBM里面再进行交互

247
00:14:50,923 --> 00:14:54,098
这样的话,可以减少寄存器的数量的利用

248
00:14:54,098 --> 00:14:56,376
还可以减少数据传输的时延

249
00:14:58,975 --> 00:15:03,544
现在来到最后一个内容了,就是Tensor Core的应用

250
00:15:03,544 --> 00:15:08,544
其实呢,在做Tensor Core,或者在H100里面呢,主要是针对大模型

251
00:15:08,544 --> 00:15:13,544
或者transformer的架构进行堆叠的这种像GPT,ChatGPT这种大模型啊

252
00:15:13,544 --> 00:15:16,620
但是呢,这些大模型输入的时候呢,有非常多的词汇

253
00:15:16,620 --> 00:15:20,012
会把词汇呢,embedded成具体的一些向量

254
00:15:20,012 --> 00:15:23,012
然后呢,输出的时候呢,还是以一个向量为主

255
00:15:23,012 --> 00:15:28,012
经过softmax就会输出一个比词表更大的一个向量

256
00:15:28,012 --> 00:15:34,687
那这个时候呢,词向量的表就会变得非常非常的大,或者矩阵变得非常的大

257
00:15:34,687 --> 00:15:38,508
在整个transformer计算的时候呢,也就变得非常大

258
00:15:38,508 --> 00:15:41,644
刚才谈到,其实Tensor Core它的数量是有限的

259
00:15:41,644 --> 00:15:50,936
在V100里面,它是4x4x4,但是呢,在软件上面呢,拓展到16x16x16,不断地从局部进行搬运

260
00:15:50,936 --> 00:15:58,488
那这个时候呢,其实不是说随随便便的就能够从软件上面去处理所有的embedded

261
00:15:58,488 --> 00:16:00,315
或者处理所有的大矩阵的

262
00:16:00,315 --> 00:16:02,900
更多呢,看一下刚才的那个例子

263
00:16:02,900 --> 00:16:08,000
针对大模型呢,input Size等于1024,然后batch Size是5120

264
00:16:08,000 --> 00:16:11,330
在v100,使用FP16进行训练

265
00:16:11,330 --> 00:16:16,194
然后呢,整个词汇表的大小是三万多

266
00:16:16,194 --> 00:16:20,111
transformer里面的Attention架构呢,里面就会有很多Mac漫画矩阵层

267
00:16:20,111 --> 00:16:26,111
如果pad到8的倍数,整体的性能呢,会比没有pad到8的倍数里面高很多

268
00:16:26,111 --> 00:16:33,111
这个时候呢,就要求软件编程的时候,其实也需要注意到硬件怎么样才能实现的更加高效

269
00:16:33,111 --> 00:16:38,374
那这个呢,叫做Padding Vocabulary Size,就对矩阵需要进行Padding的操作

270
00:16:40,000 --> 00:16:44,050
就到这里为止了,进行一个简单的总结

271
00:16:44,050 --> 00:16:48,050
在历代的Tensor Core,主要有三个提升点

272
00:16:48,050 --> 00:16:52,050
第一个呢,就是提升内存,打破整体的内存墙

273
00:16:52,050 --> 00:16:55,050
第二个呢,SM里面呢,提供更多的数据格式

274
00:16:55,050 --> 00:17:00,050
从帕斯卡的标准的FP16到TF32、IP8、IP4

275
00:17:00,050 --> 00:17:07,050
最后一个呢,就是对应的硬件改了,然后对应的软件编程呢,也会去修改

276
00:17:07,050 --> 00:17:10,050
预设呢,又有了新的CUDA的编程的模式

