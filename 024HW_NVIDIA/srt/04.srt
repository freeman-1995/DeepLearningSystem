1
00:00:00,000 --> 00:00:04,000
Subtitle：PlusV98

2
00:00:05,376 --> 00:00:08,722
大家好,我是三天打鱼,两天晒网的ZOMI

3
00:00:08,722 --> 00:00:10,762
这个视频确实拖更了很久

4
00:00:10,762 --> 00:00:15,362
现在回到了AI芯片的GPU详解这个系列里面

5
00:00:15,362 --> 00:00:20,162
在这个系列重点的去关注Tensor Core和NVLink

6
00:00:20,162 --> 00:00:21,984
两个主要的内容

7
00:00:21,984 --> 00:00:22,034
今天我想给大家去汇报一下

8
00:00:22,034 --> 00:00:24,984
今天我想给大家去汇报一下

9
00:00:24,984 --> 00:00:28,904
分布式训练跟NVLink的一个简单的发展

10
00:00:28,944 --> 00:00:31,344
以前回顾一下整体的大纲

11
00:00:31,344 --> 00:00:33,810
其实之前的大纲我想分开GPU

12
00:00:33,810 --> 00:00:35,538
英伟达的整体的架构

13
00:00:35,538 --> 00:00:39,418
Tensor Core和NVLink本来想用一小节去结束的

14
00:00:39,418 --> 00:00:44,098
后来就发现NVLink跟Tensor Core里面有非常多的内容

15
00:00:44,098 --> 00:00:47,018
所以就单独的GPU详解单独的提取出来

16
00:00:47,018 --> 00:00:49,498
Tensor Core就分享了三节

17
00:00:49,498 --> 00:00:53,418
而NVLink跟NVSwitch也会分开三节来看

18
00:00:53,418 --> 00:00:57,258
最后回顾一下GPU的图形处理这一块

19
00:00:57,258 --> 00:01:01,018
就不在AI系列里面就不详细的去展开

20
00:01:01,018 --> 00:01:04,098
今天给大家汇报的内容主要分为两个

21
00:01:04,098 --> 00:01:06,338
第一个就是分布式训练

22
00:01:06,338 --> 00:01:08,698
看一下分布式训练到底是什么

23
00:01:08,698 --> 00:01:12,814
简单的回顾之前给大家汇报的内容

24
00:01:12,814 --> 00:01:15,454
接着看一下今天的主角NVLink

25
00:01:15,454 --> 00:01:17,414
NVSwitch的简单的发展

26
00:01:17,414 --> 00:01:21,294
后面分开两个小节给大家详细的汇报的

27
00:01:21,294 --> 00:01:23,774
第一个就是分布式训练

28
00:01:23,774 --> 00:01:26,174
其实ZOMI在之前的课程里面

29
00:01:26,174 --> 00:01:28,854
给大家汇报过什么是AI集群

30
00:01:28,854 --> 00:01:30,734
当模型越来越大的时候

31
00:01:30,734 --> 00:01:32,894
就需要分布式的去训练

32
00:01:32,894 --> 00:01:36,054
那分布式训练就涉及到PS的架构

33
00:01:36,054 --> 00:01:37,614
AI的软硬件通讯

34
00:01:37,614 --> 00:01:38,814
还有集合通讯

35
00:01:38,814 --> 00:01:42,094
当然AI框架也需要具备分布式的能力

36
00:01:42,094 --> 00:01:45,134
接着去看一下分布式的大模型

37
00:01:45,134 --> 00:01:46,894
有哪些不一样的算法

38
00:01:46,894 --> 00:01:49,974
这里面主要是指常用的数据并行

39
00:01:49,974 --> 00:01:50,654
张量并行

40
00:01:50,654 --> 00:01:51,534
流水线并行

41
00:01:51,534 --> 00:01:52,774
多维缓和并行

42
00:01:52,774 --> 00:01:55,014
各种各样的分布式并行的策略

43
00:01:55,014 --> 00:01:57,094
也欢迎大家去了解

44
00:01:58,015 --> 00:02:01,294
那现在回顾一下整个分布式训练

45
00:02:01,294 --> 00:02:03,734
首先 整个人工智能的发展

46
00:02:03,814 --> 00:02:06,454
这个图给大家讲过很多遍了

47
00:02:06,454 --> 00:02:08,694
现在从深度学习迎来了

48
00:02:08,694 --> 00:02:09,614
Foundation Model

49
00:02:09,614 --> 00:02:11,174
也就是大模型

50
00:02:11,174 --> 00:02:13,174
现在ZOMI负责的事情

51
00:02:13,174 --> 00:02:15,614
看一下右边的图表

52
00:02:15,614 --> 00:02:17,294
可以看到红色的这条线

53
00:02:17,454 --> 00:02:19,974
就是真正的大模型来到的时代

54
00:02:20,014 --> 00:02:22,134
而所谓的大模型主要是指

55
00:02:22,134 --> 00:02:23,974
模型的参数量

56
00:02:23,974 --> 00:02:25,814
进一步的急剧的膨胀

57
00:02:25,814 --> 00:02:28,574
那大模型给带来的好处有三个

58
00:02:28,574 --> 00:02:31,814
第一个就是引入了自监督的学习的方法

59
00:02:31,814 --> 00:02:32,814
模型大了

60
00:02:32,814 --> 00:02:33,614
数据大了

61
00:02:33,614 --> 00:02:35,854
没办法所有东西都要人工的去标注

62
00:02:35,854 --> 00:02:38,054
第二点是模型的参数量

63
00:02:38,054 --> 00:02:39,734
参数规模越来越大

64
00:02:39,734 --> 00:02:42,854
但是模型的精度却是进一步的去提升

65
00:02:42,854 --> 00:02:44,814
而且提升的非常的惊人

66
00:02:44,814 --> 00:02:48,014
第三个就是解决了模型碎片化的问题

67
00:02:48,014 --> 00:02:51,694
通过提出了预训练到后来的GPT-3

68
00:02:51,694 --> 00:02:54,894
GPT-4这种CircleSwap的能力越来越强

69
00:02:54,894 --> 00:02:58,541
所以大模型确实给AIGC带来了

70
00:02:58,541 --> 00:03:00,704
非常多的想象力

71
00:03:00,704 --> 00:03:02,941
那可以看到模型越来越大

72
00:03:02,941 --> 00:03:06,221
没办法把一个单独的模型放在一款芯片

73
00:03:06,221 --> 00:03:08,501
或者一款GPU NPU里面

74
00:03:08,501 --> 00:03:12,061
于是就出现了各种各样的并行的策略

75
00:03:12,061 --> 00:03:14,061
那可以在整体上来说

76
00:03:14,061 --> 00:03:15,621
不管是哪种并行

77
00:03:15,621 --> 00:03:18,061
它主要是对模型进行切分

78
00:03:18,061 --> 00:03:20,021
那有横着切有竖着切

79
00:03:20,021 --> 00:03:22,301
那横着切就是把模型的层数

80
00:03:22,301 --> 00:03:24,301
单独的切出来放在不同的机器

81
00:03:24,301 --> 00:03:28,101
竖着切就是指把模型像三个并行的这种

82
00:03:28,101 --> 00:03:32,501
就是竖着切把张量切出来放在不同的芯片上面

83
00:03:32,501 --> 00:03:33,901
或者不同的GPU上面

84
00:03:33,901 --> 00:03:36,330
有了这些基础的并行算法之后

85
00:03:36,330 --> 00:03:40,890
整个系统确实要支持这种计算节点的跨机

86
00:03:40,890 --> 00:03:42,730
原来的只是一个网络模型

87
00:03:42,730 --> 00:03:45,090
现在要把这一个网络模型

88
00:03:45,090 --> 00:03:46,610
分布在不同的机器

89
00:03:46,610 --> 00:03:50,410
这里面就涉及到整体的系统怎么去通讯

90
00:03:50,410 --> 00:03:53,930
那刚才看到刚才有两个节点是互相通讯的

91
00:03:53,930 --> 00:03:55,570
现在节点越来越多

92
00:03:55,570 --> 00:03:57,090
分开不同的权重

93
00:03:57,090 --> 00:04:00,050
所以参数量也放在不同的模型里面

94
00:04:00,050 --> 00:04:02,410
不同的去进行一个交互

95
00:04:02,410 --> 00:04:04,890
那这种方式就是计算图

96
00:04:04,890 --> 00:04:09,570
整个计算图跨节点同步数据进行交互

97
00:04:09,570 --> 00:04:11,490
这种就是分布式训练

98
00:04:11,490 --> 00:04:12,970
那既然谈到分布式训练

99
00:04:12,970 --> 00:04:13,930
刚才只是给

100
00:04:13,930 --> 00:04:16,290
大家汇报了软件层面

101
00:04:16,290 --> 00:04:19,290
做了一些相关的策略和算法

102
00:04:19,290 --> 00:04:21,130
接着看一下硬件层面

103
00:04:21,130 --> 00:04:24,210
在通讯硬件需要有哪些不一样的东西

104
00:04:24,210 --> 00:04:26,610
首先是机器类的通讯

105
00:04:26,610 --> 00:04:27,650
简单的来说

106
00:04:27,810 --> 00:04:30,370
就是一款机器里面有多款

107
00:04:30,370 --> 00:04:32,772
GPU或者AI加速芯片这种

108
00:04:32,772 --> 00:04:34,492
就是机器内的通讯

109
00:04:34,492 --> 00:04:36,332
而机器间的通讯

110
00:04:36,532 --> 00:04:39,052
主要是指不同的机器之间

111
00:04:39,052 --> 00:04:41,492
不同的服务器主机之间

112
00:04:41,772 --> 00:04:43,172
如何进行通讯

113
00:04:43,372 --> 00:04:44,692
在机器间的通讯

114
00:04:44,852 --> 00:04:47,092
可以通过共享内存的方式

115
00:04:47,092 --> 00:04:48,412
也可以通过PCIE

116
00:04:48,412 --> 00:04:50,452
当然也有今天的主角

117
00:04:50,452 --> 00:04:52,252
NVLink直连的模式

118
00:04:52,252 --> 00:04:56,052
把GPU跟GPU之间直接互相连起来

119
00:04:56,052 --> 00:04:56,932
另外的话

120
00:04:56,932 --> 00:04:58,172
机器之间通讯

121
00:04:58,372 --> 00:04:59,372
就上面

122
00:04:59,372 --> 00:05:00,732
这是一排机柜

123
00:05:00,892 --> 00:05:03,572
机柜之间可能有一台华为的阿特拉斯

124
00:05:03,572 --> 00:05:05,892
下面也有一台华为的阿特拉斯

125
00:05:05,892 --> 00:05:08,492
一个机柜可以在8台服务器

126
00:05:08,772 --> 00:05:09,892
机器间的通信

127
00:05:10,012 --> 00:05:11,292
有TCPIP

128
00:05:11,292 --> 00:05:12,412
还有RDMA

129
00:05:12,612 --> 00:05:15,092
现在经常谈到的IB网络

130
00:05:15,092 --> 00:05:17,372
是RDMA的其中一种

131
00:05:17,372 --> 00:05:20,892
Rocky也是RDMA网络的其中一种

132
00:05:21,370 --> 00:05:22,850
了解到硬件之后

133
00:05:23,050 --> 00:05:24,570
硬件要实现通讯

134
00:05:24,770 --> 00:05:26,370
这里面就离不开

135
00:05:26,370 --> 00:05:29,250
提供集合通讯的一些库了

136
00:05:29,410 --> 00:05:30,450
集合通讯一些库

137
00:05:30,610 --> 00:05:32,650
最常用的是MPI

138
00:05:32,650 --> 00:05:35,292
在CPU上面用的非常多

139
00:05:35,292 --> 00:05:37,532
而在英伟达的GPU上面

140
00:05:37,652 --> 00:05:40,212
用的最多的就是NCCL

141
00:05:40,212 --> 00:05:41,732
N就是英伟达

142
00:05:41,732 --> 00:05:44,012
华为自己就推出了HCCL

143
00:05:44,212 --> 00:05:46,612
Communication-Connected-Layer

144
00:05:46,612 --> 00:05:47,974
通过NCCL这个库

145
00:05:48,134 --> 00:05:50,254
英伟达就使能了NVSwitch

146
00:05:50,254 --> 00:05:51,334
或者NVLink

147
00:05:51,334 --> 00:05:53,454
把不同的GPU跟GPU之间

148
00:05:53,454 --> 00:05:54,374
互联起来

149
00:05:54,374 --> 00:05:56,534
而是通过算法层面

150
00:05:56,534 --> 00:05:58,894
对外提供对应的API

151
00:05:58,894 --> 00:06:00,014
而这里面的API

152
00:06:00,014 --> 00:06:01,694
就涉及到集合通讯

153
00:06:01,694 --> 00:06:03,374
有多对一的Gather

154
00:06:03,374 --> 00:06:05,734
有一对多的Scatter和Broadcast

155
00:06:05,734 --> 00:06:07,414
当然还有多对多的

156
00:06:07,414 --> 00:06:08,334
All Reduce Sum

157
00:06:08,334 --> 00:06:09,414
All Gather等

158
00:06:09,414 --> 00:06:11,774
各种各样的通讯集合的API

159
00:06:11,774 --> 00:06:14,174
或者通讯集合的语言

160
00:06:15,240 --> 00:06:17,520
现在来到了第2个内容

161
00:06:17,520 --> 00:06:20,640
NVLink跟NVSwitch的发展

162
00:06:20,680 --> 00:06:22,760
在正式进入内容之前

163
00:06:22,920 --> 00:06:25,960
我想给大家去看一个简单的视频的

164
00:06:25,960 --> 00:06:27,280
首先在看视频之前

165
00:06:27,400 --> 00:06:28,480
我还想介绍一下

166
00:06:28,480 --> 00:06:29,600
什么是NVLink

167
00:06:29,600 --> 00:06:31,240
什么是NVSwitch

168
00:06:31,600 --> 00:06:33,080
首先可以了解一下

169
00:06:33,080 --> 00:06:34,240
NVLink其实是一种

170
00:06:34,240 --> 00:06:36,320
总线或者通讯的协议

171
00:06:36,320 --> 00:06:38,774
在CPU跟GPU之间互联

172
00:06:38,774 --> 00:06:41,854
也可以在GPU跟GPU之间互联

173
00:06:41,854 --> 00:06:43,694
现在用的最多的是

174
00:06:43,694 --> 00:06:45,294
GPU跟GPU之间互联

175
00:06:45,294 --> 00:06:47,174
而GPU跟CPU之间互联

176
00:06:47,374 --> 00:06:50,134
现在最新一代的是H100

177
00:06:50,134 --> 00:06:53,094
还有之前的IBM的Power系列

178
00:06:53,094 --> 00:06:55,894
NVSwitch就是一种高速的互联技术

179
00:06:56,134 --> 00:06:59,374
主要是作为独立的NVLink的芯片

180
00:06:59,374 --> 00:07:02,294
提供了非常多路的NVLink的接口

181
00:07:03,200 --> 00:07:05,360
NVSwitch大家可以理解为

182
00:07:05,360 --> 00:07:07,280
具体模块的芯片

183
00:07:07,280 --> 00:07:08,760
而NVLink就是一种

184
00:07:08,760 --> 00:07:10,680
总线和通讯的协议

185
00:07:11,000 --> 00:07:13,200
现在看一个具体的视频

186
00:07:13,200 --> 00:07:14,200
来看看

187
00:07:14,200 --> 00:07:15,200
到底NVLink

188
00:07:15,200 --> 00:07:17,400
NVSwitch的具体的差别

189
00:07:34,440 --> 00:07:35,440
NVSwitch提供了

190
00:07:35,440 --> 00:07:37,560
7倍的PCIe Gen5的速度

191
00:07:37,560 --> 00:07:39,880
能提供更快的总体表现

192
00:07:40,320 --> 00:07:42,080
NVIDIA提供的这种表现

193
00:07:42,080 --> 00:07:45,880
是NVLink和NVIDIA NVSwitch

194
00:07:45,880 --> 00:07:47,440
的两个互联的能力

195
00:07:47,440 --> 00:07:49,720
来提供多个GPU的通讯

196
00:07:50,000 --> 00:07:51,720
NVSwitch支援

197
00:07:51,720 --> 00:07:53,880
16个GPU在一个伺服器上

198
00:07:53,880 --> 00:07:56,520
这里展示8个GPU在一个伺服器上

199
00:07:56,520 --> 00:07:58,160
任何4个GPU的双方

200
00:07:58,160 --> 00:08:00,000
可以同时通讯

201
00:08:00,000 --> 00:08:01,920
在900Gbps的情况下

202
00:08:01,960 --> 00:08:03,160
提供了一个不可思议的

203
00:08:03,160 --> 00:08:05,160
3.6Tbps的

204
00:08:05,160 --> 00:08:07,680
统一通讯在一个伺服器上

205
00:08:07,680 --> 00:08:10,080
NVSwitch还能迅速地

206
00:08:10,080 --> 00:08:11,560
通讯

207
00:08:11,560 --> 00:08:12,680
通过多层架构

208
00:08:12,680 --> 00:08:14,760
和鲜明的缩减在网络上

209
00:08:15,120 --> 00:08:16,560
NVLink延伸

210
00:08:16,560 --> 00:08:19,440
同样的高速GPU-GPU连接

211
00:08:19,440 --> 00:08:20,440
在伺服器上

212
00:08:20,440 --> 00:08:22,440
来连接伺服器的设备

213
00:08:22,840 --> 00:08:24,080
NVLink的连接器

214
00:08:24,080 --> 00:08:25,800
可以直接连接

215
00:08:25,800 --> 00:08:28,360
NVIDIA H100 Tensor Core GPU

216
00:08:28,360 --> 00:08:29,240
在每个伺服器上

217
00:08:29,240 --> 00:08:31,160
与高速NVLink连接

218
00:08:31,640 --> 00:08:35,080
直到32个NVIDIA DGX-H100伺服器

219
00:08:35,080 --> 00:08:37,680
每个都有8个H100的GPU

220
00:08:37,680 --> 00:08:39,400
都可以连接

221
00:08:39,400 --> 00:08:40,760
每个GPU

222
00:08:40,760 --> 00:08:42,600
都由NVLink连接

223
00:08:42,600 --> 00:08:44,440
到每个其他GPU

224
00:08:44,440 --> 00:08:47,080
每个18个NVLink的伺服器

225
00:08:47,080 --> 00:08:48,320
都连接到每个

226
00:08:48,320 --> 00:08:50,680
32个H100的伺服器

227
00:08:51,160 --> 00:08:52,240
这种架构

228
00:08:52,240 --> 00:08:55,400
组成了256个GPU

229
00:08:55,400 --> 00:08:57,520
连接伺服器的团体

230
00:08:57,520 --> 00:09:00,600
提供了57.6Tbps

231
00:09:00,640 --> 00:09:02,200
全线连接的快速连接

232
00:09:02,720 --> 00:09:05,240
NVLink的连接系统

233
00:09:05,240 --> 00:09:06,960
MagnumIO的设备

234
00:09:06,960 --> 00:09:09,040
与伺服器的专业程度

235
00:09:09,040 --> 00:09:12,240
提供了上至9次的AI训练连接

236
00:09:12,240 --> 00:09:14,360
与专业模式的混合

237
00:09:14,360 --> 00:09:15,920
这让研究员

238
00:09:15,920 --> 00:09:17,480
在业务的速度下

239
00:09:17,480 --> 00:09:19,360
连接大量的模式

240
00:09:24,240 --> 00:09:24,920
好嘞

241
00:09:24,920 --> 00:09:26,240
通过刚才的视频

242
00:09:26,240 --> 00:09:27,320
了解到了

243
00:09:27,320 --> 00:09:29,997
NVLink跟NVSwitch的关系

244
00:09:29,997 --> 00:09:31,237
还有NVLink、NVSwitch

245
00:09:31,237 --> 00:09:32,157
跟A100

246
00:09:32,157 --> 00:09:33,757
或者DGX

247
00:09:33,757 --> 00:09:35,397
服务器主机

248
00:09:35,397 --> 00:09:36,437
跟服务器柜

249
00:09:36,437 --> 00:09:37,837
之间的一个关系了

250
00:09:37,837 --> 00:09:38,957
现在来看一下

251
00:09:38,957 --> 00:09:40,637
NVLink的整体的发展

252
00:09:40,637 --> 00:09:42,997
首先NVLink到现在为止

253
00:09:43,117 --> 00:09:44,757
也就是2023年年初

254
00:09:44,757 --> 00:09:46,437
已经经历了4代

255
00:09:46,437 --> 00:09:47,237
从第一代

256
00:09:47,237 --> 00:09:47,597
第二代

257
00:09:47,597 --> 00:09:48,077
第三代

258
00:09:48,077 --> 00:09:48,957
第四代

259
00:09:48,957 --> 00:09:51,477
每一代的NVLink的带宽

260
00:09:51,477 --> 00:09:53,797
也是不断的去提升的

261
00:09:53,797 --> 00:09:55,317
而NVLink之间

262
00:09:55,477 --> 00:09:57,077
能够互联的GPU

263
00:09:57,077 --> 00:09:58,997
从简单的4路

264
00:09:58,997 --> 00:10:01,157
到现在的18路

265
00:10:01,157 --> 00:10:01,997
整体的架构

266
00:10:01,997 --> 00:10:03,557
简单的对应一下

267
00:10:03,557 --> 00:10:05,277
第一代是

268
00:10:05,277 --> 00:10:07,557
Pascal架构

269
00:10:07,557 --> 00:10:09,157
第二代是Volta架构

270
00:10:09,157 --> 00:10:11,277
也就是V100用的非常的多

271
00:10:11,277 --> 00:10:12,477
到现在来说

272
00:10:12,477 --> 00:10:13,237
大量的供货

273
00:10:13,397 --> 00:10:15,557
应该是第三代的

274
00:10:15,557 --> 00:10:18,157
NVLink的GPU安培架构

275
00:10:18,157 --> 00:10:20,117
可能到现在为止

276
00:10:20,117 --> 00:10:23,077
暂时买不到的是Hopper架构

277
00:10:23,077 --> 00:10:24,957
那Hopper架构的NVLink

278
00:10:24,957 --> 00:10:27,877
因为带宽非常的夸张惊人

279
00:10:27,877 --> 00:10:30,597
所以这一代也是被国外所禁止

280
00:10:30,597 --> 00:10:32,037
进口到中国

281
00:10:32,037 --> 00:10:33,397
现在看一下

282
00:10:33,397 --> 00:10:35,037
NVLink每一代的发展

283
00:10:35,197 --> 00:10:36,757
其实从2015年

284
00:10:36,757 --> 00:10:38,717
刚才提到的Pascal架构

285
00:10:38,717 --> 00:10:41,917
到2022年的Hopper架构的推出

286
00:10:41,917 --> 00:10:43,677
整体的GPU的性能

287
00:10:43,917 --> 00:10:46,237
是不断的翻倍的增长

288
00:10:46,237 --> 00:10:48,157
到现在最夸张的是

289
00:10:48,157 --> 00:10:51,997
900GB每秒的数据的传输速率

290
00:10:52,277 --> 00:10:54,637
回顾一下整体的NVLink的架构

291
00:10:54,797 --> 00:10:56,477
刚才讲到的

292
00:10:56,477 --> 00:10:57,997
GPU跟GPU之间互联

293
00:10:58,197 --> 00:11:00,557
在一开始2016年的时候

294
00:11:00,557 --> 00:11:03,917
Pascal架构只有4路数据进行互联

295
00:11:03,917 --> 00:11:05,597
到现在的Hopper架构

296
00:11:05,757 --> 00:11:07,317
它有18路的数据

297
00:11:07,317 --> 00:11:10,037
进行一个GPU跟GPU之间的互联

298
00:11:10,037 --> 00:11:10,997
整体来说

299
00:11:11,237 --> 00:11:12,877
每一条NVLink的链路

300
00:11:13,037 --> 00:11:16,397
在第一代其实只有40GB每秒

301
00:11:16,397 --> 00:11:20,807
所以可以看到 NVLink里面每路的数据

302
00:11:20,807 --> 00:11:23,487
基本上到后面都是50GB

303
00:11:23,487 --> 00:11:25,607
每秒每一条链路

304
00:11:25,607 --> 00:11:28,447
但是随着链路数整体的增加了

305
00:11:28,447 --> 00:11:31,647
NVLink之间的带宽是不断的增长的

306
00:11:31,647 --> 00:11:35,047
现在来看一看NVSwitch

307
00:11:35,047 --> 00:11:37,327
NVSwitch其实只有三代

308
00:11:37,607 --> 00:11:39,247
从第一代的First Generation

309
00:11:39,407 --> 00:11:41,567
其实是在Volta架构

310
00:11:41,567 --> 00:11:43,367
也就伏特架构开始

311
00:11:43,367 --> 00:11:44,567
到Ampere架构

312
00:11:44,567 --> 00:11:45,447
Hopper架构

313
00:11:45,447 --> 00:11:46,807
每一款架构

314
00:11:46,927 --> 00:11:48,567
演进新的一代

315
00:11:48,567 --> 00:11:50,647
所以P系列的Pascal架构

316
00:11:50,647 --> 00:11:53,527
它其实是没有出现NVSwitch的

317
00:11:53,527 --> 00:11:54,647
它只有NVLink

318
00:11:54,647 --> 00:11:55,927
NVLink之间

319
00:11:55,927 --> 00:11:58,247
进行一个CubeMesh的互联

320
00:11:58,247 --> 00:11:59,727
现在来看一下

321
00:11:59,727 --> 00:12:02,127
这几代NVSwitch有什么不一样

322
00:12:02,447 --> 00:12:04,607
首先GPU之间互联的节点

323
00:12:04,767 --> 00:12:06,447
也就是第一行

324
00:12:06,447 --> 00:12:09,007
基本上每一代最大的是8

325
00:12:09,007 --> 00:12:10,327
但是NVSwitch

326
00:12:10,327 --> 00:12:11,727
GPU跟GPU之间的互联

327
00:12:11,727 --> 00:12:12,727
刚才讲到了

328
00:12:12,727 --> 00:12:15,127
其实它跟NVLink是相关的

329
00:12:15,127 --> 00:12:16,327
因为NVSwitch

330
00:12:16,487 --> 00:12:17,567
就是NVLink

331
00:12:17,567 --> 00:12:19,887
具体的承载的芯片模组

332
00:12:19,887 --> 00:12:21,567
里面从300GB每秒

333
00:12:21,567 --> 00:12:23,167
到600GB每秒

334
00:12:23,207 --> 00:12:24,927
到现在的900GB每秒

335
00:12:24,927 --> 00:12:27,847
当然了国内进口限制在400GB每秒

336
00:12:27,847 --> 00:12:29,207
所以大部分时候

337
00:12:29,887 --> 00:12:32,567
只能买到H800

338
00:12:32,567 --> 00:12:34,167
或者A800

339
00:12:34,167 --> 00:12:36,328
往下 看一下

340
00:12:36,328 --> 00:12:38,968
整个NVSwitch的一个关系

341
00:12:39,168 --> 00:12:41,168
一开始从Pascal架构

342
00:12:41,328 --> 00:12:43,008
其实在DGX1里面

343
00:12:43,248 --> 00:12:44,688
只有一个CubeMesh

344
00:12:44,688 --> 00:12:47,528
也就是这里面是没有NVSwitch的

345
00:12:47,528 --> 00:12:48,408
只有NVLink

346
00:12:48,568 --> 00:12:51,488
NVLink每一款GPU里面有4路

347
00:12:51,488 --> 00:12:54,128
所以这里面有1234

348
00:12:54,128 --> 00:12:57,208
只能跟4条链路进行互联

349
00:12:57,208 --> 00:12:59,488
同样的这里面这一台P100

350
00:12:59,808 --> 00:13:02,968
距1234上下左右

351
00:13:02,968 --> 00:13:04,968
这几个只有连接4条

352
00:13:04,968 --> 00:13:07,808
里面就各自组成一个CubeMesh

353
00:13:07,808 --> 00:13:09,968
但是在Volta架构

354
00:13:09,968 --> 00:13:11,568
DGX2里面

355
00:13:11,568 --> 00:13:13,288
应该是2018年推出的

356
00:13:13,288 --> 00:13:14,448
每一款V100

357
00:13:14,608 --> 00:13:16,568
就可以跟NVSwitch里面

358
00:13:16,568 --> 00:13:18,168
另外一款V100

359
00:13:18,328 --> 00:13:20,048
进行一个互联

360
00:13:20,088 --> 00:13:21,808
到了2022年

361
00:13:21,808 --> 00:13:23,448
也就是A100

362
00:13:23,448 --> 00:13:25,128
Ampere架构的出现的时候

363
00:13:25,368 --> 00:13:26,928
NVSwitch的模组

364
00:13:27,088 --> 00:13:28,648
经过了一个更新

365
00:13:28,648 --> 00:13:29,528
每一款A100

366
00:13:29,888 --> 00:13:31,688
可以跟任何一款A100

367
00:13:31,688 --> 00:13:32,928
进行一个互联的

368
00:13:32,928 --> 00:13:34,488
这个时候对于硬件来说

369
00:13:34,648 --> 00:13:36,568
是节省了非常多的链路

370
00:13:36,568 --> 00:13:38,728
而技术也是进一步的提升的

371
00:13:38,728 --> 00:13:41,008
到最新的H100里面

372
00:13:41,008 --> 00:13:43,768
又有了一个新的技术的突破

373
00:13:45,836 --> 00:13:47,608
最后 简单的总结一下

374
00:13:47,608 --> 00:13:49,600
刚才讲过的几个概念

375
00:13:49,600 --> 00:13:53,583
首先NCCL是一个集合的通讯库

376
00:13:53,583 --> 00:13:55,383
能够实现集合的通讯

377
00:13:55,383 --> 00:13:56,503
和点对点的通讯

378
00:13:56,503 --> 00:13:57,303
另外的话

379
00:13:57,303 --> 00:13:59,823
NVLink它是一个总线的

380
00:13:59,823 --> 00:14:01,063
或者通讯的协议

381
00:14:01,063 --> 00:14:03,983
NVSwitch是独立的NVLink的芯片

382
00:14:03,983 --> 00:14:06,143
而HGX或者DGX

383
00:14:06,343 --> 00:14:09,423
是一个AI的超级计算的平台

384
00:14:09,423 --> 00:14:11,063
也就是简单的一台主机

385
00:14:11,063 --> 00:14:13,783
一台主机里面挂了很多GPU

386
00:14:13,783 --> 00:14:15,903
和多个NVSwitch


