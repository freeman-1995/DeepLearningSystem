1
00:00:01,572 --> 00:00:03,572
字幕组：赵含霖 谢鑫鑫

2
00:00:05,880 --> 00:00:07,600
Hello，大家好，我是ZOMI酱

3
00:00:07,600 --> 00:00:09,600
欢迎来到ZOMI的课堂

4
00:00:09,600 --> 00:00:12,960
那这一节可能是我给大家分享的

5
00:00:12,960 --> 00:00:15,720
所有的知识里面最复杂最深奥的

6
00:00:15,720 --> 00:00:19,880
所以我尽可能希望能够讲得明白简单一点

7
00:00:25,760 --> 00:00:28,840
这里面要讲讲自动微分的模式

8
00:00:29,200 --> 00:00:30,280
什么？

9
00:00:30,280 --> 00:00:32,360
自动微分还分不同模式

10
00:00:32,360 --> 00:00:34,000
不就搞个微分吗

11
00:00:34,000 --> 00:00:35,840
搞那么多干嘛

12
00:00:35,840 --> 00:00:37,040
哎 不要急

13
00:00:37,040 --> 00:00:38,440
来看看自动微分

14
00:00:38,440 --> 00:00:39,880
实际上分两种模式

15
00:00:39,880 --> 00:00:41,720
只有两种没有太复杂

16
00:00:41,720 --> 00:00:43,320
一种叫做前向微分

17
00:00:43,320 --> 00:00:45,240
一种叫做后向微分

18
00:00:45,240 --> 00:00:48,320
前向很明显就是从前往后算

19
00:00:48,320 --> 00:00:50,440
后向微分比较直接了

20
00:00:50,440 --> 00:00:52,160
就是从后往前算

21
00:00:52,160 --> 00:00:55,680
虽然前向微分在英文里面叫做Forward Model

22
00:00:55,680 --> 00:00:58,000
后向叫做Backward Model

23
00:00:58,000 --> 00:01:00,640
为了方便数学表示

24
00:01:00,640 --> 00:01:02,800
所以引入了雅可比矩阵

25
00:01:02,800 --> 00:01:06,480
通过雅可比矩阵去表示前向微分

26
00:01:06,480 --> 00:01:09,120
或者后向微分的一个数学表现

27
00:01:13,320 --> 00:01:18,000
现在继续回到上一节里面去给大家分享的

28
00:01:18,000 --> 00:01:19,720
什么是自动微分

29
00:01:19,720 --> 00:01:23,560
自动微分上一节其实没有太多的去展开去讲

30
00:01:23,560 --> 00:01:26,080
只是给大家介绍了一个概念

31
00:01:26,120 --> 00:01:30,440
它是由很多有限的基本运算组成的

32
00:01:30,440 --> 00:01:32,640
也就是加减乘除

33
00:01:32,640 --> 00:01:35,360
通过这么简单的一些自动微分的方式

34
00:01:35,360 --> 00:01:38,200
或者链式求导法则记录下来

35
00:01:38,200 --> 00:01:40,080
然后进行组合

36
00:01:40,080 --> 00:01:45,080
这个公式f(x1,x2)就会贯穿一整个系列的

37
00:01:45,080 --> 00:01:46,760
那这里面很简单

38
00:01:46,760 --> 00:01:51,560
它是ln(x1)+x1*x2-sin(x2)

39
00:01:51,560 --> 00:01:55,120
正向去计算Forward Primer Trace

40
00:01:55,120 --> 00:01:59,080
是表示我对这个公式进行正向的计算

41
00:01:59,080 --> 00:02:01,920
跟正向自动微分没有半点关系

42
00:02:01,920 --> 00:02:04,600
看一下现在我有两个输入

43
00:02:04,600 --> 00:02:05,520
一个输入是x1

44
00:02:05,520 --> 00:02:06,760
一个输入是x2

45
00:02:06,760 --> 00:02:08,000
x1=2

46
00:02:08,000 --> 00:02:09,480
x2=5

47
00:02:09,480 --> 00:02:12,680
然后带进这个公式不断的往下计算

48
00:02:12,680 --> 00:02:14,760
可以看到先算ln2

49
00:02:14,760 --> 00:02:15,880
然后算2x5

50
00:02:15,880 --> 00:02:17,280
再算sin(5)

51
00:02:17,280 --> 00:02:18,600
然后再算加号

52
00:02:18,600 --> 00:02:20,200
再算减号

53
00:02:20,200 --> 00:02:24,320
最后得到我的f(x1,x2)是我的y

54
00:02:24,360 --> 00:02:26,040
等于11.652

55
00:02:26,040 --> 00:02:27,960
这个就是正向的一个计算

56
00:02:30,320 --> 00:02:31,840
有了正向的计算

57
00:02:31,840 --> 00:02:36,240
希望能够把它变成一个更加简单的方式

58
00:02:36,240 --> 00:02:37,920
或者变成一个图

59
00:02:37,920 --> 00:02:41,440
那图在计算机里面是很容易去实现的

60
00:02:41,440 --> 00:02:44,000
叫做DAG图

61
00:02:44,000 --> 00:02:46,160
有向无环图

62
00:02:46,160 --> 00:02:49,280
因为这个图从x1、x2输入

63
00:02:49,280 --> 00:02:52,280
然后到f(x1,x2)输出

64
00:02:52,280 --> 00:02:53,880
都是没有回环的

65
00:02:53,880 --> 00:02:55,680
也就是不会往后走

66
00:02:55,680 --> 00:02:56,920
返回前面的

67
00:02:56,920 --> 00:02:58,640
所以叫做DAG图

68
00:02:58,640 --> 00:03:01,720
那每一个节点就是一个中间结果

69
00:03:01,720 --> 00:03:03,760
或者叫做中间变量

70
00:03:03,760 --> 00:03:06,400
而边就是我的每一个计算

71
00:03:06,400 --> 00:03:07,760
包括我的加减乘除

72
00:03:07,760 --> 00:03:09,360
求根号、sin

73
00:03:09,360 --> 00:03:11,800
不同的计算方式

74
00:03:11,800 --> 00:03:13,240
代表我的边

75
00:03:13,240 --> 00:03:15,480
现在假设要对这个公式

76
00:03:15,480 --> 00:03:17,600
对x1进行求导

77
00:03:17,600 --> 00:03:20,760
那这个时候根据链式求导法则

78
00:03:20,760 --> 00:03:23,840
就展开变成这一条公式了

79
00:03:23,920 --> 00:03:25,920
公式还是这一条公式

80
00:03:25,920 --> 00:03:29,960
不过现在正式的进入到第一个内容

81
00:03:29,960 --> 00:03:33,000
就是正向自动微分

82
00:03:33,000 --> 00:03:35,600
叫做Forward或者Tangent model

83
00:03:35,600 --> 00:03:37,600
现在有两个概念

84
00:03:37,600 --> 00:03:39,720
第一个就是求vi的偏导

85
00:03:39,720 --> 00:03:42,560
第二个就是求yj的偏导

86
00:03:42,560 --> 00:03:46,640
那vi的偏导代表我对x1进行求导

87
00:03:46,640 --> 00:03:50,240
yj就代表我的y对x1进行求导

88
00:03:50,240 --> 00:03:52,680
可以看到现在左边的这一列

89
00:03:52,720 --> 00:03:54,200
都是加了一点的

90
00:03:54,200 --> 00:03:57,800
代表我左边的是对每一次小的操作

91
00:03:57,800 --> 00:03:59,280
进行求导

92
00:03:59,280 --> 00:04:00,840
正向的时候很简单

93
00:04:00,840 --> 00:04:03,800
首先我现在是对x1进行求导

94
00:04:03,800 --> 00:04:06,480
所有的东西都是对x1没有对x2

95
00:04:06,480 --> 00:04:09,360
对x1求导的时候我还是1

96
00:04:09,360 --> 00:04:12,080
x2这个时候已经变成常量了

97
00:04:12,080 --> 00:04:14,520
对常量进行求导是我的0

98
00:04:14,520 --> 00:04:18,240
这个就是最简单的ln(x1)的一个导数

99
00:04:18,240 --> 00:04:21,280
下面就是x1乘以x2的导数

100
00:04:21,280 --> 00:04:23,760
每一次小的展开

101
00:04:23,760 --> 00:04:26,840
都是一个最简单最原始的计算

102
00:04:26,840 --> 00:04:31,680
最后就得到了我的y对x1的导数是5.5

103
00:04:31,680 --> 00:04:34,760
现在一个正向只是算了一遍

104
00:04:34,760 --> 00:04:37,600
算了我对x1的导数

105
00:04:37,600 --> 00:04:39,480
还没有算对x2的导数

106
00:04:42,000 --> 00:04:46,920
那刚才讲了简单的正向的求导方式

107
00:04:46,920 --> 00:04:49,680
下面来讲讲反向怎么求

108
00:04:49,680 --> 00:04:50,960
正向很简单的

109
00:04:50,960 --> 00:04:52,320
图还是那个图

110
00:04:52,320 --> 00:04:57,480
现在在刚才的图加了很多灰色的一些计算

111
00:04:57,480 --> 00:05:01,800
这里面每一个是单节点的一个隐节点

112
00:05:01,800 --> 00:05:03,560
在反向的时候

113
00:05:03,560 --> 00:05:06,160
我是从y去计算

114
00:05:06,160 --> 00:05:09,480
然后算得我每一个输入的导数

115
00:05:09,480 --> 00:05:12,800
根据链式求导法则

116
00:05:12,800 --> 00:05:14,040
进行展开

117
00:05:14,040 --> 00:05:15,960
我对f(x)进行求导

118
00:05:15,960 --> 00:05:20,040
实际上是对每一个中间变量的一个偏导

119
00:05:20,080 --> 00:05:21,760
然后累积起来的

120
00:05:23,520 --> 00:05:24,756
那Reverse Model就是

121
00:05:24,756 --> 00:05:27,993
完全根据刚才讲的那条公式来计算的

122
00:05:28,520 --> 00:05:32,440
这里面的vi就是我的yi对vi进行求导

123
00:05:32,440 --> 00:05:35,160
那下面从下面往上看起

124
00:05:35,160 --> 00:05:37,280
这里面有一个假设

125
00:05:37,280 --> 00:05:40,040
就是yi等于1

126
00:05:40,040 --> 00:05:41,680
这个假设很重要

127
00:05:41,680 --> 00:05:44,240
任何在计算的时候或者计算机里面

128
00:05:44,240 --> 00:05:46,160
都会把它变成一个1

129
00:05:46,160 --> 00:05:47,800
然后再往上去计算

130
00:05:47,840 --> 00:05:51,160
后面进行实际的一个代码编写的时候

131
00:05:51,160 --> 00:05:52,920
也会这么去做

132
00:05:52,920 --> 00:05:56,200
v4的一个导数实际上是v5的导数

133
00:05:56,200 --> 00:05:58,440
然后v5对v4的一个偏导

134
00:05:58,440 --> 00:06:01,120
然后这里面很简单就是v5乘以1

135
00:06:01,120 --> 00:06:03,520
因为这个的偏导其实是1

136
00:06:03,520 --> 00:06:06,680
然后不断的去往上计算

137
00:06:06,680 --> 00:06:11,080
算到我的x1的导数和x2的导数

138
00:06:12,560 --> 00:06:13,720
有点神奇

139
00:06:13,720 --> 00:06:15,760
x1的导数是5.5

140
00:06:15,800 --> 00:06:18,120
跟刚才的计算是一模一样的

141
00:06:18,120 --> 00:06:22,160
就是我的这一个5.5

142
00:06:22,160 --> 00:06:26,320
而我的那个x2的导数也算出来了

143
00:06:26,320 --> 00:06:28,880
x2的导数是1.167

144
00:06:28,880 --> 00:06:31,040
Reverse Model有点不一样的就是

145
00:06:31,040 --> 00:06:32,840
我一次输入

146
00:06:32,840 --> 00:06:36,640
通过刚才的链式求导法则展开

147
00:06:36,640 --> 00:06:41,200
然后去计算得所有x的一个导数结果

148
00:06:42,400 --> 00:06:45,360
在计算机里面或者在神经网络里面

149
00:06:45,360 --> 00:06:48,160
当我的输入不断的增大的时候

150
00:06:48,160 --> 00:06:49,640
只有一个输出

151
00:06:49,640 --> 00:06:53,240
这个时候就非常适合用Reverse Model了

152
00:06:53,240 --> 00:06:55,400
这也是为什么去讲

153
00:06:55,400 --> 00:06:58,160
会有一个Reverse Model去进行求导

154
00:06:58,160 --> 00:07:00,120
而不是像高等数学一样

155
00:07:00,120 --> 00:07:03,120
只讲一个正向的求导方式

156
00:07:03,120 --> 00:07:07,160
下面来看看这个叫做雅可比矩阵

157
00:07:07,160 --> 00:07:09,240
雅可比矩阵很简单

158
00:07:09,265 --> 00:07:11,680
是y关于x的一个梯度

159
00:07:11,680 --> 00:07:11,705
然后表示成为雅可比矩阵

160
00:07:11,705 --> 00:07:13,600
然后表示成为雅可比矩阵

161
00:07:13,600 --> 00:07:15,920
用Jf来表示

162
00:07:15,920 --> 00:07:17,040
可以看到

163
00:07:17,040 --> 00:07:21,320
这里面每一个输出对x1进行求导

164
00:07:21,320 --> 00:07:23,920
我有多个y的时候怎么办

165
00:07:23,920 --> 00:07:27,200
所以就会把它变成一个大的矩阵

166
00:07:27,200 --> 00:07:29,800
那行就是我只有一个输出

167
00:07:29,800 --> 00:07:31,880
然后从x1到xn

168
00:07:31,880 --> 00:07:33,840
每个x对它进行的求导

169
00:07:33,840 --> 00:07:36,800
竖的呢就是我从y1到ym

170
00:07:36,800 --> 00:07:38,960
但是每一次只对我的x1

171
00:07:38,960 --> 00:07:41,760
就是一个单的输入进行求导

172
00:07:41,800 --> 00:07:43,360
那通过雅可比矩阵

173
00:07:43,360 --> 00:07:45,040
就可以去表示

174
00:07:45,040 --> 00:07:48,800
刚才的正向微分和反向微分了

175
00:07:48,800 --> 00:07:51,920
刚才讲的那么多正向微分反向微分的

176
00:07:51,920 --> 00:07:53,720
这只是一个过程

177
00:07:53,720 --> 00:07:56,680
过程也很难用一个矩阵去表示的

178
00:07:56,680 --> 00:07:59,160
引入了雅可比矩阵之后

179
00:07:59,160 --> 00:08:01,160
不是很方便的去表示的吗

180
00:08:20,945 --> 00:08:22,945
实际上正向的自动微分模式

181
00:08:22,945 --> 00:08:25,865
叫做雅可比向量的积

182
00:08:25,865 --> 00:08:28,854
还是这条公式

183
00:08:28,854 --> 00:08:28,910
Jf是雅可比矩阵

184
00:08:28,910 --> 00:08:30,854
Jf是雅可比矩阵

185
00:08:31,486 --> 00:08:36,277
现在假设向量v是关于函数l=g(y)的梯度

186
00:08:36,277 --> 00:08:39,140
就是把y(损失函数)或者说输出

187
00:08:39,140 --> 00:08:41,275
通过g去表示

188
00:08:41,275 --> 00:08:42,925
多了一个函数之后呢

189
00:08:42,925 --> 00:08:44,640
对y进行求导

190
00:08:44,640 --> 00:08:46,640
这个v就对y进行求导

191
00:08:46,640 --> 00:08:48,840
为什么要多这么一个函数呢

192
00:08:48,840 --> 00:08:51,240
因为实际上在机器学习里面

193
00:08:51,240 --> 00:08:52,800
会有一个损失函数

194
00:08:52,800 --> 00:08:54,800
就是最后一个多一个导数

195
00:08:54,800 --> 00:08:55,840
那如果它是1

196
00:08:55,840 --> 00:08:57,520
它是一个最后已经得到的数

197
00:08:57,520 --> 00:09:00,240
对它进行多一次求导是没有关系的

198
00:09:00,240 --> 00:09:04,400
现在来去看一下对l这个函数

199
00:09:04,400 --> 00:09:05,520
关于x1的导数

200
00:09:05,520 --> 00:09:08,000
也就是j乘以我的v

201
00:09:08,000 --> 00:09:10,880
所以叫做雅可比向量的积

202
00:09:10,960 --> 00:09:12,960
然后去得到输出

203
00:09:12,960 --> 00:09:15,360
对我的第1个输入x进行求导

204
00:09:15,360 --> 00:09:17,360
就正向的表示了

205
00:09:18,160 --> 00:09:20,480
那反向的时候怎么办呢

206
00:09:20,480 --> 00:09:21,760
反向的时候

207
00:09:21,760 --> 00:09:25,280
叫做Vector-Jacobian乘积

208
00:09:25,280 --> 00:09:27,600
这个反向反过来是不一样的

209
00:09:27,600 --> 00:09:30,560
因为反向反过来去算的时候

210
00:09:30,560 --> 00:09:32,240
这里面加了一个T

211
00:09:32,240 --> 00:09:34,160
反向加了一个T有什么不一样

212
00:09:34,160 --> 00:09:36,560
看一下最后一个乘积之后

213
00:09:36,560 --> 00:09:38,400
最后的一个l

214
00:09:38,400 --> 00:09:42,800
l是对每一个输入都进行求导

215
00:09:42,800 --> 00:09:44,400
所以反向的时候

216
00:09:44,400 --> 00:09:48,720
就变成了Vector Jacobin Production

217
00:09:50,240 --> 00:09:53,520
现在来看看正向跟反向模式之间的比较

218
00:09:53,520 --> 00:09:56,240
雅可比矩阵还是那个雅可比矩阵

219
00:09:56,240 --> 00:09:59,280
只是我每一次去计算的时候

220
00:09:59,280 --> 00:10:03,280
都是去算雅可比矩阵的不同的内容

221
00:10:03,280 --> 00:10:06,320
对于一个输入的时候特别多

222
00:10:06,320 --> 00:10:07,520
就我的输入非常多

223
00:10:07,520 --> 00:10:09,040
我的x有非常多

224
00:10:09,040 --> 00:10:13,040
那这个时候只需要进行一次反向叠代

225
00:10:13,040 --> 00:10:17,600
就可以算出牙可比矩阵的每一行了

226
00:10:17,600 --> 00:10:19,760
就我的x特别多的时候

227
00:10:19,760 --> 00:10:22,480
那如果我的x特别小的少的时候

228
00:10:22,480 --> 00:10:24,240
但是我的y特别多

229
00:10:24,240 --> 00:10:25,680
输出特别多的时候

230
00:10:25,680 --> 00:10:27,520
我通过一个正向的模式

231
00:10:27,520 --> 00:10:31,680
就可以算出牙可比矩阵的每一列了

232
00:10:31,680 --> 00:10:34,800
这个就是牙可比矩阵所表示的内容

233
00:10:34,960 --> 00:10:36,400
回到机器学习里面

234
00:10:36,400 --> 00:10:38,960
或者回到自动微分里面

235
00:10:38,960 --> 00:10:40,160
有个最大的区别

236
00:10:40,160 --> 00:10:42,000
就是当我的m大于n

237
00:10:42,000 --> 00:10:44,960
我的输出大于我的输的时候

238
00:10:44,960 --> 00:10:46,560
适用于前向模式

239
00:10:46,560 --> 00:10:48,400
当我的n大于m

240
00:10:48,400 --> 00:10:51,760
就是我的输入绝对的大于我的输出

241
00:10:51,760 --> 00:10:54,160
那适合用反向自动微分

242
00:10:54,160 --> 00:10:58,400
回到跟机器学习或者跟神经网络相结合

243
00:10:58,400 --> 00:11:00,800
可以看到神经网络

244
00:11:00,800 --> 00:11:04,240
输入特别特别的多

245
00:11:04,560 --> 00:11:08,080
输入可能是几百万张图片或者几百万个神经元

246
00:11:08,080 --> 00:11:10,640
假设我的输入有非常多

247
00:11:10,640 --> 00:11:14,480
我的输出就是去猜我是李冬梅

248
00:11:14,480 --> 00:11:16,160
我的输入有可能是李红梅

249
00:11:16,160 --> 00:11:16,720
李小梅

250
00:11:16,720 --> 00:11:17,520
李大梅

251
00:11:17,520 --> 00:11:19,200
你的神经网络里面

252
00:11:19,200 --> 00:11:21,520
输入绝对的大于输出

253
00:11:21,520 --> 00:11:24,160
所以更倾向于用反向模式

254
00:11:24,160 --> 00:11:29,040
一次过去算出输入不同x的一个值

255
00:11:29,040 --> 00:11:30,080
通过这种方式

256
00:11:30,080 --> 00:11:32,320
就可以迭代式的去求得

257
00:11:32,400 --> 00:11:34,880
雅可比矩阵的每一行

258
00:11:34,880 --> 00:11:36,400
算得出来每一行

259
00:11:36,400 --> 00:11:39,680
就知道每一个神经元的导数

260
00:11:39,680 --> 00:11:40,880
有了每一个神经元

261
00:11:40,880 --> 00:11:44,080
后面就会介绍我算这个导数

262
00:11:44,080 --> 00:11:46,640
对神经网络有什么用

263
00:11:46,640 --> 00:11:49,120
这个不会在自动微分里面去展开

264
00:11:49,120 --> 00:11:53,600
而是在AI框架基础里面去展开

265
00:11:53,600 --> 00:11:54,480
所以不要急

266
00:11:54,480 --> 00:11:57,120
还是去看看数学概念

267
00:11:58,640 --> 00:12:01,680
自动微分就是通过电视求导法则

268
00:12:01,760 --> 00:12:05,280
把所有的最简单的计算组合起来

269
00:12:05,280 --> 00:12:07,360
最后通过雅可比矩阵

270
00:12:07,360 --> 00:12:09,600
去对数学进行表示

271
00:12:09,600 --> 00:12:12,560
那优势就是数值的精度非常高

272
00:12:12,560 --> 00:12:16,560
因为它是真正的通过电视求导法则去计算出来的

273
00:12:16,560 --> 00:12:20,000
另外一个优势就是没有表达式膨胀

274
00:12:20,000 --> 00:12:21,920
因为它不会不断的去表达

275
00:12:21,920 --> 00:12:24,960
它不会无限制的去膨胀公式

276
00:12:24,960 --> 00:12:30,080
那缺点就是我需要去存储很多中间变量

277
00:12:30,160 --> 00:12:33,520
这里面每条公式都要存起来

278
00:12:33,520 --> 00:12:35,680
每个V0、V2、V1

279
00:12:35,680 --> 00:12:37,760
每个中间变量都要存起来

280
00:12:37,760 --> 00:12:42,960
存起来方便上一次或者下一次调用的时候去使用

281
00:12:42,960 --> 00:12:48,000
所以它的坏处就是需要很多中间变量去求导结果

282
00:12:48,000 --> 00:12:51,840
那这就会导致占用大量的计算机内存

283
00:12:53,120 --> 00:12:57,120
这个就是自动微分的一个基本概念

284
00:12:57,120 --> 00:12:58,640
可能这里面有点难

285
00:12:58,640 --> 00:13:01,440
今天的课稍微长了一点点

286
00:13:01,440 --> 00:13:03,440
今天来总结一下

287
00:13:03,440 --> 00:13:05,040
学会了自动微分

288
00:13:05,040 --> 00:13:08,640
实际上是分成前向微分和反向微分

289
00:13:08,640 --> 00:13:11,680
还了解了雅克比矩阵的基本原理

290
00:13:11,680 --> 00:13:14,400
前向和反向的雅克比的表示

291
00:13:14,400 --> 00:13:16,800
还了解了自动微分的优缺点

292
00:13:16,800 --> 00:13:19,600
还有在AI框架最常用的模式
